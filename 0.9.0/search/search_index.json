{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"thankyou/","title":"Thank You","text":""},{"location":"blog/202209-building-our-first-optimizer/","title":"Building our first Optimizer","text":"<p>1st October 2022</p> <p>Building a SQL query engine is no trivial task, we started building Opteryx about 10 months ago on a complete rewrite of our first query engine. It's only recently we've started to focus on improving performance, initially the performance of the new engine was such a massive improvement over the old, almost purely due to architectural differences, there is a lot of room for improvement.</p> <p>We since users started using the engine about five months ago we've been able to implement some point improvements.</p> <ul> <li>Selection before Aggregation, this isn't implemented by an Optimizer, instead it was part of the Aggregator Operator.</li> <li>We've also implemented page merging into the Selection Operator to make the most of SIMD and parallel execution.</li> </ul> <p>This week saw the first iteration of plan-based optimization, where the Optimizer receives a query plan and uses rules to rewrite it to run faster.</p> <p>All complex systems are built on something simple. Our first iteration of the Optimizer is simple. It currently does one action - it takes <code>AND</code> conjunctions in the <code>WHERE</code> and <code>HAVING</code> clauses and splits them into individual Selection Operators to run in serial.</p> <p>The effect of doing this, with no intelligence to the order or other factors, has been observed to be up to a 85% reduction in the execution time of the Selection step of real-world queries - i.e. ones we see being written and run by users.</p> <p>The query time is still dominated by the read time in most situations, but saving 6 seconds on a 42 second query is still meaningful.</p> <p>The Optimizer is heuristic, that is, it has some simple rules when it applies. It is not cost-based as we've not built a statistics model and store to compliment the Optimizer yet. This is the expected evolution of the planner, but for now we will focus on extending the rule-based planner with more rules.</p> <p>We look forward to being able to update you on other major updates and improvements to Opteryx.</p>"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/","title":"Writing a SQL Engine","text":""},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#motivation","title":"Motivation","text":"<p>No-one in their right mind would write a SQL Engine if they didn't need to. There are a lot of options in the space of providing SQL query access to distributed data - with a few players dominating the market like Trino, DuckDB and SQLite.</p> <p>We had a problem where we wanted a SQL interface to our data, but none of the existing tools were a good fit for our situation. We could change ourselves to fit an existing toolset, but wanted to explore other options before committing to vendor-defined design.</p>"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#prior-attempts","title":"Prior Attempts","text":"<p>The data store we're working with was designed to be transctional (read a row of data, process it, save the result, repeat). We use JSON lines files, which for this use case we were unable to find anything better in the sweet spot of human and machine readable, and performance to read and write.</p> <p>With this as the datastore, our first attempt at a SQL engine was also transactional, following what is known as the Volcano Model. This aligned well with the tools that we had written to process the data so most of the effort was with translating the SQL syntax to filters that the existing tools could understand. Functionality like GROUP BY was added to make it feel more like a database and less like a log-viewer.</p> <p>This provided an acceptable level of functionality for single-table queries (the existing tools only ever read from one table and write to one table) and the engine was implemented into user-facing systems.</p> <p>As data grew, we started to hit problems. Reading tens of million of rows, constraints outside the control of the system meant that jobs that ran for longer than 180 seconds were terminated. This generally meant that queries with more than about 30 million records (or far fewer records but with calculations) timed out. A lot of queries were still able to be run as not everything hit these thresholds, but it couldn't be used for large data analysis.</p>"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#redesign","title":"Redesign","text":"<p>The decision to write the SQL Engine as a new library separate from the existing data handling library was made early it freed us from some of the implementation decisions of the existing solution.</p> <p>XXX leveraging Apache Arrow (using Parquet) to help improve performance. Parquet was assessed for the transactional use case but the optimized JSONL implementation in Mabel consistently outperformed Parquet. However, reassessing performance for a SQL engine, Parquet out performs JSONL.</p> <p>The previous SQL Engine had a fixed execution plan, this meant that no matter what your query was it followed the same steps, with some steps doing nothing. This was simplier to write, but will have affected performance. Opteryx creates a query plan, the initial version doesn't optimize this plan by doing things like running selections and projections early, but it does only add steps to the plan that are required. </p> <p>Writing a robust SQL parser is hard, there are a considerable number of rules and exceptions that need to be followed. We chose SqlOxide to parse the statements to an AST. There is still a lot of work to convert the AST to a query plan - but the AST removes ambiguity, something like <code>SELECT SELECT FROM FROM FROM</code> would be difficult to parse with a regex-based parser.</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/","title":"Introduction to Query Engines","text":""},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#scope","title":"Scope","text":"<p>Just the Data Query Language aspects - that\u2019s more or less the bit that handles <code>SELECT</code> statements.</p> <p>Will cover generic aspects of implementation, but will include detail relating to Opteryx.</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-steps","title":"Key Steps","text":"<p>Query Language Interpretation</p> <p>Query Planning and Optimization</p> <p>Execution Engine</p> <p>Files / Storage</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-steps_1","title":"Key Steps","text":"<p>SQL -&gt; Query Language Interpretation</p> <p>Abstract Syntax Tree -&gt; Query Planning and Optimization</p> <p>Query Plan -&gt; Execution Engine</p> <p>Resource Access -&gt; Files / Storage</p> <p>Result Creation</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-components","title":"Key Components","text":"<p>Parser / Lexer Interprets SQL into a semantic representation (AST) Abstract Syntax Tree (AST) First machine processable representation of the query (we can rewrite the query here) Query Plan Describes the steps to take to fulfil the request Optimizer Reworks the Query Plan to improve performance Executor Runs the Query Plan</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#fixed-query-plan","title":"Fixed Query Plan","text":"<p>Based on Relational Algrebra.</p> <p>This is the order items are processed before optimizations.</p> <p>Has implications, e.g. can\u2019t <code>GROUP BY</code> aliases defined in the <code>SELECT</code> clause.</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#naive-plan-order","title":"Naive Plan Order","text":"<p>SELECT (5) [project]  DISTINCT (6) [distinct] FROM (1) WHERE (2) [select] GROUP BY (3) [aggregate] HAVING (4) [select] ORDER BY (7) [sort] OFFSET (8) LIMIT (9)</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#plan-optimization","title":"Plan Optimization","text":"<p>Optimized plan has to create the same result as naive plan.</p> <p>Get rid of data (rows and columns) as quickly as possible</p> <ul> <li>Selection (<code>WHERE</code>) and Projection (<code>SELECT</code>) push-downs</li> <li><code>LIMIT</code> push-downs</li> </ul> <p>Algorithm Decisions</p> <ul> <li>Choose <code>JOIN</code> order and algorithm (HASH or SORT MERGE)</li> </ul>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#execution-models","title":"Execution Models","text":"<ul> <li>Row Processing (Volcano) Mabel</li> <li>Block/Column Processing (Vectorized) Opteryx</li> </ul>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#volcano-model","title":"Volcano Model","text":"<p>1) The step at the end of our plan tries to return a record 1) It asks the previous step, which asks the previous step 1) Until we get to the files, which we read line-by-line</p> <p>All calculations are done on each line, one at a time</p>"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#blockcolumn-processing","title":"Block/Column Processing","text":"<p>1) The step at the end of our plan tries to return a block 1) It asks the previous step, which asks the previous step 1) Util we get to the files, which we read an entire file/block</p> <p>All calculations are done block at a time.</p>"},{"location":"blog/unpublished/20221001%20Storage/","title":"20221001 Storage","text":"<p>there's the classic diagram that shows that on CPU cache is fast, through to spinning rust is slow and it shows scaled timeframes - if it take a second to get data out of registers on the CPU, HDD would take a lifetime. The reference user for Opteryx uses a cloud storage provider for their data - I wish I had spinning rust.</p> <p>There's two broad approaches to speeding up disk access:</p> <ul> <li>make disk access faster</li> <li>don't do it</li> </ul>"},{"location":"blog/unpublished/202212%20Lessons%20Learnt%20so%20Far/","title":"Lessons Learnt So Far","text":"<p>9th December 2022</p> <p>We never thought building a SQL Engine from scratch was going to be easy, and we expected some hard learnings.</p> <p>The conventions in SQL Engines generally make good sense, follow them</p> <p>If most SQL Engines work a particular way, chances are they've done that because they've probably </p> <p>Unit testing is fine, but write hundreds of tests cases which run real SQL queries</p> <p>Testing individual components in isolation is important, and for some components the only real way to get assurance that it works as expected; but we find that writing SQL queries to exercise the entire stack usually, test-per-test, provides more value on average.</p> <p>The key finding is that a SQL Engine is complex, where a minor change in one component can have an unexpected impact on another.</p> <p>Have real systems and real users as your beta testers</p> <p>If you want real users to be able to use your system, make sure you have real users on your system early. This can be problematic, especially if you can't guarantee the accuracy of the engine and perforance is likely to be pretty poor.</p> <p>You can't fabricate test data for all your test scenarios</p> <p>You will never write tests that cover the variation of real-world data.</p> <p>Storage read speed will kill any performance boosts from algorithmic improvements</p> <p>If you don't control of the the thing writing of the data - assume the worst</p> <p>Bonus Point: PyArrow is awesome, but it has bugs, odd limitations and some parts are so slow it hurts</p> <p>bugs - date diff just doesn't work for months</p> <p>odd limitations - can't join on tables with arrays or structs</p> <p>so slow it hurts - Abstraction == Slow, if you want fast, you need to get as close to the bare metal or raw API as possible</p>"},{"location":"contributing/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via Slack (@joocer) or GitHub (@joocer).</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period.</p> <p>Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of, or an extreme violation, of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/contributing/","title":"Contributor Guide","text":""},{"location":"contributing/contributing/#welcome-to-the-opteryx-contributor-guide","title":"Welcome to the Opteryx Contributor Guide","text":"<p>We're excited your considering contributing to Opteryx and look forward to reviewing your contribution. </p> <p>Opteryx is open source software, we value your feedback and want to make contributing to this project as easy and transparent as possible. In this section you will find information to help you to bring your unique skills and experience and join us in building Opteryx.</p> <p>Opteryx is primarily written in Python, but you don't need to be a Python developer to contribute. All contributions, bug reports, documentation improvements, and ideas are welcome.</p>"},{"location":"contributing/contributing/#our-tools","title":"Our Tools","text":""},{"location":"contributing/contributing/#github","title":"GitHub","text":"<p> Join us on Github</p> <p>We use GitHub to host the code, track feature requests, perform testing, report bugs and maintain documentation.</p> <p>All submissions, including submissions by core project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. </p> <p>Pull requests that add support for, or fix a bug in, a feature in a popular RDBMS, or address deficiency in documentation, will likely be accepted after a brief review. For more substative changes you should start a discussion to coordinate with maintainers.</p> <p>We have included a number of tests which run automatically when code is submitted to help maintain consistency and quality of the codebase, note that these may not be automatically triggered for new contributors. Pull requests which do not meet the quality criteria will not be reviewed or merged.</p> <p>Changes should have:  </p> <ul> <li>Corresponding unit and regression tests  </li> <li>A clean execution of the CI tests   </li> <li>Updated documentation (code-level, module-level and docs/ folder as appropriate)</li> <li>Attributed external sources  </li> </ul>"},{"location":"contributing/contributing/#gitter","title":"Gitter","text":"<p>Gitter is our target channel for interactive conversations and support.</p> <p> Join us on Gitter</p>"},{"location":"contributing/contributing/#licence","title":"Licence","text":"<p>All code is licensed under the Apache Software License 2.0, unless clearly stated otherwise.</p> <p>Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions.</p> <p>Any submissions with incompatible licence or licence conditions may be rejected from inclusion.</p>"},{"location":"contributing/etiquette/","title":"Community Etiquette","text":""},{"location":"contributing/etiquette/#be-nice-to-everyone","title":"Be nice to everyone","text":"<p>Assume everyone has good intentions.</p>"},{"location":"contributing/etiquette/#check-off-your-resolved-questions","title":"Check off your resolved questions","text":"<p>If you have received a useful reply to your question, please drop a \u2705 reaction or a reply for affirmation.</p>"},{"location":"contributing/etiquette/#try-not-to-repost-question","title":"Try not to repost question","text":"<p>If you have asked a question and have not got a response in 24hrs, please review your question for clarity and revise it.</p>"},{"location":"contributing/etiquette/#post-in-public","title":"Post in public","text":"<p>Please don't direct message any individual member of Mabel community without their explicit permission, independent of reason. Your question might be helpful for other community members.</p>"},{"location":"contributing/etiquette/#dont-spam-tags","title":"Don't spam tags","text":"<p>Mabel and Opteryx are supported by volunteers, avoid tagging members unless it is urgent.</p>"},{"location":"contributing/etiquette/#use-threads-for-discussion","title":"Use threads for discussion","text":"<p>To keep the main channel area clear, we request to use threads to keep an ongoing conversation organized.</p>"},{"location":"contributing/version-goals/","title":"Version Goals","text":"<p>Version goals are set out to provide a view of a road map. Goals are intended to demonstrate direction of enhancements and evolution of the engine, inclusion on the list does not guarantee delivery at a particular point in time, or at all. As always, conditions on the road may require you to consider your current path.</p>"},{"location":"contributing/version-goals/#version-10","title":"Version 1.0","text":"<p>Version 1.0 goals will be delivered across various minor versions building toward v1.0. These minor releases will also include bug fixes, performance improvements and functional completeness. The items listed below are major pieces of functionality or milestones.</p> <ul> <li> CTEs (<code>WITH</code>) statements supported [v0.8]</li> <li> Read across multiple data sources (e.g. GCS and Postgres in the same query) [v0.2]</li> <li> Support different plaform data sources (e.g. FireStore [v0.3] and BigQuery)</li> <li> Rule-based query optimizer [v0.5]</li> <li> <code>JOIN</code> statements supported [v0.1]</li> <li> Functions using the result of Functions (e.g. <code>LENGTH(LIST(field))</code>) [v0.3]</li> <li> Inline operators (e.g. <code>firstname || surname</code>) [v0.3]</li> <li> Local Buffer Cache implemented [v0.6]</li> </ul>"},{"location":"contributing/version-goals/#after-version-10","title":"After Version 1.0","text":"<p>These goals indicate which items are considered important for the engine to support, but we are willing to lower the priority against other items.</p> <ul> <li> Python PEP249 compatibility</li> <li> ANSI SQL92 compatibility</li> <li> All TPH-C statements execute</li> <li> Persisted materialized views</li> <li> Distributed execution</li> <li> Metastore used in planning and optimization</li> <li> Permissions Model</li> <li> Correctness benchmarks written [v0.6, v0.9] and acceptable pass-rate obtained</li> <li> Performance benchmarks written [v0.5, v0.9] and monitored</li> </ul>"},{"location":"contributing/internals/project-structure/","title":"Project Structure","text":""},{"location":"contributing/internals/project-structure/#folder-structure","title":"Folder Structure","text":"<p>Opteryx's repository folder structure is described below:</p> <pre><code>opteryx/                 &lt;- main opteryx library\n \u251c\u2500\u2500 connectors/         &lt;- modules to connect to data sources\n \u251c\u2500\u2500 functions/          &lt;- modules to execute functions within SQL statements\n \u251c\u2500\u2500 managers/           &lt;- libraries responsible for key functional units\n \u2502   \u251c\u2500\u2500 expression/     &lt;- modules implementing expression evaluation\n \u2502   \u251c\u2500\u2500 kvstore/        &lt;- modules implementing interfacing with KV Stores\n \u2502   \u251c\u2500\u2500 planner/        &lt;- modules implementing query planning and optimizing\n \u2502   \u2514\u2500\u2500 schemes/        &lt;- modules implementing storage schemes\n \u251c\u2500\u2500 models/             &lt;- internal data models\n \u251c\u2500\u2500 operators/          &lt;- modules implementing steps in the query plan\n \u251c\u2500\u2500 samples/            &lt;- sample data\n \u251c\u2500\u2500 shared/             &lt;- global resources\n \u251c\u2500\u2500 third_party/        &lt;- third party code\n \u2502   \u251c\u2500\u2500 distogram/ \n \u2502   \u251c\u2500\u2500 fuzzy/   \n \u2502   \u251c\u2500\u2500 hyperloglog/  \n \u2502   \u251c\u2500\u2500 pyarrow_ops/ \n \u2502   \u2514\u2500\u2500 ...  \n \u251c\u2500\u2500 utils/              &lt;- helper libraries\n \u2514\u2500\u2500 ...       \n</code></pre>"},{"location":"contributing/internals/query-engine/","title":"Query Engine","text":"<p>If you are interested in how databases work, I recommend the resources from The CMU Database Group and the collection of resources at Awesome Database Learning.</p> <p>The Opteryx query engine has the following key components and processing queries follows this high-level series of steps:</p> <p> Parser &amp; Lexer receives the user SQL and builds an Abstract Syntax Tree (AST). Binder maps contextual information to the AST. Planner receives the AST and builds a Query Plan. Optimizer receives a Query Plan and rewrites it to improve performance.  Executor receives the Query Plan and returns the result dataset.  </p>"},{"location":"contributing/internals/query-engine/#parser-lexer","title":"Parser &amp; Lexer","text":"<p>The primary goal of the Parser and Lexer (which in some engines is two separate components) is to interpret the SQL provided by the user. This is generally done in two steps, the first is the break the query into separate tokens (or words) and the second is to understand the meaning of those tokens.</p> <p>For example for this statement</p> <pre><code>SELECT SELECT\nFROM FROM\n</code></pre> <p>The Parser and Lexer will understand that we're requesting the field <code>SELECT</code> from the relation <code>FROM</code>.</p> <p>Opteryx uses sqlparser-rs as its Parser and Lexer, as a Rust library, Opteryx creates Python bindings for sqlparser-rs (derived from sqloxide). Opteryx does not support all features and functionality provided by this library.</p> <p>This sqlparser-rs interprets all SQL except for the Temporal <code>FOR</code> clause which are handled separately.</p>"},{"location":"contributing/internals/query-engine/#binder","title":"Binder","text":"<p>The Binder's primary goal is to embelish and replace information in the AST with details which the Parser &amp; Lexer did not have.</p> <p>This is used for replacing variables in queries with their literal equivalents and adding temporal information to relations.</p>"},{"location":"contributing/internals/query-engine/#query-planner","title":"Query Planner","text":"<p>The Query Planner's primary goal is to convert the AST into a plan to respond to the query. The Query Plan is described in a Directed Acyclic Graph (DAG) with the nodes that acquire the raw data, usually from storage, at the start of the flow and the node that forms the data to return to the user (usually the <code>SELECT</code> step) at the end.</p> <p>The DAG is made of different nodes which process the data as they pass through then node. Different node types exist for processing actions like Aggregations (<code>GROUP BY</code>), Selection (<code>WHERE</code>) and Distinct (<code>DISTINCT</code>).</p> <p>Query plans follow a generally accepted order of execution. This does not match the order queries are usually written in, instead it follows this order:</p> <p> </p> <p>The planner ensures the processes to be applied to the data reflect this order and creates the most convenient plan to achieve this.</p> <p>The Query Plan can be seen for a given query using the <code>EXPLAIN</code> query.</p>"},{"location":"contributing/internals/query-engine/#query-optimizer","title":"Query Optimizer","text":"<p>The goal of the Query Optimizer is to rewrite the Query Plan to a plan which will return result to users faster. This is generally achieved through reducing the data being handled as early in the query as possible (such as projection push-down), reducing the complexity of steps (such as using logical equivelences to make expressions simpler) or combining steps (such as sort and limit into a heap sort).</p> <p>The current optimizer in Opteryx is immature with very few rules and requires hand-tuning of the input query for the optimizer to achieve best results.</p>"},{"location":"contributing/internals/query-engine/#query-executor","title":"Query Executor","text":"<p>The goal of the Query Executor is to produce the results for the user. It takes the Plan and executes the steps in the plan.</p> <p>Opteryx implements a vectorized Volcano model executor. This means that the planner starts at the node closest to the end of the plan (e.g. <code>LIMIT</code>) and asks it for a page of data. This node asks its preceeding node for a page of data, etc etc until it gets to the node which aquires data from source. The data is then processed by each node until it is returned to the <code>LIMIT</code> node at the end.</p>"},{"location":"contributing/internals/query-engine/#performance-features","title":"Performance Features","text":"<p>The following features are build into the query engine to improve performance</p> <ul> <li>Small pages are merged together (referred to as 'defragmentation') before activities which operate on the entire page-at-a-time (such as selections)</li> <li>Projections are pushed to the blob parser, either to prevent parsing of unwanted fields (Parquet), or before passing to the next operation</li> <li>A buffer pool is used to maintain an in-memory cache of blobs</li> <li>A shared page cache can be used (e.g. memcached) to reduce reads to attached or remote storage</li> <li>An LRU-K cache eviction strategy with a fixed eviction budget per query to help ensure effective use of the page cache</li> <li>Aggressive pruning of date partitioned datasets</li> <li>SIMD and vectorized execution where available (via Numpy and PyArrow)</li> <li>Projection before <code>GROUP BY</code> to reduce data handled by the aggregators</li> <li><code>null</code> values are eliminated from filters before they are executed, and added back in after values have been compared, reducing the pointless work of comparing <code>null</code> values</li> </ul>"},{"location":"contributing/internals/storage-engine/","title":"Storage Formats","text":"<p>This document primarily applies to the Blob and File stores, such as GCS, S3 and local disk.</p>"},{"location":"contributing/internals/storage-engine/#supported-data-files","title":"Supported Data Files","text":""},{"location":"contributing/internals/storage-engine/#parquet","title":"Parquet","text":"<p>Parquet is the preferred file format for Opteryx and use of Parquet offers optimizations not available with other formats. If a datasource has query performance issues or is hot in terms of query use, converting to Parquet is likely to improve performance. Performance testing suggests Parquet with zStandard compression provides best balance of IO to read the files and CPU to to the files.</p> <p>As will all guidance on performance tuning - this appears to be generally correct but test for your specific circumstances.</p>"},{"location":"contributing/internals/storage-engine/#orc-feather","title":"ORC &amp; Feather","text":"<p>Opteryx support ORC and Feather, but not all optimizations implemented for Parquet are implemented for these formats. These will still provide better performance than traditional data formats.</p> <p>ORC files have limited support on Windows and PyPy environments.</p>"},{"location":"contributing/internals/storage-engine/#jsonl","title":"JSONL","text":"<p>JSONL and zStandard compressed JSONL files.</p> <p>Files don't need an explicit schema, but each partition must have the same columns in the same order in every row of every file.</p> <p>Data types are inferred from the records, where data types are not consistent, the read will fail.</p> <p>Opteryx supports zStandard Compressed JSONL files as created by Mabel, these perform approximately 20% faster than raw JSONL files primarily due to reduced IO.</p>"},{"location":"contributing/internals/storage-engine/#csv-tsv","title":"CSV &amp; TSV","text":"<p>Comma-separated and tab-delimited files can be used with Opteryx, however this is only provided to meet the base expectation that the system can support these formats. It is not recommended and limited regression tests are written relating to CSV handling.</p>"},{"location":"contributing/internals/storage-engine/#avro","title":"Avro","text":"<p>Avro formatted files are supported, however require an additional library to be installed (<code>pip install avro</code>) and performance is considered poor.</p>"},{"location":"contributing/internals/storage-engine/#storage-layout","title":"Storage Layout","text":"<p>For Blob/File stores, the path of the data is used as the name of the relation in the query. There are currently two built in data schemas, none (or flat) and Mabel.</p>"},{"location":"contributing/internals/storage-engine/#flat","title":"Flat","text":"<p>The flat schema is where the data files are stored in the folder which names the relation, such as:</p> <pre><code>customer/\n    preferences/\n        file_1\n        file_2\n        file_3\n</code></pre> <p>This would be available to query with a query such as:</p> <pre><code>SELECT *\nFROM customer.preferences;\n</code></pre> <p>Which would read the three files to return the query.</p>"},{"location":"contributing/internals/storage-engine/#mabel","title":"Mabel","text":"<p>The Mabel schema is where data is structured in date labelled folders</p> <pre><code>customer/\n    preferences/\n        year_2020/\n            month_03/\n                day_04/\n                    file_1\n                    file_2\n                    file_3\n</code></pre> <p>The date components of the folder structure are part of the temporal processing, and are not directly referenced as part of the query, instead they form part of the temporal clause (<code>FOR</code>)</p> <pre><code>SELECT *\nFROM customer.preferences\nFOR '2020-03-04'\n</code></pre> <p>This approach enables data to be partitioned by date and pruned using temporal filters.</p>"},{"location":"contributing/internals/testing/","title":"How Opteryx is Tested","text":"<p>Opteryx utilizes a number of test approaches to help ensure the system is performant, secure and correct. The key test harnesses which are used to test Opteryx are listed here.</p> <p>Most testing is part of the main Opteryx repository on GitHub, however, some testing is located in other respositories.</p>"},{"location":"contributing/internals/testing/#unit-testing","title":"Unit Testing","text":"<p>Frequency: On Commit to GitHub  Maturity: Medium Location: mabel-dev/opteryx</p> <p>Part of the CI process. Tests specific aspects of the internals.</p> <p>Combined with the SQL Battery test, the aim is for 95% coverage (with explicit exceptions). Whilst 95% coverage does not ensure the tests are 'good', it does help ensures any material changes to the function of the system are captured early.</p>"},{"location":"contributing/internals/testing/#sql-battery","title":"SQL Battery","text":"<p>Frequency: On Commit to GitHub   Maturity: Medium Location: mabel-dev/opteryx</p> <p>Part of the CI process. Executes hundreds of hand-crafted SQL statements against the engine.</p> <p>The SQL Battery helps to ensure the entire system performs as expected and when used in tandem with Unit Testing, which primarily focuses on ensuring parts work as they should, this provides a level of confidence that the system continues to perform as expected.</p> <p>The battery essentially has four variations:</p> <ul> <li>Does the Query run - with no checking or validation of the outputs</li> <li>Does the Query fail - for scenarios when the query is expected to fail</li> <li>Is the shape of the results as expected - only the row and column counts are checked</li> <li>Does the Query return the right results - the returned dataset is checked</li> </ul> <p>The SQL Battery the most effective test to identify when functionality has been broken or changed by updates. The shape testing is currently considered the best value of this suite - it is fast and easy to write new tests for this suite, and the execution give reasonable considence in the correctness of the result in most situations.</p>"},{"location":"contributing/internals/testing/#performance-testing","title":"Performance Testing","text":"<p>Frequency: Ad hoc Maturity: Low Location: mabel-dev/wrenchy-bench</p> <p>The performance testing framework is only able to be run ad hoc, and there is currently no meaningful treatment or tracking out outcomes. It us currently used to confirm optimizations do have the impact of reducing query execution times.</p>"},{"location":"contributing/internals/testing/#sql-logic-test","title":"SQL Logic Test","text":"<p>Frequency: Ad hoc Maturity: Low Location: mabel-dev/wrenchy-bench</p> <p>Runs SQL statements in both Operyx and DuckDB to verify Opteryx returns the same answer as DuckDB. This has a growing set of tests which are executed, but as how relations are referenced in these systems, most queries require some hand-tuning and many are not possible with the framework as it currently is written.</p> <p>What has been able to be tested has demonstrated some deviation between these systems, so is a valuable and useful test, even in its current form.</p>"},{"location":"contributing/internals/testing/#fuzzing","title":"Fuzzing","text":"<p>Frequency: On Commit to GitHub &amp; Nightly Maturity: Low Location: mabel-dev/opteryx</p> <p>As part of the CI process, executes 100 iterations of random inputs.</p> <p>As part of a nightly test, executes 100,000 iterations of random inputs.</p> <p>Fuzzing supplies some key functions with random data to try to capture scenarios which are unexpected or unhandled.</p>"},{"location":"contributing/internals/testing/#security-code-quality-testing","title":"Security &amp; Code Quality Testing","text":"<p>Frequency: On Commit to GitHub   Maturity: Medium Location: mabel-dev/opteryx</p> <p>Various other tests are performed to help ensure code quality is maintained, these include security, form, typing, secret detection and test coverage checks using the following tools: Bandit, Semgrep, Black, MyPy, PyLint, PerfLint, Fides, SonarCloud, and Coverage.</p>"},{"location":"contributing/set-up-guides/debian/","title":"Debian/Ubuntu","text":"<p>This guide will help you to set up a Debian or Ubuntu workstation to work with the code and develop Opteryx.</p> <p>Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly.</p>"},{"location":"contributing/set-up-guides/debian/#setting-up","title":"Setting Up","text":""},{"location":"contributing/set-up-guides/debian/#1-install-python","title":"1. Install Python","text":"<p>3.10 recommended </p> <p>We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.</p>"},{"location":"contributing/set-up-guides/debian/#2-install-pip","title":"2. Install pip","text":"<pre><code>python3 -m ensurepip --upgrade\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#3-install-git","title":"3. Install Git","text":"<pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install git\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#4-install-rust","title":"4. Install Rust","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#5-clone-the-repository","title":"5. Clone the Repository","text":"<pre><code>git clone https://github.com/mabel-dev/opteryx\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#6-install-dependencies","title":"6. Install Dependencies","text":"<pre><code>python3 -m pip install --upgrade -r requirements.txt\n</code></pre> <pre><code>python3 -m pip install --upgrade setuptools setuptools_rust numpy cython\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#7-build-binaries","title":"7. Build Binaries","text":"<pre><code>python3 setup.py build_ext --inplace\n</code></pre>"},{"location":"contributing/set-up-guides/debian/#running-tests","title":"Running Tests","text":"<p>To run the regression and unit tests:</p> <p>First, install the optional dependencies, on most devices:</p> <pre><code>python3 -m pip install --upgrade -r tests/requirements.txt\n</code></pre> <p>On ARM-based devices (like Raspberry Pi):</p> <pre><code>python3 -m pip install --upgrade -r tests/requirements_arm.txt\n</code></pre> <p>Then run the regression tests.</p> <pre><code>python3 -m pytest\n</code></pre> <p>Note</p> <p>Some tests require external services like GCS and Memcached and may fail if these have not been configured.</p>"},{"location":"contributing/set-up-guides/macos/","title":"MacOS","text":"<p>This guide will help you to set up a MacOS workstation to work with the code and develop Opteryx.</p> <p>Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly.</p>"},{"location":"contributing/set-up-guides/macos/#setting-up","title":"Setting Up","text":""},{"location":"contributing/set-up-guides/macos/#1-install-python","title":"1. Install Python","text":"<p>3.10 recommended</p> <p>We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.</p>"},{"location":"contributing/set-up-guides/macos/#2-install-pip","title":"2. Install pip","text":"<pre><code>python3 -m ensurepip --upgrade\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#3-install-git","title":"3. Install Git","text":"<pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install git\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#4-install-rust","title":"4. Install Rust","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#5-clone-the-repository","title":"5. Clone the Repository","text":"<pre><code>git clone https://github.com/mabel-dev/opteryx\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#6-install-dependencies","title":"6. Install Dependencies","text":"<pre><code>python3 -m pip install --upgrade -r requirements.txt\n</code></pre> <pre><code>python3 -m pip install --upgrade setuptools setuptools_rust numpy cython\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#7-build-binaries","title":"7. Build Binaries","text":"<pre><code>python3 setup.py build_ext --inplace\n</code></pre>"},{"location":"contributing/set-up-guides/macos/#running-tests","title":"Running Tests","text":"<p>To run the regression and unit tests:</p> <p>First, install the optional dependencies, on intel-based Macs:</p> <pre><code>python3 -m pip install --upgrade -r tests/requirements.txt\n</code></pre> <p>On M-series (ARM) CPU Macs:</p> <pre><code>python3 -m pip install --upgrade -r tests/requirements_arm.txt\n</code></pre> <p>Then run the regression tests.</p> <pre><code>python3 -m pytest\n</code></pre> <p>Note</p> <p>Some tests require external services like GCS and Memcached and may fail if these have not been configured.</p>"},{"location":"contributing/set-up-guides/windows/","title":"Windows","text":"<p>This guide will help you to set up a Windows workstation to work with the code and develop Opteryx.</p> <p>If using WSL, refer to the Debian/Ubuntu set up guide. Initial set up of the WSL component is not covered in these guides.</p> <p>Intel/x86 is the recommended environment, it has not been confirmed that Opteryx operates as expected on ARM Windows - it does operate on Linux and Mac ARM.</p>"},{"location":"contributing/set-up-guides/windows/#setting-up","title":"Setting Up","text":""},{"location":"contributing/set-up-guides/windows/#1-install-python","title":"1. Install Python","text":"<p>3.10 recommended</p> <p>We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.</p>"},{"location":"contributing/set-up-guides/windows/#2-install-pip","title":"2. Install pip","text":"<pre><code>python -m ensurepip --upgrade\n</code></pre>"},{"location":"contributing/set-up-guides/windows/#3-install-git","title":"3. Install Git","text":"<p>Follow the instructions at https://git-scm.com/download/win</p>"},{"location":"contributing/set-up-guides/windows/#4-install-rust","title":"4. Install Rust","text":"<p>Follow the instructions at https://rustup.rs/</p>"},{"location":"contributing/set-up-guides/windows/#5-clone-the-repository","title":"5. Clone the Repository","text":"<pre><code>git clone https://github.com/mabel-dev/opteryx\n</code></pre>"},{"location":"contributing/set-up-guides/windows/#5-install-dependencies","title":"5. Install Dependencies","text":"<pre><code>python -m pip install --upgrade -r requirements.txt\n</code></pre> <pre><code>python -m pip install --upgrade setuptools setuptools_rust numpy cython\n</code></pre>"},{"location":"contributing/set-up-guides/windows/#6-build-binaries","title":"6. Build Binaries","text":"<pre><code>python setup.py build_ext --inplace\n</code></pre>"},{"location":"contributing/set-up-guides/windows/#running-tests","title":"Running Tests","text":"<p>To run the regression and unit tests:</p> <p>First, install the optional dependencies:</p> <pre><code>python -m pip install --upgrade -r tests/requirements.txt\n</code></pre> <p>Then run the regression tests.</p> <pre><code>python -m pytest\n</code></pre> <p>Note</p> <p>Some tests require external services like GCS and Memcached and may fail if these have not been configured.</p>"},{"location":"get-started/clients/","title":"Client Connectivity Overview","text":"<p>Opteryx supports two methods of invocation; as an importable Python library and as a command line tool.</p>"},{"location":"get-started/clients/#python-embedded","title":"Python Embedded","text":"<p>Opteryx is an embeddable package into Python applications, scripts and Notebooks which implements a partial Python DBAPI (PEP 249) interface.</p> <pre><code>import opteryx\nconn = opteryx.connect()\ncur = conn.cursor()\ncur.execute('SELECT * FROM $planets')\nrows = cur.fetchall()\n</code></pre> <p>The results of the query are availble via the cursor using <code>fetchone()</code> that returns a tuple, <code>fetchmany()</code> and <code>fetchall()</code> that return generators of tuples, or <code>arrow()</code> that returns an Arrow Table.</p>"},{"location":"get-started/clients/#command-line-interface","title":"Command Line Interface","text":"<p>Opteryx Command Line Interface (CLI) provides a terminal-based interactive shell for running queries. The CLI is a Python script usually run by invoking Python, like this:</p> <pre><code>python -m opteryx --o planets.csv \"SELECT * FROM \\$planets\"\n</code></pre> <p>Note: CLI usage may have character escaping requirements, such as a backslash before dollar signs and backticks.</p> <p>Querying individual files quires the relative path in place of the relation/table name in the query, this usually requires the filename to be put in quotes as filenames usually contain illegal characters.</p> <p>Abridged usage guidance is available below:</p> <pre><code>Usage: python -m opteryx [OPTIONS] [SQL] \n\n--ast         Display the AST for the query.\n--o &lt;target&gt;  Where to output the results. [default: console]\n--no-color    Do not colorize console output. \n--help        Show the full help details.          \n</code></pre> <p>To see the full help and usage details for the CLI use the <code>--help</code> option:</p> <pre><code>python -m opteryx --help\n</code></pre>"},{"location":"get-started/configuration-guide/","title":"Configuration Guide","text":""},{"location":"get-started/configuration-guide/#configuration-file","title":"Configuration File","text":"<p>Configuration values are set a <code>opteryx.yaml</code> file in the directory the application is run from.</p> Key Default Description <code>INTERNAL_BATCH_SIZE</code> 500 Batch size for left-table of a join processes <code>MAX_JOIN_SIZE</code> 10000 Maximum records created in a <code>CROSS JOIN</code> frame <code>MEMCACHED_SERVER</code> not set Address of Memcached server, in <code>IP:PORT</code> format <code>DATASET_PREFIX_MAPPING</code> not set Data store prefix mapping <code>PARTITION_SCHEME</code> none How the blob/file data is partitioned <code>MAX_SIZE_SINGLE_CACHE_ITEM</code> 1048576 The maximum size of an item to store in the cache <code>MAX_CACHE_EVICTIONS</code> 25 The maximum number of evictions from in-memory read cache per query <code>PAGE_SIZE</code> 67108864 The size to try to make data pages as they are processed <code>LOCAL_BUFFER_POOL_SIZE</code> 256 The size of the in-memory Buffer Pool (blob size) <code>DISABLE_HIGH_PRIORITY</code> False Disable trying to set the process priority"},{"location":"get-started/configuration-guide/#environment-variables","title":"Environment Variables","text":"<p>The environment is the preferred location for secrets, although the engine will read <code>.env</code> files if dotenv has also been installed.</p> <ul> <li><code>MONGO_CONNECTION</code></li> <li><code>MINIO_END_POINT</code></li> <li><code>MINIO_ACCESS_KEY</code></li> <li><code>MINIO_SECRET_KEY</code></li> <li><code>MINIO_SECURE</code></li> </ul>"},{"location":"get-started/configuration-guide/#caching-for-blob-stores","title":"Caching for Blob Stores","text":""},{"location":"get-started/configuration-guide/#read-cache","title":"Read Cache","text":"<p>The observed bottleneck for query performance is almost always IO. It is not uncommon for 90% of the execution time is initial load of data - this can vary considerably by storage and query.</p> <p>The Read Cache currently has one implementation, using Memcached.</p> <p>When your main storage is local disk, using a Read Cache is unlikely to provide significant performance improvement. However, when using remote storage, such as S3 or GCS, Read Cache can provide significant improvements. </p> <p>As will all optimization recommendations, test in your unique set of circumstances before assuming this to always be true.</p> <p>Read Cache is only used for Blob stores, and is not used for Document stores.</p>"},{"location":"get-started/configuration-guide/#buffer-pool","title":"Buffer Pool","text":"<p>The Buffer Pool is similar to the Read Cache with two key differences; </p> <ul> <li>The Buffer Pool is held in local memory.</li> <li>The Buffer Pool is a read-through for the Read Cache.</li> </ul> <p>That means that reads try to locate the fastest access to a blob in this order:</p> <p>1) Check the Buffer Pool (in memory) 2) Check the Read Cache (fast KV store) 3) Read from storage</p> <p>The Buffer Pool and Read Cache creates a storage heirarchy, where blobs more likely to be read (based on what has been read in the past) are more likely to be in a location which is faster to read.</p> <p>Buffer Pools are local to the asset serving the requests, in a serverless environment this asset may only serve one or two requests before terminating. If your environment is configured like this, have a small Buffer Pool and a large Read Cache. If assets serve 10s or more requests before terminating, have a large Buffer Cache.</p> <p>As will all optimization recommendations, test in your unique set of circumstances before assuming this to always be true.</p> <p>Buffer Pool is only used for Blob stores, and is not used for Document stores.</p>"},{"location":"get-started/deployment-guide/","title":"Deployment Guide","text":""},{"location":"get-started/deployment-guide/#requirements","title":"Requirements","text":""},{"location":"get-started/deployment-guide/#host-specifications","title":"Host Specifications","text":"<p>Minimum: 1 Gb RAM, 1 CPU (x86)  Recommended: 8 Gb RAM, 4 CPUs (x86)</p> <p>Opteryx balances memory consumption with performance, however, being able to process large datasets will require larger memory specifications compared to what is needed to process smaller datasets. The reference implementation of Opteryx regularly processes 100Gb of data in a container with 4 CPUs and 8Gb of memory allocated.</p> <p>Note</p> <p>This is a general recommendation and is a good place to start, your environment and specific problem may require, or perform significantly better, with a different configuration.</p> <p>Note</p> <p>Opteryx contains no specific optimiations to make use of multiple CPUs, although multiple CPUs may be beneficial as some libraries Opteryx is built on may use multiple CPUs.</p> <p>Warning</p> <p>Non x86 environments, such as Raspberry Pi or the M1 Macs, may require additional set up steps.</p>"},{"location":"get-started/deployment-guide/#operating-system-support","title":"Operating System Support","text":"<p>Recommended Operating System: Ubuntu 20 (64bit)</p> <p>Opteryx can be installed and deployed on a number of different platforms. It has heavy dependency on Apache Arrow and cannot be run on systems which do not support Arrow.</p> <p>The full regression suite is run on Ubuntu (Ubuntu 20.04) for Python versions 3.8, 3.9, 3.10 and 3.11. The below table shows regression suite coverage:</p> OS Python 3.8 Python 3.9 Python 3.10 Python 3.11 MacOS (x86/Intel) Partial Partial Partial Partial Windows (x86) Partial Partial Partial Partial Ubuntu (x86) Full Full Full Full Debian (ARM) None Partial None None <p> Full - no tests are excluded from the test suite - coverage statistics are from Full tests. Partial - some tests are excluded from the test suite or that some tests fail. None - there is no automated test for this configuration.  </p> <p>Note</p> <ul> <li>PyPy regression suite fails due to issues with Apache Arrow.</li> <li>MacOs (M1) is not included in the regression suite due to lack of support on the test platform, but there is known usage on this configuration.</li> <li>Windows (ARM) is not included in the regression suite  due to lack of support on the test platform.</li> <li>Partial coverage is primarily due to testing platform constraints, not core-compatibility issues.</li> </ul>"},{"location":"get-started/deployment-guide/#python-environment","title":"Python Environment","text":"<p>Recommended Version: 3.9</p> <p>Opteryx supports Python versions 3.8, 3.9, 3.10 and 3.11.</p> <p>Opteryx has builds for Python 3.8, 3.9, 3.10 and 3.11 on 64-bit (x86) versions of Windows, MacOS and Linux and ARM versions of MacOS and Linux.</p> <p>Opteryx is primarily developed on workstations running Python 3.10 (Debian, MacOS), is known to be deployed in production environments running Python 3.9 (Debian). Python 3.9 has the greatest test coverage due to it being supported on more platforms.</p>"},{"location":"get-started/deployment-guide/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Opteryx can run in Jupyter Notebooks to access data locally or, if configured, remotely on systems like GCS and S3.</p> <p>It is recommended that the Notebook host is located close to the data being queried - such as running Vertex AI Notebooks if the data sources are primarily on GCP, or querying local files if running Jupyter on a local device. Other configurations will work, but are less optimal.</p>"},{"location":"get-started/deployment-guide/#docker-kubernetes","title":"Docker &amp; Kubernetes","text":"<p>There is no Docker image for Opteryx, this is because Opteryx is an embedded Python library. However, system built using Opteryx can be deployed via Docker or Kubernetes.</p>"},{"location":"get-started/deployment-guide/#google-cloud","title":"Google Cloud","text":"<p>Cloud Run</p> <p>Opteryx is well-suited for running data manipulation tasks in Cloud Run as this was the target platform for the initial development.</p> <p>Running in the Generation 2 container environment is likely to result in faster query processing, but has a slower start-up time. Opteryx runs in Generation 1 container, taking approximately 10% longer to execute queries.</p>"},{"location":"get-started/deployment-guide/#data-storage","title":"Data Storage","text":""},{"location":"get-started/deployment-guide/#connectors","title":"Connectors","text":"<p>Built-In Connectors</p> Platform Connector Name Implementation Google Cloud Storage GcpCloudStorageConnector Blob/File Store AWS S3 AwsS3Connector Blob/File Store MinIo AwsS3Connector Blob/File Store Google FireStore GcpFireStoreConnector Document Store MongoDB MongoDbConnector Document Store Local Disk DiskConnector Blob/File Store <p>Connectors are registered with the storage engine using the <code>register_store</code> method. Multiple prefixes can be added, using different connectors - multiple storage types can be combined into a single query.</p> <pre><code>opteryx.storage.register_store(\"tests\", DiskConnector)\n</code></pre> <p>A more complete example using the <code>register_store</code> method to set up a connector to Google Cloud Storage (GCS) and then query data on GCS is below:</p> <pre><code>import opteryx\nfrom opteryx.connectors import GcpCloudStorageConnector\n# Tell the storage engine that datasets with the prefix 'your_bucket'\n# are to be read using the GcpCloudStorageConnector connector.\n# Multiple prefixes can be added and do not need to be the same\n# connector.\nopteryx.register_store(\"your_bucket\", GcpCloudStorageConnector)\nconnextion = opteryx.connect()\ncursor = connection.cursor()\ncursor.execute(\"SELECT * FROM your_bucket.folder;\")\nprint(cursor.fetchone())\n</code></pre>"},{"location":"get-started/deployment-guide/#blobfile-stores","title":"Blob/File Stores","text":""},{"location":"get-started/deployment-guide/#datasets","title":"Datasets","text":"<p>Opteryx references datasets using their relative path as the table name. For example in the following folder structure</p> <pre><code>/\n \u251c\u2500 products/\n \u251c\u2500 customers/\n \u2502   \u251c\u2500 profiles/\n \u2502   \u2514\u2500 preferences/\n \u2502       \u251c\u2500 marketing/\n \u2502       \u2514\u2500 site/\n \u2514\u2500\u2500 purchases/ \n</code></pre> <p>Would have the following datasets available (assuming leaf folders have data files within them)</p> <ul> <li>products</li> <li>customers.profiles</li> <li>customers.preferences.marketing</li> <li>customers.preferences.site</li> <li>purchases</li> </ul> <p>These are queryable like this:</p> <pre><code>SELECT *\nFROM customers.profiles\n</code></pre>"},{"location":"get-started/deployment-guide/#temporal-structures","title":"Temporal Structures","text":"<p>To enable temporal queries, data must be structured into date hierarchy folders below the dataset folder. Using just the products dataset from the above example, below the products folder must be year, month and day folders like this:</p> <pre><code>/\n \u2514\u2500 products/\n     \u2514\u2500 year_2022/\n         \u2514\u2500 month_05/\n             \u2514\u2500 day_01/\n</code></pre> <p>To query the data for today with this structure, you can execute:</p> <pre><code>SELECT *\nFROM products\n</code></pre> <p>To query just the folder shown in the example (1st May 2022), you can execute:</p> <pre><code>SELECT *\nFROM products\nFOR '2022-05-01'\n</code></pre> <p>This is the default structure created by Mabel and within Opteryx this is called Mabel Partitioning.</p>"},{"location":"get-started/deployment-guide/#file-types","title":"File Types","text":"<p>Opteryx is primarily designed for use with Parquet to store data, Parquet is fast to process and offers optimizations not available for other formats, however, in some benchmarks ORC out performs Parquet.</p> <p>Opteryx supports:</p> <ul> <li>Parquet formatted files (<code>.parquet</code>)</li> <li>CSV formatted files (<code>.csv</code>)</li> <li>Tab delimited files (<code>.tsv</code>)</li> <li>JSONL formatted files (<code>.jsonl</code>)</li> <li>JSONL formatted files which have been Zstandard compressed (<code>.zstd</code>)</li> <li>ORC formatted files (<code>.orc</code>)</li> <li>Feather (Arrow) formatted files (<code>.arrow</code>)</li> <li>Avro formatted files (<code>.avro</code>)</li> </ul> <p>Note</p> <ul> <li>ORC is not fully supported on Windows environments</li> <li>CSV and TSV support is limited and is not recommended beyond trivial usage</li> <li>Avro is not recommended for use in performance-sensitive contexts</li> </ul>"},{"location":"get-started/deployment-guide/#file-sizes","title":"File Sizes","text":"<p>Opteryx loads entire files (pages) into memory one at a time, this requires the following to be considered:</p> <ul> <li>Reading one record from a file loads the entire page. If you regularly only read a few records, prefer smaller pages.</li> <li>Reading each page, particularly from Cloud Storage (S3/GCS), incurs a per-read overhead. If you have large datasets, prefer larger pages.</li> </ul> <p>If you are unsure where to start, 64Mb (before compression) is a recommended general-purpose page size.</p>"},{"location":"get-started/get-started/","title":"Get Started","text":""},{"location":"get-started/get-started/#installation","title":"Installation","text":"<p>Install from PyPI (recommended)</p> <p>This will install the latest release version.</p> <pre><code>pip install --upgrade opteryx\n</code></pre> <p>Install from GitHub</p> <p>The lastest version, including pre-release and beta versions can be installed, this is not recommended for production environments.</p> <pre><code>pip install git+https://github.com/mabel-dev/opteryx\n</code></pre>"},{"location":"get-started/get-started/#your-first-query","title":"Your First Query","text":"<p>You can quickly test your installation is working as expected by querying one of the internal sample datasets.</p> <pre><code>import opteryx\nconn = opteryx.connect()\ncur = conn.cursor()\ncur.execute(\"SELECT * FROM $planets;\")\nprint(cur.arrow())\n</code></pre>"},{"location":"get-started/metastore/","title":"Metastore","text":"<p>IN DEVELOPMENT</p> <p>Tarchia is in development and will be the metastore provider for Mabel and Opteryx.</p> <p>It has two key parts:</p> <ul> <li>The Data Catalogue</li> <li>Data Profiles</li> </ul>"},{"location":"get-started/metastore/#the-data-catalogue","title":"The Data Catalogue","text":"<p>Used by Mabel to look up schema and data quality tests to apply to data as it is being written.</p> <p>Used by Opteryx to for schema information for schema evolution and query planning</p>"},{"location":"get-started/metastore/#data-profiles","title":"Data Profiles","text":"<p>Used by Opteryx to respond to simple queries</p> <p>Used by Opteryx to make cost estimations for queries</p>"},{"location":"get-started/overview/","title":"Opteryx Overview","text":"<p>Opteryx is a SQL query engine to query large data sets designed to run in low-cost serverless environments.</p> <p>Opteryx is not a database, it does understand and respond to a subset of SQL statements like a database, but it does not support any activities which insert, update or delete data. It is not a replacement for databases like SQL Server, MySQL or Postgres, it is designed to allow querying of data sources as part of an data analytics process.</p>"},{"location":"get-started/overview/#use-cases","title":"Use Cases","text":"<p>Where you need to query data across different data platforms but don't want the cost or effort to move this data to a common platform.</p> <p>Great for use in cost-optimized environments, where a traditional data solution like Hadoop is out of reach.</p> <p>Querying ad hoc data stores for third-party systems, such as querying logs output to storage.</p> <p>Where you have many different environments, and each would require their own database to query static data.</p> <p>Where you occassionally query data, and don't want the effort of loading into a database or the cost of maintaining a database for infrequent use.</p> <p>Where time to respond to queries is not time sensitive.</p>"},{"location":"get-started/python-client/","title":"Python Client","text":"<p>This document describes the Python objects intended to be accessed by users of the library. The library has many other components and interfaces which can be called, accessing these is generally not recommended.</p> class Connection () cursor () return a cursor object close () exists for interface compatibility only commit () exists for interface compatibility only rollback () exists for interface compatibility only class Cursor (connection) id () The unique internal reference for this query execute (operation, params) rowcount () shape () stats () execution statistics messages () list of run-time warnings fetchone (as_dicts) Fetch one record only. <p>Parameters</p><ul><li>as_dicts:  boolean (optional) Return a dictionary, default is False, return a tuple</li></ul> fetchmany (size, as_dicts) fetch a given number of records fetchall (as_dicts) fetch all matching records arrow (size) Fetch the resultset as a pyarrow table, this is generally the fastest way to get the entire set of results.  <p>Parameters</p><ul><li>size:  int (optional) Return the head 'size' number of records.</li> <p>Returns</p><ul><li>pyarrow.Table to_df (size) Fetch the resultset as a pandas DataFrame.  <p>Parameters</p><ul><li>size:  int (optional) Return the head 'size' number of records.</li> <p>Returns</p><ul><li>pandas.DataFrame close () close the connection head (size) <p>This file has been automatically generated from the source code.</p>"},{"location":"get-started/schema-evolution/","title":"Schema Evolution","text":"<p>Opteryx has support for in-place relation evolution. You can evolve a table schema or change a partition layout without requiring existing data to be rewritten or migrated to a new dataset.</p> <p>Schema of the data is determined by the first page read to respond to a query, new columns are removed and missing columns are null-filled. This allows graceful handling of pages with different schemas, but may result in the appearance of missing data as columns not found in the first page are removed.</p> <p>Opteryx supports the following schema evolution changes:</p> <ul> <li>Add - new columns can be added - these are removed if not present on the first page read</li> <li>Remove - removed columns are null-filled</li> <li>Reorder - the order of columns can be changed</li> <li>Partitioning - partition resolution can be changed</li> </ul> <p>Note</p> <p>Renamed columns will behave like the column has been removed and a new column added.</p> <p>Opteryx has limited support for column types changing, some changes within the same broad type (e.g. between numeric types and date resolutions) are supported, but these are not all supported and changing between types is not supported.</p>"},{"location":"get-started/schema-evolution/#partitioning","title":"Partitioning","text":"<p>Changes to partition schemes are handled transparently. For example, data using Mabel partitioning moving from a daily to an hourly partition layout can occur without requiring any other changes to the configuration of the query engine. However, moving between no partition and partitioning (or vise-versa) is not supported.</p>"},{"location":"get-started/external-standards/pep249/","title":"Python PEP-249 Conformity","text":"<p>Note</p> <p>Compliance is a work in progress and this information represents the current state in order to provide transparent information about progress and capability.</p> <p>Opteryx is not a DBMS so only aims for confirmity to PEP-249 for the featureset it supports - this is primarily support related to the querying of data.</p>"},{"location":"get-started/external-standards/pep249/#module-interface","title":"Module Interface","text":"Feature Imperative Supported Module Interface uknown <code>connect</code> constructor must unknown <code>apilevel</code> global must unknown <code>threadsafety</code> global must unknown <code>paramstyle</code> global must unknown <code>Warning</code> exception should unknown <code>Error</code> exception should unknown <code>InterfaceError</code> exception should unknown <code>DatabaseError</code> exception should unknown <code>DataError</code> exception should unknown <code>OperationalError</code> exception should unknown <code>IntegrityError</code> exception should unknown <code>InternalError</code> exception should unknown <code>ProgrammingError</code> exception should unknown <code>NotSupportedError</code> exception should unknown Connection Object unknown <code>close</code> method should unknown <code>commit</code> method should unknown <code>rollback</code> method should unknown <code>cursor</code> method should unknown <code>messages</code> attribute optional no <code>errorhandler</code> attribute optional no Cursor Object unknown <code>description</code> attribute should no <code>rowcount</code> attribute should unknown <code>callproc</code> method optional n/a <code>close</code> method should unknown <code>execute</code> method should unknown <code>executemany</code> method should no <code>fetchmany</code> method should unknown <code>fetchall</code> method should unknown <code>nextset</code> method optional n/a <code>arraysize</code> attribute should unknown <code>setinputsizes</code> method should no <code>setoutputsize</code> method should no <code>rownumber</code> attribute optional unknown <code>connection</code> attribute optional unknown <code>scroll</code> method optional no <code>messages</code> attribute optional no <code>next</code> method optional unknown <code>__iter__</code> method optional no <code>lastrowid</code> attribute optional no <code>errorhandler</code> attribute optional no Type Constructors unknown <code>Date</code> optional unknown <code>Time</code> optional unknown <code>Timestamp</code> optional unknown <code>DateFromTicks</code> optional unknown <code>TimeFromTicks</code> optional unknown <code>TimestampFromTicks</code> optional unknown <code>Binary</code> optional unknown <code>STRING</code> optional unknown <code>BINARY</code> optional unknown <code>NUMBER</code> optional unknown <code>DATETIME</code> optional unknown <code>ROWID</code> optional unknown Two-Phase Commit Extensions n/a <code>xid</code> method optional n/a <code>tpc_begin</code> method optional n/a <code>tpc_prepare</code> method optional n/a <code>tpc_commit</code> method optional n/a <code>tpc_rollback</code> method optional n/a <code>tpc_recover</code> method optional n/a <p>Support statuses in these tables:</p> <p> yes The feature is supported and conformance is part of the test suite. no The feature is not supported. partial Some features are supported. n/a The feature relates to a feature not supported by Opteryx. unknown No test exists to confirm conformance.  </p>"},{"location":"get-started/external-standards/sql92/","title":"ANSI SQL-92 Conformity","text":"<p>Note</p> <p>Compliance is a work in progress and this information represents the current state in order to provide transparent information about progress and capability.</p> <p>For a system to attest to supporting SQL it should have good conformance to the ANSI SQL-92 standard. This standard is also known as ISO/IEC 9075:1992.</p> <p>Opteryx is not a DBMS, so only aims for confirmity to the subset of SQL-92 for to featureset it supports - this is primarily related to <code>SELECT</code> statements.</p> Function Description Support E011 Numeric data types partial E011-01 <code>INTEGER</code> and <code>SMALLINT</code> data types yes E011-02 <code>REAL</code>, <code>DOUBLE PRECISION</code>, and <code>FLOAT</code> data types partial E011-03 <code>DECIMAL</code> and <code>NUMERIC</code> data unknown E011-04 Arithmetic yes E011-05 Numeric comparison yes E011-06 Implicit casting among the numeric data types yes E021 Character string types unknown E021-01 <code>CHARACTER</code> data type unknown E021-02 <code>CHARACTER VARYING</code> data type unknown E021-03 Character literals unknown E021-04 <code>CHARACTER_LENGTH</code> function no E021-05 <code>OCTET_LENGTH</code> no E021-06 <code>SUBSTRING</code> function partial E021-07 Character concatenation yes E021-08 <code>UPPER</code> and <code>LOWER</code> functions yes E021-09 <code>TRIM</code> function yes E021-10 Implicit casting among the fixed-length and variable-length character string types unknown E021-11 <code>POSITION</code> function partial E021-12 Character comparison yes E031 Identifiers unknown E031-01 Delimited identifiers unknown E031-02 Lower case identifiers unknown E031-03 Trailing underscore unknown E051 Basic query specification unknown E051-01 <code>SELECT DISTINCT</code> unknown E051-02 <code>GROUP BY</code> clause unknown E051-04 <code>GROUP BY</code> can contain columns not in select-list unknown E051-05 Select list items can be renamed unknown E051-06 <code>HAVING</code> clause unknown E051-07 Qualified <code>*</code> in select list unknown E051-08 Correlation names in the <code>FROM</code> clause unknown E051-09 Rename columns in the <code>FROM</code> clause unknown E061 Basic predicates and search conditions unknown E061-01 Comparison predicate unknown E061-02 <code>BETWEEN</code> predicate unknown E061-03 <code>IN</code> predicate with list of values unknown E061-04 <code>LIKE</code> predicate unknown E061-05 <code>LIKE</code> predicate: <code>ESCAPE</code> clause unknown E061-06 <code>NULL</code> predicate unknown E061-07 Quantified comparison predicate unknown E061-08 <code>EXISTS</code> predicate no E061-09 Subqueries in comparison predicate unknown E061-11 Subqueries in <code>IN</code> predicate unknown E061-12 Subqueries in quantified comparison predicate unknown E061-13 Correlated subqueries unknown E061-14 Search condition unknown E071 Basic query expressions unknown E071-01 <code>UNION DISTINCT</code> table operator no E071-02 <code>UNION ALL</code> table operator no E071-03 <code>EXCEPT DISTINCT</code> table operator no E071-05 Columns combined via table operators need not have exactly the same data type unknown E071-06 Table operators in subqueries unknown E081 Basic Privileges unknown E081-01 <code>SELECT</code> privilege at the table level unknown E081-02 <code>DELETE</code> privilege n/a E081-03 <code>INSERT</code> privilege at the table level n/a E081-04 <code>UPDATE</code> privilege at the table level n/a E081-05 <code>UPDATE</code> privilege at the column level n/a E081-06 <code>REFERENCES</code> privilege at the table level n/a E081-07 <code>REFERENCES</code> privilege at the column level n/a E081-08 <code>WITH GRANT OPTION</code> n/a E081-09 <code>USAGE</code> privilege n/a E081-10 <code>EXECUTE</code> privilege n/a E091 Set functions partial E091-01 <code>AVG</code> yes E091-02 <code>COUNT</code> yes E091-03 <code>MAX</code> yes E091-04 <code>MIN</code> yes E091-05 <code>SUM</code> yes E091-06 <code>ALL</code> quantifier unknown E091-07 <code>DISTINCT</code> quantifier unknown E101 Basic data manipulation n/a E101-01 <code>INSERT</code> statement n/a E101-03 Searched <code>UPDATE</code> statement n/a E101-04 Searched <code>DELETE</code> statement n/a E111 Single row SELECT statement unknown E121 Basic cursor support unknown E121-01 <code>DECLARE CURSOR</code> no E121-02 <code>ORDER BY</code> columns need not be in select yes E121-03 Value expressions in <code>ORDER BY</code> clause unknown E121-04 <code>OPEN</code> statement unknown E121-06 Positioned <code>UPDATE</code> statement no E121-07 Positioned <code>DELETE</code> statement no E121-08 <code>CLOSE</code> statement no E121-10 <code>FETCH</code> statement: implicit <code>NEXT</code> no E121-17 <code>WITH HOLD</code> cursors no E131 Null value support yes E141 Basic integrity constraints unknown E141-01 <code>NOT NULL</code> constraints n/a E141-02 <code>UNIQUE</code> constraints of <code>NOT NULL</code> n/a E141-03 <code>PRIMARY KEY</code> constraints n/a E141-04 Basic <code>FOREIGN KEY</code> constraint with the <code>NO ACTION</code> default for both referential delete action and referential update action n/a E141-06 <code>CHECK</code> constraints n/a E141-07 Column defaults unknown E141-08 <code>NOT NULL</code> inferred on <code>PRIMARY KEY</code> n/a E141-10 Names in a foreign key can be specified in any order n/a E151 Transaction support n/a E151-01 <code>COMMIT</code> statement n/a E151-02 <code>ROLLBACK</code> statement n/a E152 Basic <code>SET TRANSACTION</code> statement n/a E152-01 <code>SET TRANSACTION</code> statement: <code>ISOLATION LEVEL SERIALIZABLE</code> clause n/a E152-02 <code>SET TRANSACTION</code> statement: <code>READ ONLY</code> and <code>READ WRITE</code> clauses n/a E+ Other unknown E153 Updatable queries with subqueries unknown E161 SQL comments using leading double minus yes E171 <code>SQLSTATE</code> support unknown E182 Host language binding unknown F021 Basic information schema unknown F021-01 <code>COLUMNS</code> view unknown F021-02 <code>TABLES</code> view unknown F021-03 <code>VIEWS</code> view unknown F021-04 <code>TABLE_CONSTRAINTS</code> view unknown F021-05 <code>REFERENTIAL_CONSTRAINTS</code> view unknown F021-06 <code>CHECK_CONSTRAINTS</code> view unknown F031 Basic schema manipulation unknown F031-01 <code>CREATE TABLE</code> statement to create persistent base tables n/a F031-02 <code>CREATE VIEW</code> statement n/a F031-03 <code>GRANT</code> statement unknown F031-04 <code>ALTER TABLE</code> statement: <code>ADD COLUMN</code> clause n/a F031-13 <code>DROP TABLE</code> statement: <code>RESTRICT</code> clause n/a F031-16 <code>DROP VIEW</code> statement: <code>RESTRICT</code> clause n/a F031-19 <code>REVOKE</code> statement: <code>RESTRICT</code> clause unknown F041 Basic joined table unknown F041-01 Inner join (but not necessarily the <code>INNER</code> keyword) unknown F041-02 <code>INNER</code> keyword unknown F041-03 <code>LEFT OUTER JOIN</code> unknown F041-04 <code>RIGHT OUTER JOIN</code> unknown F041-05 Outer joins can be nested unknown F041-07 The inner table in a left or right outer join can also be used in an inner join unknown F041-08 All comparison operators are supported (rather than just <code>=</code>) unknown F051 Basic date and time unknown F051-01 <code>DATE</code> data type (including support of <code>DATE</code> literal) unknown F051-02 <code>TIME</code> data type (including support of <code>TIME</code> literal) with fractional seconds unknown F051-03 <code>TIMESTAMP</code> data type (including support of <code>TIMESTAMP</code> literal) with fractional seconds precision of at least 0 and 6 unknown F051-04 Comparison predicate on <code>DATE</code>, <code>TIME</code>, and <code>TIMESTAMP</code> data types partial F051-05 Explicit <code>CAST</code> between datetime types and character string types unknown F051-06 <code>CURRENT_DATE</code> yes F051-07 <code>LOCALTIME</code> unknown F051-08 <code>LOCALTIMESTAMP</code> unknown F081 <code>UNION</code> and <code>EXCEPT</code> in views unknown F131 Grouped operations unknown F131-01 <code>WHERE</code>, <code>GROUP BY</code>, and <code>HAVING</code> clauses supported in queries with grouped views unknown F131-02 Multiple tables supported in queries with grouped views unknown F131-03 Set functions supported in queries with grouped views unknown F131-04 Subqueries with <code>GROUP BY</code> and <code>HAVING</code> clauses and grouped views unknown F131-05 Single row <code>SELECT</code> with <code>GROUP BY</code> and <code>HAVING</code> clauses and grouped views unknown F181 Multiple module support unknown F201 <code>CAST</code> function unknown F221 Explicit defaults unknown F261 <code>CASE</code> expression yes F261-01 Simple <code>CASE</code> yes F261-02 Searched <code>CASE</code> yes F261-03 <code>NULLIF</code> yes F261-04 <code>COALESCE</code> yes F311 Schema definition statement n/a F311-01 <code>CREATE SCHEMA</code> n/a F311-02 <code>CREATE TABLE</code> n/a F311-03 <code>CREATE VIEW</code> n/a F311-04 <code>CREATE VIEW</code>: <code>WITH CHECK OPTION</code> n/a F311-05 <code>GRANT</code> statement n/a F471 Scalar subquery values unknown F481 Expanded <code>NULL</code> predicate unknown F501 Features and conformance views unknown F501-01 <code>SQL_FEATURES</code> view unknown F501-02 <code>SQL_SIZING</code> view unknown F501-03 <code>SQL_LANGUAGES</code> view unknown F812 Basic flagging unknown S011 Distinct data types unknown S011-01 <code>USER_DEFINED_TYPES</code> view no T321 Basic SQL-invoked routines unknown T321-01 User-defined functions with no overloading unknown T321-02 User-defined stored procedures with no overloading unknown T321-03 Function invocation unknown T321-04 <code>CALL</code> statement unknown T321-05 <code>RETURN</code> statement unknown T321-06 <code>ROUTINES</code> view unknown T321-07 <code>PARAMETERS</code> view unknown T631 <code>IN</code> predicate with one list element unknown <p>Support statuses in this table:</p> <p> yes The feature is supported and conformance is part of the test suite. no The feature is not supported. partial Some features are supported. n/a The feature relates to a feature not supported by Opteryx. unknown No test exists to confirm conformance.  </p>"},{"location":"get-started/external-standards/tpch/","title":"TPC-H Benchmark","text":"<p>Note</p> <p>Compliance is a work in progress and this information represents the current state in order to provide transparent information about progress and capability.</p> <p>TPC publish a set of benchmarks for computer and database systems. TPC-H is a decision support benchmark which consists of a set of business-oriented queries. This benchmark demonstrates a decision support system that examines large amounts of data, executes highly complex queries, and answers key business questions. </p> Query Modified Pass query1 no yes query2 yes no query3 no no query4 no no query5 no no query6 no yes query7 no no query8 no no query9 no no query10 no no query11 yes no query12 no no query13 no no query14 no unknown query15 yes no query16 no no query17 no no query18 yes no query19 no unknown query20 no no query21 yes no query22 yes no <p>All queries have been modified to refer to the location of the datasets, modified in the above table is where the SQL has been written to replace unsupported functionality with supported functionality - this is where the original query either created a view or a temporary table, both of these have been replaced with a CTE definition.</p> <p>The test suite for this benchmark is in the Opteryx Benchmarking repository.</p>"},{"location":"get-started/release-notes/change-log/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file, where appropriate the GitHub issue reference will be noted along with the change. Breaking changes will be clearly indicated with the  icon.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"get-started/release-notes/change-log/#unreleased-","title":"[Unreleased] -","text":""},{"location":"get-started/release-notes/change-log/#fixed","title":"Fixed","text":"<ul> <li>[#782] Support literal predicates in <code>JOIN</code> conditions. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed","title":"Changed","text":"<ul> <li>Updated sqlparser-rs to version 0.30.0 @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added","title":"Added","text":"<ul> <li>[#521] Query files directly. @joocer </li> <li>[#786] Save dataset as pandas DataFrame. @joocer </li> <li>[#787] Run queries against pandas DataFrames. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#082-2023-01-06","title":"[0.8.2] - 2023-01-06","text":""},{"location":"get-started/release-notes/change-log/#fixed_1","title":"Fixed","text":"<ul> <li>[#757] Multiple bugs in config manager. @joocer </li> <li>[#769] <code>ARRAY_AGG</code> couldn't be nested. @joocer </li> <li>[#775] Connection <code>arrow</code> materializes before applying limit. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_1","title":"Changed","text":"<ul> <li>Internal refactoring relating to creation of metadata service. @joocer </li> <li>Updated sqlparser-rs to version 0.29.0 @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added_1","title":"Added","text":"<ul> <li>[#750] CLI improvements. @joocer </li> <li>[#758] Support AVRO and TSV formatted files. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#081-2022-12-30","title":"[0.8.1] - 2022-12-30","text":""},{"location":"get-started/release-notes/change-log/#fixed_2","title":"Fixed","text":"<ul> <li>[#754] Occassional segfaults on Aggregates. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#080-2022-12-27","title":"[0.8.0] - 2022-12-27","text":""},{"location":"get-started/release-notes/change-log/#fixed_3","title":"Fixed","text":"<ul> <li>[#703] <code>ORDER BY</code> columns not in <code>SELECT</code> clause. @joocer </li> <li>[#712] Aggregates on literals when combined with a <code>GROUP BY</code> clause. @joocer </li> <li>[#710] <code>SEARCH</code> mishandles pages with empty values in first row. @joocer </li> <li>[#711] <code>DATE_TRUNC</code> is case sensitive. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_2","title":"Changed","text":"<ul> <li>[#707] First try to estimate unique values using the Distogram for <code>SHOW EXTENDED COLUMNS</code>. @joocer </li> <li>[#707] <code>SHOW EXTENDED COLUMNS</code> creates histograms of 20 bins. @joocer </li> <li>[#707] Distogram (data profiler) significant performance improvements. @joocer </li> <li>[#722] Allow temporal <code>FOR</code> after alias <code>AS</code> clauses. @joocer </li> <li>[#743] 'Did you mean' prompt for columns better suggestions when casing is different. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added_2","title":"Added","text":"<ul> <li>[#515] Implement various new functions. @joocer </li> <li>[#19] Initial support for CTE expressions. @joocer </li> <li>[#204] Initial support predicate pushdowns. @joocer </li> <li>[#721] Improved temporal range error messages. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#070-2022-12-02","title":"[0.7.0] - 2022-12-02","text":""},{"location":"get-started/release-notes/change-log/#fixed_4","title":"Fixed","text":"<ul> <li>[#653] <code>LIKE</code> and <code>FOR</code> clauses cannot coexist in <code>SHOW</code> queries. @joocer </li> <li>[#669] <code>COUNT(*)</code> cannot be mixed with other aggregates. @joocer </li> <li>[#518] <code>SELECT *</code> and <code>GROUP BY</code> can't be used together. @joocer </li> <li>[#689] <code>IS</code> comparisons cannot be combined with other comparisons when optimization is off. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_3","title":"Changed","text":"<ul> <li>Updated sqlparser-rs to version 0.27.0 @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added_3","title":"Added","text":"<ul> <li>[#629] Optimizer pre-evaluates constant expressions. @joocer </li> <li>[#439] Support <code>SHOW STORES</code>. @joocer </li> <li>[#542] Support <code>POSITION</code>. @joocer </li> <li>[#22] Support <code>CASE</code> statements. @joocer </li> <li>[#665] Partial support of <code>ARRAY_AGG</code> function. @joocer </li> <li>[#668] Optimizer exchanges functions with constant results. @joocer </li> <li>[#300] Support advanced <code>TRIM</code> syntax. @joocer </li> <li>[#570] Optimizer implements De Morgan's Law. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#060-2022-11-08","title":"[0.6.0] - 2022-11-08","text":""},{"location":"get-started/release-notes/change-log/#fixed_5","title":"Fixed","text":"<ul> <li>[#568] Unable to perform aggregates on literals. @joocer </li> <li>[#592] Dates not always handled correctly. @joocer </li> <li>[#600] Parameterization when used on query batches fails. @joocer </li> <li>[#580] Empty result sets have no column information. @joocer </li> <li>[#548] 'did you mean' message restored for dataset <code>WITH</code> hints. @joocer </li> <li>[#640] <code>COUNT(*)</code> shortcut only used when in uppercase. @joocer </li> <li> [#645] (correction) <code>null</code> values not handled correctly in comparisions. @joocer </li> <li>Problem installing on M1 Mac. @joocer </li> <li>Support <code>AND</code>, <code>OR</code>, and <code>XOR</code> in <code>SELECT</code> statement. @joocer </li> <li>[#646] Temporal clauses in incorrect place were ignored @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_4","title":"Changed","text":"<ul> <li>[#566] Change from using SQLite3 to DuckDB for SQL comparision tests in Wrenchy-Bench. @joocer </li> <li> [#584] (clarity) <code>enable_page_management</code> configuration and parameter renamed <code>enable_page_defragmentation</code> with some minor refactoring of approach to defragmentation. @joocer </li> <li> (alignment) <code>TIMESTAMP</code> casting no longer supports casting from a number. @joocer </li> <li>[#588] Integrate sqloxide into Opteryx to reduce lag with sqlparser-rs updates. @joocer </li> <li>[#619] Page defragmentation moved to an Operator and positioned by the Optimizer. @joocer </li> <li> (correction) cursor 'fetch*' methods return Python tuple, rather than Python lists. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added_4","title":"Added","text":"<ul> <li>[#533] Support <code>LIKE</code> on <code>SHOW FUNCTIONS</code>, see sqlparser-rs/#620. @joocer </li> <li>[#570] Query Optimizer rule to reduce steps in expression evaluation by partial elimination of negatives. @joocer </li> <li>[#129] Support <code>FOR</code> clauses for all datasets. @joocer </li> <li>[#543] Support 'type string' notation for casting values. @joocer </li> <li>[#596] Optimizer replaces <code>ORDER BY</code> and <code>LIMIT</code> plan steps with a single 'HeapSort' plan step. @joocer </li> <li>[#515] <code>NULLIF</code> function. @joocer </li> <li>[#581] New SQL Battery test that tests results, and initial set of tests. @joocer </li> <li>[#577] Hierarchical buffer pool and configuration. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#050-2022-10-02","title":"[0.5.0] - 2022-10-02","text":""},{"location":"get-started/release-notes/change-log/#fixed_6","title":"Fixed","text":"<ul> <li>[#528] <code>.shape</code> and <code>.count</code> not working as expected. @joocer </li> <li>Numbers expressed in the form <code>+n</code> not parsed correctly. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_5","title":"Changed","text":"<ul> <li> (alignment) <code>as_arrow</code> renamed to <code>arrow</code> to align to DuckDB naming. @joocer </li> <li> (consistency) <code>SHOW COLUMNS</code> returns the column name in the <code>name</code> column, previously <code>column_name</code> @joocer </li> <li> (correction) cursor 'fetch*' methods returns tuples rather than dictionaries as defaults, this is correcting a bug in PEP249 compatibility. @joocer </li> <li> [#517] (security) Placeholder changed from '%s' to '?'. @joocer </li> <li>[#522] Implementation of LRU-K(2) for cache evictions. @joocer </li> <li>[#537] Significant refactor of Query Planner. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#added_5","title":"Added","text":"<ul> <li>[#397] Time Travel with '$planets' dataset. @joocer </li> <li>[#519] Introduce a size limit on <code>as_arrow()</code>. @joocer </li> <li>[#324] Support <code>IN UNNEST()</code>. @joocer </li> <li>[#386] Support <code>SET</code> statements. @joocer </li> <li>[#531] Support <code>SHOW VARIABLES</code> and <code>SHOW PARAMETERS</code>. @joocer </li> <li>[#464] Support <code>LEFT JOIN &lt;relation&gt; USING</code> @joocer </li> <li>[#402] <code>INNER JOIN ON</code> supports multiple conditions @joocer </li> <li>[#551] Document stores (MongoDb + FireStore) return '_id' column holding string version of document ID. @joocer </li> <li>[#532] Runtime parameters are able to be altered using the <code>SET</code> statement. @joocer </li> <li>[#524] Query Optimizer - conjunctive predicate splitter. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#041-2022-09-12","title":"[0.4.1] - 2022-09-12","text":""},{"location":"get-started/release-notes/change-log/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed missing <code>__init__</code> file. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#040-2022-09-12","title":"[0.4.0] - 2022-09-12","text":""},{"location":"get-started/release-notes/change-log/#added_6","title":"Added","text":"<ul> <li>[#366] Implement 'function not found' suggestions. @joocer </li> <li>[#443] Introduce a CLI. @joocer </li> <li>[#351] Support <code>SHOW FUNCTIONS</code>. @joocer </li> <li>[#442] Various functions. @joocer </li> <li>[#483] Support <code>SHOW CREATE TABLE</code>. @joocer </li> <li>[#375] Results to an Arrow Table. @joocer </li> <li>[#486] Support functions on aggregates and aggregates on functions. @joocer </li> <li>Initial support for <code>INTERVAL</code>s. @joocer </li> <li>[#395] Support reading CSV files. @joocer </li> <li>[#498] CLI support writing CSV/JSONL/Parquet. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_6","title":"Changed","text":"<ul> <li> [#457] (correction) <code>null</code> values are removed before performing <code>INNER JOIN USING</code>. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_8","title":"Fixed","text":"<ul> <li>[#448] <code>VERSION()</code> failed and missing from regression suite. @joocer </li> <li>[#404] <code>COALESCE</code> fails for NaN values. @joocer </li> <li>[#453] PyArrow bug with long lists creating new columns. @joocer </li> <li>[#444] Very low cardinality <code>INNER JOINS</code> exceed memory allocation. @joocer </li> <li>[#459] Functions lose some detail on non-first page. @joocer </li> <li>[#465] Pages aren't matched to schema for simple queries. @joocer </li> <li>[#468] Parquet reader shows some fields as \"item\". @joocer </li> <li>[#471] Column aliases not correctly applied when the relation has an alias. @joocer </li> <li>[#489] Intermittent behaviour on hash <code>JOIN</code> algorithm. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#030-2022-08-28","title":"[0.3.0] - 2022-08-28","text":""},{"location":"get-started/release-notes/change-log/#added_7","title":"Added","text":"<ul> <li>[#196] Partial implementation of projection pushdown (Parquet Only). @joocer </li> <li>[#41] Enable the results of functions to be used as parameters for other functions. @joocer </li> <li>[#42] Enable inline operations. @joocer </li> <li>[#330] Support <code>SIMILAR TO</code> alias for RegEx match. @joocer </li> <li>[#331] Support <code>SAFE_CAST</code> alias for <code>TRY_CAST</code>. @joocer </li> <li>[#419] Various simple functions (<code>SIGN</code>, <code>SQRT</code>, <code>TITLE</code>, <code>REVERSE</code>). @joocer </li> <li>[#364] Support <code>SOUNDEX</code> function. @joocer </li> <li>[#401] Support SHA-based hash algorithm functions. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_7","title":"Changed","text":"<ul> <li> (alignment) Paths to storage adapters has been updated to reflect 'connector' terminology.</li> <li> (sensible defaults) Default behaviour changed from Mabel partitioning to no partitioning.</li> <li> (correction) - Use of aliases defined in the <code>SELECT</code> clause can no longer be used in <code>WHERE</code> and <code>GROUP BY</code> clauses - this is a correction to align to standard SQL behaviour.</li> <li> (correction) - Use of 'None' as an alias for <code>null</code> is no longer supported - this is a correction to align to standard SQL behaviour.</li> <li>[#326] Prefer pyarrow's 'promote' over manually handling missing fields. @joocer </li> <li>[#39] Rewrite Aggregation Node to use Pyarrow <code>group_by</code>. @joocer </li> <li>[#338] Remove Evaluation Node. @joocer </li> <li>[#58] Performance of <code>ORDER BY RAND()</code> improved. @joocer </li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_9","title":"Fixed","text":"<ul> <li>[#334] All lists should be cast to lists of strings. (@joocer)</li> <li>[#382] <code>INNER JOIN</code> on <code>UNNEST</code> relation. (@joocer)</li> <li>[#320] Can't execute functions on results of <code>GROUP BY</code>. (@joocer)</li> <li>[#399] Strings in double quotes aren't parsed. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#020-2022-07-31","title":"[0.2.0] - 2022-07-31","text":""},{"location":"get-started/release-notes/change-log/#added_8","title":"Added","text":"<ul> <li>[#232] Support <code>DATEPART</code> and <code>EXTRACT</code> date functions. @joocer </li> <li>[#63] Estimate row counts when reading blobs. (@joocer)</li> <li>[#231] Implement <code>DATEDIFF</code> function. (@joocer)</li> <li>[#301] Optimizations for <code>IS</code> conditions. (@joocer)</li> <li>[#229] Support <code>TIME_BUCKET</code> function. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_8","title":"Changed","text":"<ul> <li>[#35] Table scan planning done during query planning. @joocer </li> <li>[#173] Data not found raises different errors under different scenarios. (@joocer)</li> <li>Implementation of <code>LEFT</code> and <code>RIGHT</code> functions to reduce execution time. (@joocer)</li> <li>[#258] Code release approach. (@joocer)</li> <li>[#295] Removed redundant projection when <code>SELECT *</code>. (@joocer)</li> <li>[#297] Filters on <code>SHOW COLUMNS</code> execute before profiling. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_10","title":"Fixed","text":"<ul> <li>[#252] Planner should gracefully convert byte strings to ascii strings. (@joocer)</li> <li>[#184] Schema changes cause unexpected and unhelpful failures. (@joocer)</li> <li>[#261] Read fails if buffer cache is unavailable. (@joocer)</li> <li>[#277] Cache errors should be transparent. (@joocer)</li> <li>[#285] <code>DISTINCT</code> on nulls throws error. (@joocer)</li> <li>[#281] <code>SELECT</code> on empty aggregates reports missing columns. (@joocer)</li> <li>[#312] Invalid dates in <code>FOR</code> clauses treated as <code>TODAY</code>. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#010-2022-07-02","title":"[0.1.0] - 2022-07-02","text":""},{"location":"get-started/release-notes/change-log/#added_9","title":"Added","text":"<ul> <li>[#165] Support S3/MinIO data stores for blobs. (@joocer)</li> <li><code>FAKE</code> dataset constructor (part of #179). (@joocer)</li> <li>[#177] Support <code>SHOW FULL COLUMNS</code> to read entire datasets rather than just the first blob. (@joocer)</li> <li>[#194] Functions that are abbreviations, should have the full name as an alias. (@joocer)</li> <li>[#201] <code>generate_series</code> supports CIDR expansion. (@joocer)</li> <li>[#175] Support <code>WITH (NO_CACHE)</code> hint to disable using cache. (@joocer)</li> <li>[#203] When reporting that a column doesn't exist, it should suggest likely correct columns. (@joocer)</li> <li>'Not' Regular Expression match operator, <code>!~</code> added to supported set of operators. (@joocer)</li> <li>[#226] Implement <code>DATE_TRUNC</code> function. (@joocer)</li> <li>[#230] Allow addressing fields as numbers. (@joocer)</li> <li>[#234] Implement <code>SEARCH</code> function. (@joocer)</li> <li>[#237] Implement <code>COALESCE</code> function. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_9","title":"Changed","text":"<ul> <li>Blob-based readers (disk &amp; GCS) moved from 'local' and 'network' paths to a new 'blob' path. (@joocer)</li> <li>Query Execution rewritten. (@joocer)</li> <li>[#20] Split query planner and query plan into different modules. (@joocer)</li> <li>[#164] Split dataset reader into specific types. (@joocer)</li> <li>Expression evaluation short-cuts execution when executing evaluations against an array of <code>null</code>. (@joocer)</li> <li>[#244] Improve performance of <code>IN</code> test against literal lists. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_11","title":"Fixed","text":"<ul> <li>[#172] <code>LIKE</code> on non string column gives confusing error (@joocer)</li> <li>[#179] Aggregate Node creates new metadata for each chunk (@joocer)</li> <li>[#183] <code>NOT</code> doesn't display in plan correctly (@joocer)</li> <li>[#182] Unable to evaluate valid filters (@joocer)</li> <li>[#178] <code>SHOW COLUMNS</code> returns type OTHER when it can probably work out the type (@joocer)</li> <li>[#128] <code>JOIN</code> fails, using PyArrow .join() (@joocer)</li> <li>[#189] Explicit <code>JOIN</code> algorithm exceeds memory (@joocer)</li> <li>[#199] <code>SHOW EXTENDED COLUMNS</code> blows memory allocations on large tables (@joocer)</li> <li>[#169] Selection nodes in <code>EXPLAIN</code> have nested parentheses. (@joocer)</li> <li>[#220] <code>LIKE</code> clause fails for columns that contain nulls. (@joocer)</li> <li>[#222] Column of <code>NULL</code> detects as <code>VARCHAR</code>. (@joocer)</li> <li>[#225] <code>UNNEST</code> does not assign a type to the column when all of the values are <code>NULL</code>. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#002-2022-06-03","title":"[0.0.2] - 2022-06-03","text":""},{"location":"get-started/release-notes/change-log/#added_10","title":"Added","text":"<ul> <li>[#72] Configuration is now read from <code>opteryx.yaml</code> rather than the environment. (@joocer)</li> <li>[#139] Gather statistics on planning reading of segements. (@joocer)</li> <li>[#151] Implement <code>SELECT table.*</code>. (@joocer)</li> <li>[#137] <code>GENERATE_SERIES</code> function. (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_12","title":"Fixed","text":"<ul> <li>[#106] <code>ORDER BY</code> on qualified fields fails (@joocer)</li> <li>[#103] <code>ORDER BY</code> after <code>JOIN</code> errors (@joocer)</li> <li>[#110] SubQueries <code>AS</code> statement ignored (@joocer)</li> <li>[#112] <code>SHOW COLUMNS</code> doesn't work for non sample datasets (@joocer)</li> <li>[#113] Sample data has \"NaN\" as a string, rather than the value <code>NaN</code> (@joocer)</li> <li>[#111] <code>CROSS JOIN UNNEST</code> should return a <code>NONE</code> when the list is empty (or <code>NONE</code>) (@joocer)</li> <li>[#119] 'NoneType' object is not iterable error on <code>UNNEST</code> (@joocer)</li> <li>[#127] Reading from segments appears to only read the first segment (@joocer)</li> <li>[#132] Multiprocessing regressed Caching functionality (@joocer)</li> <li>[#140] Appears to have read both frames rather than the latest frame (@joocer)</li> <li>[#144] Multiple <code>JOINS</code> in one query aren't recognized (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#001-2022-05-09","title":"[0.0.1] - 2022-05-09","text":""},{"location":"get-started/release-notes/change-log/#added_11","title":"Added","text":"<ul> <li>Additional statistics recording the time taken to scan partitions (@joocer)</li> <li>Support for <code>FULL JOIN</code> and <code>RIGHT JOIN</code> (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#changed_10","title":"Changed","text":"<ul> <li>Use PyArrow implementation for <code>INNER JOIN</code> and <code>LEFT JOIN</code> (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#fixed_13","title":"Fixed","text":"<ul> <li>[#99] Grouping by a list gives an unhelpful error message  (@joocer)</li> <li>[#100] Projection ignores field qualifications (@joocer)</li> </ul>"},{"location":"get-started/release-notes/change-log/#000","title":"[0.0.0]","text":"<ul> <li>Initial Version</li> </ul>"},{"location":"get-started/release-notes/notices/","title":"Notices","text":""},{"location":"get-started/release-notes/notices/#incorporated-components","title":"Incorporated Components","text":"<p>Opteryx is built on the shoulders of other great libraries and components:</p> Library Disposition Copyright Licence cityhash Installed Bespoke cython Installed Apache 2.0 datasketch Integrated 2015 Eric Zhu MIT datetime_truncate Integrated 2020 Media Pop MIT distogram Integrated 2020 Romain Picard MIT fuzzy Integrated Jason R. Coombs MIT levenshtein Integrated Vatsal Patel Public Domain (assumed) mbleven Integrated 2018 Fujimoto Seiji Public Domain numpy Installed BSD-3 orjson Installed Apache 2.0 pyarrow Installed Apache 2.0 pyarrow_ops Derived TomScheffers (assumed) Apache 2.0 pyyaml Installed MIT sqloxide Integrated 2020 Will Eaton MIT sqlparser-rs Installed Apache 2.0 typer Installed MIT <p> Derived components originated from this module but has undergone significant change. Installed components are installed from PyPI/Cargo. Integrated components have their source code (or significant parts of) included in the Opteryx codebase.  </p> <p>This list does not include transitive dependencies nor is guaranteed to be complete. Only components which have been integrated have copyright information noted, best efforts have been made to ensure this information is correct. Corrections should be raised as issues for remediation. </p> <p>Integrated components may differ from their original form. Cosmetic changes are not generally noted however where functionality has been added or altered, this is recorded in comments. </p> <p>Note</p> <p>License information was last checked on 2022-06-03, or when specific entries were added to this document, if later.</p>"},{"location":"get-started/release-notes/notices/#other-assets","title":"Other Assets","text":"<p>The Icarus Opteryx image based on 'Evening: Fall of Day' by William Rimmer (Public Domain), more commonly associated with Led Zepplin's Swan Song. The Icarus Opteryx image is created using visual components from 'Archaeopteryx' fossil image by Caro Asercion (CC BY 3.0).</p> <p>Satellite and Planet datasets acquired from devstronomy.</p> <p>Astronaut dataset acquired from Kaggle.</p> <p>Exoplanet dataset acquire from Kaggle.</p> <p>Diagrams created using ASCII Flow or draw.io.</p> <p>Website build using mkdocs-material.</p> <p>SQL-92 conformity tests are based on sqltest.</p>"},{"location":"sql-reference/adv-engine-configuration/","title":"Engine Configuration","text":""},{"location":"sql-reference/adv-engine-configuration/#query-variables","title":"Query Variables","text":"<p>Variables can be used when a value within a query would benefit from being configurable by the user running the query. For example pre-built queries which perform the same core statement, but with a variable input.</p> <p>Variables are defined using the <code>SET</code> statement. These variables are available to <code>SELECT</code> statements as part of the same query batch. For example:</p> <pre><code>-- set the planet id, change for different planets\nSET @id = 3;\nSELECT name\nFROM $planets\nWHERE id = @id;\n</code></pre> <p>The above query batch contains two statements, the <code>SET</code> and the <code>SELECT</code> separated by a semicolon (<code>;</code>). The variable is defined in the <code>SET</code> statement and must start with an at symbol (<code>@</code>). The variable is then used within a filter in the <code>WHERE</code> clause of the <code>SELECT</code> statement.</p>"},{"location":"sql-reference/adv-engine-configuration/#query-parameters","title":"Query Parameters","text":"<p>Query parameters which affect the execution of the query can be tuned on a per-query basis using the <code>SET</code> statement.</p> <p><code>enable_optimizer</code>: boolean = True</p> <p>Use the query optimizer.</p> <p><code>internal_batch_size</code>: int = 500</p> <p>The maximum input frame size for <code>JOIN</code>s.</p> <p><code>max_join_size</code>: int = 10000</p> <p>The maximum number of records to create in a <code>CROSS JOIN</code> frame.</p> <p><code>page_size</code>: int = 67108864</p> <p>Approximate Page Size in bytes - default is 64Mb.</p> <p><code>enable_page_defragmentation</code>: boolean = True</p> <p>Use the internal page defragmentation.</p>"},{"location":"sql-reference/adv-engine-configuration/#with-hints","title":"WITH hints","text":"<p>Hints are used to direct the planner, optimizer or the executor to make specific decisions. If a hint is not recognized, it is ignored by the planner and executor and is reported in the messages.</p> <p>Multiple Hints can be provided for the same dataset in a query statement by providing the desired Hints in a comma separated list.</p> <p>Note</p> <p>Hints use the keyword <code>WITH</code> which is also the keyword for CTEs, this information relates to hints and not CTEs.</p> <pre><code>FROM dataset WITH(NO_CACHE)\n</code></pre> <p>Instructs blob/file connectors to not use cache, regardless of other settings. This is almost always followed but only applies to the local Buffer Pool and any remote cache - Operating System, CDN or other caches which may be used are not affected by this hint.</p> <pre><code>FROM dataset WITH(NO_PARTITION)\n</code></pre> <p>Instructs blob/file connectors to not use partitioning, regardless of other settings. This is always followed.</p> <pre><code>FROM dataset WITH(NO_PUSH_PROJECTION)\n</code></pre> <p>Instructs blob/file connectors not to try to prune columns at read time. The decision if to follow this hint is made by the reader. </p> <pre><code>FROM dataset WITH(NO_PUSH_SELECTION)\n</code></pre> <p>Instructs the connectors to not attempt to filter results at read time, usually pushing the filter step into the read step. This does not prevent the optimizer placing filter operator elsewhere in the query plan.</p>"},{"location":"sql-reference/adv-null-semantics/","title":"NULL Semantics","text":"<p>Most comparisons to <code>null</code> return <code>null</code>. Exceptions are generally in functions or comparisons specifically to handle <code>null</code>, such as <code>IS NULL</code>.</p> <p>When the outcome of a comparison is <code>null</code>, this will be coerced to <code>false</code> when used in a filter (<code>WHERE</code> or <code>HAVING</code>) but return as <code>null</code> in a <code>SELECT</code> statement.</p> <p>To demonstrate, first a <code>null</code> comparison in a <code>SELECT</code> statement: <pre><code>SELECT name = null\nFROM $planets;\n</code></pre></p> <p>This returns <code>null</code> for all values.</p> <pre><code> name=null\n-----------\n null\n null\n null\n null\n null\n null\n null\n null\n null\n</code></pre> <p>Then a <code>null</code> comparison in a <code>WHERE</code> statement:</p> <pre><code>SELECT name\nFROM $planets\nWHERE name = null;\n</code></pre> <p>Returns an empty set.</p> <p>Note</p> <p><code>null</code> comparison returning <code>null</code> holds true even for <code>null = null</code>. Do not test for null using an equals condition, use <code>IS NULL</code>.</p>"},{"location":"sql-reference/adv-query-optimization/","title":"Query Optimization","text":"<p>Adapted from 15 Best Practices for SQL Optimization.</p> <p>No optimization technique is universally true, these recommendations should improve performance in most cases. As will all optimization, test in your unique set of circumstances before assuming it to be true.</p>"},{"location":"sql-reference/adv-query-optimization/#1-avoid-using-select","title":"1. Avoid using <code>SELECT *</code>","text":"<p>Selecting only the fields you need to be returned improves query performance by reducing the amount of data that is processed internally.</p> <p>A principle the Query Optimizer uses is to eliminate rows and columns to process as early as possible, <code>SELECT *</code> removes the option to remove columns from the data being processed.</p>"},{"location":"sql-reference/adv-query-optimization/#2-prune-early","title":"2. Prune Early","text":"<p>Where available, use temporal filters (<code>FOR DATE</code>) to limit the date range over will limit the number of partitions that need need to be read.</p> <p>Not reading the record is faster than reading and working out if it needs to be filtered out of the result set.</p>"},{"location":"sql-reference/adv-query-optimization/#3-group-by-field-selection","title":"3. <code>GROUP BY</code> field selection","text":"<p>VARCHAR Grouping by <code>VARCHAR</code> columns is usually slower than grouping by <code>NUMERIC</code> columns, if you have an option of grouping by a username or a numeric user id, prefer the user id.</p> <p>cardinality Grouping by columns with high cardinality (mostly unique) is generally slower than grouping where there is a lot of duplication in the groups.</p>"},{"location":"sql-reference/adv-query-optimization/#4-avoid-cross-join","title":"4. Avoid <code>CROSS JOIN</code>","text":"<p>Cross join will very likely create a lot of records that are not required - if you then filter these records from the two source tables using a <code>WHERE</code> clause, it's likely you should use an <code>INNER JOIN</code> instead.</p>"},{"location":"sql-reference/adv-query-optimization/#5-small-table-drives-big-table","title":"5. Small table drives big table","text":"<p>Most <code>JOIN</code>s require iterating over two relations, the left relation, which is the one in the <code>FROM</code> clause, and the right relation which is the one in the <code>JOIN</code> clause (<code>SELECT * FROM left JOIN right</code>). It is generally faster to put the smaller relation to the left.</p> <p>Note</p> <p>This advice may be contradictory to how other database engines optimize queries and may change in the future.</p>"},{"location":"sql-reference/adv-query-optimization/#6-use-like-when-comparing-strings","title":"6. Use <code>LIKE</code> when comparing strings","text":"<p><code>LIKE</code> can be used for pattern matching but it can also be used for comparisions without wildcards and generally performs faster than <code>=</code> comparisons.</p>"},{"location":"sql-reference/adv-query-optimization/#7-use-the-correct-join","title":"7. Use the correct <code>JOIN</code>","text":"<p>A <code>CROSS JOIN</code> can quickly generate millions of records to be filtered, if you can use any join other than the <code>CROSS JOIN</code>, do that.</p>"},{"location":"sql-reference/adv-query-optimization/#8-use-limit","title":"8. Use <code>LIMIT</code>","text":"<p><code>LIMIT</code> stops a query when it has returned the desired number of results; if you do not want the full dataset, using <code>LIMIT</code> can reduce the time taken to process a statement.</p> <p>However, some operations are 'greedy', that is, they need all of the data for their operation (for example <code>ORDER BY</code>, and <code>GROUP BY</code>) - <code>LIMIT</code> does not have the same impact on these queries.</p>"},{"location":"sql-reference/adv-query-optimization/#9-use-where-to-filter-before-group-by","title":"9. Use <code>WHERE</code> to filter before <code>GROUP BY</code>","text":"<p>Only using <code>HAVING</code> to filter the aggregation results of <code>GROUP BY</code>. <code>GROUP BY</code> is a relatively expensive operation in terms of memory and compute, filter as much before the <code>GROUP BY</code> by using the <code>WHERE</code> clause and only use <code>HAVING</code> to filter the by aggregation function (e.g. <code>COUNT</code>, <code>SUM</code>).</p>"},{"location":"sql-reference/adv-query-optimization/#10-is-filters-are-generally-faster-than","title":"10. <code>IS</code> filters are generally faster than <code>=</code>","text":"<p><code>IS</code> comparisons are optimized for a specific check and perform up to twice as fast as <code>=</code> comparisons. However, they are only available for a limited set of checks:</p> <ul> <li><code>IS NONE</code></li> <li><code>IS NOT NONE</code></li> <li><code>IS TRUE</code></li> <li><code>IS FALSE</code></li> </ul>"},{"location":"sql-reference/adv-sample-data/","title":"Sample Data","text":"<p>There are three built-in relations for demonstration and testing.</p> <p> <code>$satellites</code> (8 columns, 177 rows)  <code>$planets</code> (20 columns, 9 rows) #plutoisaplanet <code>$astronauts</code> (19 columns,  357 rows)   </p> <p>Satellite and Planet datasets acquired from this source.</p> <p>Astronaut dataset acquired from Kaggle.</p> <p>These relations are prefixed with a dollar sign (<code>$</code>) and can be accessed as per user datasets. For example:</p> <pre><code>SELECT *\nFROM $planets;\n</code></pre> <p>Note</p> <p>A dataset called <code>$no_table</code> is used internally to represent no table has been specified, this is not intended for end-users and should not be used.</p>"},{"location":"sql-reference/adv-temp-tables/","title":"Relation Constructors","text":"<p>There are multiple options to create temporary relations as part of query definitions. These relations exist only for the execution of the query that defines them.</p>"},{"location":"sql-reference/adv-temp-tables/#using-values","title":"Using <code>VALUES</code>","text":"<p><code>VALUES</code> allows you to create a multi-column temporary relation where the values in the relation are explicitly defined in the statement.</p> <p>A simple example is as follows:</p> <pre><code>SELECT * FROM (\nVALUES ('High', 3),\n('Medium', 2),\n('Low', 1)\n) AS ratings (name, rating);\n</code></pre> <p>Result:</p> <pre><code> name   | rating\n--------+--------\n High   |      3\n Medium |      2\n Low    |      1\n</code></pre>"},{"location":"sql-reference/adv-temp-tables/#using-unnest","title":"Using <code>UNNEST</code>","text":"<p><code>UNNEST</code> allows you to create a single column temporary relation where the values in the relation are explicitly defined in the statement.</p> <p>A simple example is as follows:</p> <pre><code>SELECT *\nFROM UNNEST((1,2,3));\n</code></pre> <p>Result:</p> <pre><code> unnest \n--------\n      1\n      2\n      3\n</code></pre> <p>Note</p> <p>The values in the <code>UNNEST</code> function are in two sets of parenthesis. The function accepts a list of values, parenthesis is used to wrap parameters to functions and also used to define lists.</p>"},{"location":"sql-reference/adv-temp-tables/#using-generate_series","title":"Using <code>generate_series</code>","text":"<p><code>generate_series</code> allows you to create series by defining the bounds of the series, and optionally, an interval to step between values in the created series. </p> <p><code>generate_series</code> supports the following variations:</p> Form Types Description <code>generate_series(stop)</code> NUMERIC Generate a NUMERIC series between 1 and 'stop', with a step of 1 <code>generate_series(start, stop)</code> NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with a step of 1 <code>generate_series(start, stop, step)</code> NUMERIC, NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with an explicit step size <code>generate_series(start, stop, interval)</code> TIMESTAMP, TIMESTAMP, INTERVAL Generate a TIMESTAMP series between 'start' and 'stop', with a given interval <code>generate_series(cidr)</code> VARCHAR Generate set of IP addresses from a given CIDR (e.g. <code>192.168.0.0/24</code>) <p>Single Parameter Example (NUMERIC):</p> <p><pre><code>SELECT *\nFROM generate_series(3)\n</code></pre> <pre><code> generate_series \n-----------------\n               1\n               2\n               3\n</code></pre></p> <p>Single Parameter Example (VARCHAR):</p> <p><pre><code>SELECT *\nFROM generate_series('192.168.1.0/30')\n</code></pre> <pre><code> generate_series \n-----------------\n 192.168.1.0\n 192.168.1.1\n 192.168.1.2\n 192.168.1.3\n</code></pre></p> <p>Two parameter Example:</p> <p><pre><code>SELECT *\nFROM generate_series(2, 4)\n</code></pre> <pre><code> generate_series \n-----------------\n               2\n               3\n               4\n</code></pre></p> <p>Three parameter NUMERIC Example:</p> <p><pre><code>SELECT *\nFROM generate_series(-5, 5, 5)\n</code></pre> <pre><code> generate_series \n-----------------\n              -5\n               0\n               5\n</code></pre></p> <p>Three parameter TIMESTAMP example:</p> <p><pre><code>SELECT *\nFROM generate_series('2020-01-01', '2025-12-31', '1y')\n</code></pre> <pre><code>  generate_series \n------------------\n 2020-01-01 00:00\n 2021-01-01 00:00\n 2022-01-01 00:00\n 2023-01-01 00:00\n 2024-01-01 00:00\n 2025-01-01 00:00\n</code></pre></p>"},{"location":"sql-reference/adv-temp-tables/#interval-definitions","title":"Interval Definitions","text":"<p>Intervals are defined quantifying one or more periods which make up the interval, supported periods and their notation are:</p> <p>Recognized interval parts for the <code>GENERATE_SERIES</code> function are:</p> Period Symbol Aliases Years year / years y / yr / yrs Months month / months mo / mon / mons / mth / mths Weeks week / weeks w / wk / wks Days day / days d Hours hour / hours h / hr / hrs Minutes minute / minutes m / min / mins Seconds second / seconds s / sec / secs <p>Where required, periods can be combined to define more complex intervals, for example <code>1h30m</code> represents one hour and 30 minutes.</p>"},{"location":"sql-reference/adv-temp-tables/#using-fake","title":"Using <code>FAKE</code>","text":"<p><code>FAKE</code> creates a table of random integers from provided row and column counts. This functionality has limited application outside of creating datasets for testing.</p> <p>A simple example is as follows:</p> <pre><code>SELECT * FROM FAKE(3, 2);\n</code></pre> <p>Result:</p> <pre><code>   column_0 \u2502   column_1 \n------------\u253c------------\n      32981 \u2502      50883\n       5037 \u2502      42087\n      51741 \u2502      49456\n</code></pre>"},{"location":"sql-reference/adv-time-travel/","title":"Time Travel","text":"<p>The engine support temporality, the ability to view things as they were at a different point in time.</p> <p>For datasets which are snapshots, this allows you to recall the data of that snapshop as at a data in the past. For datasets which are logs, this allows you to prune queries to just the dates which contain relevant data.</p> <p>Note</p> <ul> <li>Data must be Mabel partitioned or using a custom partition schema which supports data partitioning.</li> <li>Data returned for previous days with be the latest data as at today. For example if a backfill updates data from seven days ago, when querying that data today the backfilled data will be returned.</li> <li>There is no implicit deduplication of records as they are returned.</li> </ul> <p>Partition schemes that supports temporal queries allow you to view data from a different date by using a <code>FOR</code> clause after the dateset name in the SQL statement. <code>FOR</code> clauses state the date, or date range, a query should retrieve results for.</p> <p>If no temporal clause is provided and the schema supports it, <code>FOR TODAY</code> is assumed.</p> <p>Warning</p> <p>Temporal clauses operate on calendar days in UTC. For example, from midnight <code>FOR TODAY</code> will return no data until data is written for that day.</p>"},{"location":"sql-reference/adv-time-travel/#single-dates","title":"Single Dates","text":"<p>Data from a specific, single, date can be obtained using the <code>FOR date</code> syntax. </p> <pre><code>FOR date\n</code></pre> <p>Date values in <code>FOR</code> clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example.</p> <ul> <li><code>FOR TODAY</code></li> <li><code>FOR YESTERDAY</code></li> <li><code>FOR '2022-02-14'</code></li> </ul>"},{"location":"sql-reference/adv-time-travel/#date-ranges","title":"Date Ranges","text":"<p>Data within a range of dates can be specified using <code>FOR DATES BETWEEN</code> or <code>FOR DATES IN</code> syntax. Where data is retrieved for multiple dates, the datasets for each day have an implicit <code>UNION ALL</code> applied to them.</p> <p><pre><code>FOR DATES BETWEEN start AND end\n</code></pre> <pre><code>FOR DATES IN range\n</code></pre></p> <p>Date values in <code>BETWEEN</code> clauses must either be in 'YYYY-MM-DD' format or a recognized date placeholder, for example:</p> <ul> <li><code>FOR DATES BETWEEN '2000-01-01' AND TODAY</code></li> <li><code>FOR DATES BETWEEN '2020-04-01' AND '2020-04-30'</code></li> </ul> <p>Date range values in <code>IN</code> clauses must be recognized date range placeholders, for example:</p> <ul> <li><code>FOR DATES IN LAST_MONTH</code></li> </ul>"},{"location":"sql-reference/adv-time-travel/#placeholders","title":"Placeholders","text":"Placeholder Applicability Description <code>TODAY</code> FOR, BETWEEN This calendar day <code>YESTERDAY</code> FOR, BETWEEN The previous calendar day <code>THIS_MONTH</code> IN Since the first of the current month <code>LAST_MONTH</code> IN The previous calendar month (also <code>PREVIOUS_MONTH</code>) <p>Caution</p> <ul> <li><code>FOR</code> clauses cannot contain comments or reference column values or aliases  </li> <li>Dates can not include times and must be in the format 'YYYY-MM-DD'  </li> <li>The default partition scheme does not support Temporal queries  </li> <li>Temporal clauses must follow the relation name they relate to, and they only apply to that relation.</li> </ul>"},{"location":"sql-reference/adv-time-travel/#time-travel_1","title":"Time Travel","text":"<p>You can query dates or date ranges using a <code>FOR</code> clause in your query. For example to view the contents of partition</p> <pre><code>SELECT *\nFROM $planets\nFOR YESTERDAY;\n</code></pre> <p>This technique is well suited to viewing snapshotted datasets from a previoud point in time. </p> <p>The '$planets' sample dataset has special handling to respond to temporal queries; Uranus was discovered in 1846 and Pluto was discovered in 1930, we and use the <code>FOR</code> clause to query the '$planets' relation from before those planets were discovered like this:</p> <pre><code>SELECT name\nFROM $planets\nFOR '1846-01-01';\n</code></pre> <p>Returns (order may differ):</p> <pre><code>name\n-------\nMercury\nVenus\nEarth\nMars\nJupiter\nSaturn\nNeptune\n</code></pre>"},{"location":"sql-reference/adv-time-travel/#accumulation","title":"Accumulation","text":"<p>For datasets which are continually added to, such as logs, the <code>FOR</code> clause can be used to quickly filter ranges of records to search over. The <code>FOR</code> clause will most likely record the date the record was written (the 'SYSTEM_TIME' for the record) which may not be the same as the logical or effective date for a record, especially in situations where there is a lag in the records being recorded.</p> <p>The <code>BETWEEN</code> keyword can be used to describe ranges of records, this is useful for querying logged data between two dates.</p> <pre><code>SELECT name\nFROM $planets\nFOR DATES BETWEEN '2021-01-01' and '2022-12-31';\n</code></pre>"},{"location":"sql-reference/adv-working-with-lists/","title":"Working with Lists","text":"<p>A list is an ordered collection of zero or more <code>VARCHAR</code> values.</p>"},{"location":"sql-reference/adv-working-with-lists/#actions","title":"Actions","text":""},{"location":"sql-reference/adv-working-with-lists/#accessing","title":"Accessing","text":"<pre><code>list[index]\n</code></pre>"},{"location":"sql-reference/adv-working-with-lists/#testing","title":"Testing","text":"<pre><code>value IN list\n</code></pre>"},{"location":"sql-reference/adv-working-with-lists/#searching","title":"Searching","text":"<p><pre><code>SEARCH(list, value)\n</code></pre> <pre><code>IN UNNEST(list)\n</code></pre> <pre><code>LIST_CONTAINS   \n</code></pre> <pre><code>LIST_CONTAINS_ANY   \n</code></pre> <pre><code>LIST_CONTAINS_ALL\n</code></pre></p>"},{"location":"sql-reference/adv-working-with-lists/#transforms","title":"Transforms","text":"<pre><code>SORT\n</code></pre>"},{"location":"sql-reference/adv-working-with-lists/#converting-lists-to-relations","title":"Converting Lists to Relations","text":""},{"location":"sql-reference/adv-working-with-lists/#using-unnest","title":"Using <code>UNNEST</code>","text":"<p><code>UNNEST</code> allows you to create a single column table either as a list of literals, or from a column of LIST type in a dataset.</p> <pre><code>SELECT * FROM UNNEST((True, False)) AS Booleans;\n</code></pre>"},{"location":"sql-reference/adv-working-with-lists/#limitations","title":"Limitations","text":"<p>Lists have the following limitations</p> <ul> <li>Statements cannot <code>ORDER BY</code> a list column</li> <li>Statements cannot contain <code>DISTINCT</code> and <code>JOIN</code> when the relations include list columns</li> <li>Lists cannot be used in comparisons</li> </ul> <p>Note</p> <p>Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove list columns as part of optimization. However, you should not rely on the optimizer to take any particular action.</p>"},{"location":"sql-reference/adv-working-with-structs/","title":"Working with Structs","text":"<p>A struct is a collection of zero or more key, value pairs. Keys must be <code>VARCHAR</code>, values can be different types.</p>"},{"location":"sql-reference/adv-working-with-structs/#actions","title":"Actions","text":""},{"location":"sql-reference/adv-working-with-structs/#reading","title":"Reading","text":"<pre><code>struct[key]\n</code></pre> <p>Values within structs can be accessed by key using subscript notation, putting the key in square brackets following the struct.</p> <p>Example:</p> <pre><code>SELECT birth_place['town']\nFROM $astronauts\n</code></pre>"},{"location":"sql-reference/adv-working-with-structs/#searching","title":"Searching","text":"<pre><code>`SEARCH(struct, value)`\n</code></pre> <p>All values in a struct can be searched for a given value using the <code>SEARCH</code> function.</p> <p>Example:</p> <pre><code>SELECT name,\nSEARCH(birth_place, 'Italy')\nFROM $astronauts\n</code></pre>"},{"location":"sql-reference/adv-working-with-structs/#limitations","title":"Limitations","text":"<p>Structs have the following limitations</p> <ul> <li>Statements cannot <code>ORDER BY</code> a struct column</li> <li>Statements cannot contain <code>DISTINCT</code> and <code>JOIN</code> when the relations include struct columns</li> <li>Structs cannot be used in comparisons</li> </ul> <p>Note</p> <p>Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove struct columns as part of optimization. However, you should not rely on the optimizer to take any particular action.</p>"},{"location":"sql-reference/adv-working-with-timestamps/","title":"Working with Timestamps","text":"<p>Working with Timestamps often involves working with Intervals.</p>"},{"location":"sql-reference/adv-working-with-timestamps/#actions","title":"Actions","text":""},{"location":"sql-reference/adv-working-with-timestamps/#addsubtract","title":"Add/Subtract","text":"<p>timestamp <code>+</code> interval \u2192 timestamp </p> <p>timestamp <code>-</code> interval \u2192 timestamp </p> <p>timestamp <code>-</code> timestamp \u2192 interval </p> <p><code>DATEDIFF</code> (unit: varchar, start: timestamp, end: timestamp) \u2192 numeric </p> <p>Note</p> <p><code>INTERVAL</code> may not support all functions in all circumstances.  </p>"},{"location":"sql-reference/adv-working-with-timestamps/#construct","title":"Construct","text":"<pre><code>INTERVAL values units\n</code></pre>"},{"location":"sql-reference/adv-working-with-timestamps/#extract","title":"Extract","text":"<p><pre><code>EXTRACT(part FROM timestamp)\n</code></pre> <pre><code>DATE(timestamp)\n</code></pre></p>"},{"location":"sql-reference/adv-working-with-timestamps/#format","title":"Format","text":"<pre><code>DATE_FORMAT(timestamp, format)\n</code></pre>"},{"location":"sql-reference/adv-working-with-timestamps/#parse","title":"Parse","text":"<p><pre><code>CAST(field AS TIMESTAMP)\n</code></pre> <pre><code>TIMESTAMP(field)\n</code></pre></p>"},{"location":"sql-reference/adv-working-with-timestamps/#truncate","title":"Truncate","text":"<p><pre><code>DATE_TRUNC(part, timestamp)\n</code></pre> <pre><code>TIME_BUCKET(timestamp, multiple, unit)\n</code></pre></p>"},{"location":"sql-reference/adv-working-with-timestamps/#generate","title":"Generate","text":"<p><pre><code>current_date\n</code></pre> <pre><code>current_time\n</code></pre> <pre><code>YESTERDAY()\n</code></pre> <pre><code>TIME()\n</code></pre> <pre><code>generate_series()\n</code></pre></p> <p>Note that <code>current_date</code> and <code>current_time</code> support being called without parenthesis.</p> <p>Recognized date parts and periods and support across various functions:</p> Part DATE_TRUNC EXTRACT DATEDIFF TIME_BUCKET Notes second \u2713 \u2713 \u2713 \u2713 minute \u2713 \u2713 \u2713 \u2713 hour \u2713 \u2713 \u2713 \u2713 day \u2713 \u2713 \u2713 \u2713 dow \u2718 \u2713 \u2718 \u2718 day of week week \u2713 \u2713 \u2713 \u2713 iso week i.e. to monday month \u2713 \u2713 \u25b2 \u2713 DATEFIFF unreliable calculating months quarter \u2713 \u2713 \u2713 \u2713 doy \u2718 \u2713 \u2718 \u2718 day of year year \u2713 \u2713 \u2713 \u2713"},{"location":"sql-reference/adv-working-with-timestamps/#implicit-casting","title":"Implicit Casting","text":"<p>In many situation where a timestamp is expected, if an ISO1806 formatted string is provided, the engine will interpret as a timestamp.</p>"},{"location":"sql-reference/adv-working-with-timestamps/#timezones","title":"Timezones","text":"<p>The engine is opinionated to run in UTC - all instances where the system time is requested, UTC is used.</p>"},{"location":"sql-reference/aggregates/","title":"Aggregates","text":"<p>Aggregates are functions that combine multiple rows into a single value. Aggregates can only be used in the <code>SELECT</code> and <code>HAVING</code> clauses of a SQL query.</p> <p>When the <code>ORDER BY</code> clause is provided, the values being aggregated are sorted after applying the function. </p> <p>Aggregate functions generally ignore <code>null</code> values when performing calculations.</p> <p>Definitions noted with a  are only supported in a statement with a <code>GROUP BY</code> clause.</p> <p>New aggregates for this version are annotated with the  icon.</p>"},{"location":"sql-reference/aggregates/#general-functions","title":"General Functions","text":"<p><code>ANY_VALUE</code> (column) \u2192 any</p> <p>Select any single value from the grouping.  </p> <p><code>APPROXIMATE_MEDIAN</code> (column: numeric) \u2192 numeric</p> <p>Approximate median of a column with T-Digest algorithm.</p> <p><code>ARRAY_AGG</code> ([ DISTINCT ] column [ LIMIT n ]) \u2192 array </p> <p>The list of values for column in the group.  The DISTINCT modifier optionally filters to unique values only.  The LIMIT clause limits the number of items in each list to a maximum of n items.     </p> <p><code>AVG</code> (column: numeric) \u2192 numeric</p> <p>The mean average of a numeric column.  Alias for <code>MEAN</code> and <code>AVERAGE</code>.</p> <p><code>COUNT</code> (*) \u2192 numeric</p> <p>Count the number of rows.</p> <p><code>COUNT</code> (column) \u2192 numeric</p> <p>Count the number of non <code>null</code> values in column.</p> <p><code>COUNT_DISTINCT</code> (column) \u2192 numeric</p> <p>Count the number of unique values.</p> <p><code>LIST</code> (column) \u2192 array </p> <p>The complete list of values for column in the group.  Related: <code>ARRAY_AGG</code></p> <p><code>MAX</code> (column) \u2192 any</p> <p>The maximum value in column. Alias for <code>MAXIMUM</code>.</p> <p><code>MIN</code> (column) \u2192 any</p> <p>The minimum value in column. Alias for <code>MINIMUM</code>.</p> <p><code>MIN_MAX</code> (column) \u2192 struct</p> <p>The minimum and maximum values in column.  </p> <p><code>ONE</code> (column) \u2192 any</p> <p>Alias for <code>ANY_VALUE</code>.  </p> <p><code>PRODUCT</code> (column: numeric) \u2192 numeric</p> <p>The product of values in column.  </p> <p><code>STDDEV</code> (column: numeric) \u2192 numeric</p> <p>The standard deviation of values in column.  </p> <p><code>SUM</code> (column: numeric) \u2192 numeric</p> <p>The sum of values in column.  </p> <p><code>VARIANCE</code> (column: numeric) \u2192 numeric</p> <p>The variance of values in column.  </p>"},{"location":"sql-reference/data-types/","title":"Data Types","text":"<p>The engine supports a reduced set of types compared to full DBMS platforms.</p>"},{"location":"sql-reference/data-types/#types","title":"Types","text":"Name Description <code>BOOLEAN</code> Logical boolean (True/False). <code>NUMERIC</code> All numeric types. <code>LIST</code> An ordered sequence of strings. <code>VARCHAR</code> Variable-length character string. <code>STRUCT</code> A dictionary of multiple named values, where each key is a string, but the value can be a different type for each key. <code>TIMESTAMP</code> Combination of date and time. <code>INTERVAL</code> The difference between two TIMESTAMP values <p>Note</p> <ul> <li><code>INTERVAL</code> may not support all functions in all circumstances.  </li> <li><code>LIST</code>s of non-string values have limited support.</li> </ul>"},{"location":"sql-reference/data-types/#casting","title":"Casting","text":"<p>Values can be cast using the <code>CAST</code> function, its form is <code>CAST(any AS type)</code>. Where values are incompatible, an error will be thrown, to avoid errors <code>TRY_CAST</code> (or <code>SAFE_CAST</code>) can be used instead which will return <code>null</code> instead of error.</p>"},{"location":"sql-reference/data-types/#type-hints","title":"Type Hints","text":"<p>Intervals</p> <p>Intervals require definition by type hints, using the type name before providing a literal description of the value.</p> <pre><code>INTERVAL 'value' unit\n</code></pre> <p>Where unit can be 'Year', 'Month', 'Day', 'Hour', 'Minute' or 'Second'.</p> <p>Other</p> <p><code>BOOLEAN</code>, <code>NUMERIC</code> and <code>TIMESTAMP</code> also support 'type hint' notation (<code>SELECT TIMESTAMP '2022-01-01';</code>) to perform casting.</p>"},{"location":"sql-reference/data-types/#coercion","title":"Coercion","text":""},{"location":"sql-reference/data-types/#timestamps","title":"Timestamps","text":"<p>Literal values in quotes may be in interpreted as a <code>TIMESTAMP</code> when they match a valid date in ISO 8601  format (e.g. <code>YYYY-MM-DD</code> and <code>YYYY-MM-DD HH:MM</code>).</p> <p>All <code>TIMESTAMP</code> and date values read from datasets are coerced to nanosecond precision timestamps.</p>"},{"location":"sql-reference/data-types/#numbers","title":"Numbers","text":"<p>All numeric values included in SQL statements and read from datasets are coerced to 64bit floats.</p>"},{"location":"sql-reference/expressions/","title":"Expressions","text":"<p>An expression is a combination of values, operators and functions. Expressions are highly composable, and range from very simple to arbitrarily complex. They can be found in many different parts of SQL statements. In this section, we provide the different types of operators that can be used within expressions.</p>"},{"location":"sql-reference/expressions/#logical-operators","title":"Logical Operators","text":"<p>Logical Operators are used within Expressions to express how predicates combine.</p> <p>The following logical operators are available: <code>NOT</code>, <code>AND</code>, <code>OR</code>, and <code>XOR</code>.</p> a b a <code>AND</code> b a <code>OR</code> b a <code>XOR</code> b <code>NOT</code> a true true true true false false true false false true true false false false false false false true null true null null null null null false null null null null <p>The operators <code>AND</code>, <code>OR</code>, and <code>XOR</code> are commutative, that is, you can switch the left and right operand without affecting the result.</p>"},{"location":"sql-reference/expressions/#comparison-operators","title":"Comparison Operators","text":"<p>Comparison Operators are used within Expressions to compare values, usually involving comparing a field within the datasets against a literal value - although comparisons can be used against two fields, or two literals.</p> <p>Usually when one of the values involved in the comparison is <code>null</code>, the result is <code>null</code>.</p> Operator Description <code>=</code> equal to <code>&lt;&gt;</code> not equal to <code>&lt;</code> less than <code>&gt;</code> greater than <code>&lt;=</code> less than or equal to <code>&gt;=</code> greater than or equal to <code>IN</code> value in list <code>NOT IN</code> value not in list <code>LIKE</code> pattern match <code>NOT LIKE</code> inverse results of <code>LIKE</code> <code>ILIKE</code> case-insensitive pattern match <code>NOT ILIKE</code> inverse results of <code>ILIKE</code> <code>~</code> regular expression match (also <code>SIMILAR TO</code>) <code>!~</code> inverse results of <code>~</code> (also <code>NOT SIMILAR TO</code>) <code>~*</code> case insensitive regular expression match <code>!~*</code> inverse results of <code>~*</code> <code>IS</code> special comparison for <code>true</code>, <code>false</code> and <code>null</code>"},{"location":"sql-reference/expressions/#other-comparisons","title":"Other Comparisons","text":""},{"location":"sql-reference/expressions/#between","title":"BETWEEN","text":"Predicate Description <code>a BETWEEN x AND y</code> equivalent to <code>a &gt;= x AND a &lt;= y</code> <code>a NOT BETWEEN x AND y</code> equivalent to <code>a &lt; x OR a &gt; y</code> <p>Warning</p> <p>Using <code>BETWEEN</code> with other predicates, especially when used with an <code>AND</code> conjunction, can cause the query parser to fail. </p>"},{"location":"sql-reference/expressions/#case","title":"CASE","text":"<p>The <code>CASE</code> expression has two forms. The 'simple' form searches each value expression from top to bottom until it finds one that equals expression:</p> <pre><code>CASE expression\nWHEN value THEN result\n[ WHEN ... ]\n[ ELSE result ]\nEND\n</code></pre> <p>The result for the matching value is returned. If no match is found, the result from the <code>ELSE</code> clause is returned if it exists, otherwise <code>null</code> is returned. Example:</p> <pre><code>SELECT name, CASE numberOfMoons WHEN 0 THEN 'none' WHEN 1 THEN 'one' ELSE 'lots' END as how_many_moons\nFROM $planets;\n</code></pre> <p>The 'searched' form evaluates each boolean condition from top to bottom until one is true and returns the matching result:</p> <pre><code>CASE\nWHEN condition THEN result\n[ WHEN ... ]\n[ ELSE result ]\nEND\n</code></pre> <p>If no conditions are true, the result from the <code>ELSE</code> clause is returned if it exists, otherwise <code>null</code> is returned. Example:</p> <pre><code>SELECT name, CASE\nWHEN numberOfMoons = 0 THEN 'none' WHEN numberOfMoons = 1 THEN 'one' ELSE 'lots' END as how_many_moons\nFROM $planets;\n</code></pre>"},{"location":"sql-reference/expressions/#subqueries","title":"Subqueries","text":"<p>The <code>IN</code> operator can reference a sub query, this sub query cannot include a temporal clause (<code>FOR</code>), but otherwise the full syntax for <code>SELECT</code> queries are supported.</p> <p>For example, to find the planets without any satellites.</p> <pre><code>SELECT name\nFROM $planets\nWHERE id NOT IN (\nSELECT DISTINCT planetId\nFROM $satellites\n);\n</code></pre>"},{"location":"sql-reference/functions/","title":"Functions","text":"<p>This document describes the supported SQL functions and operators.</p> <p>Generally functions will return <code>null</code> on <code>null</code> input, although note that this is not true in all circumstances, especially for null-aware functions like <code>COALESCE</code> and <code>IFNULL</code>.</p> <p>Definitions noted with a  accept different input arguments.</p> <p>New functions for this version are annotated with the  icon.</p>"},{"location":"sql-reference/functions/#conversion-functions","title":"Conversion Functions","text":"<p><code>BOOLEAN</code> any: any \u2192 boolean</p> <p>Cast any to a <code>BOOLEAN</code>, raises an error if cast is not possible. Note <code>BOOLEAN</code> does not require parenthesis, however any aliases do.     Alias for <code>CAST</code>(any AS BOOLEAN)   </p> <p><code>CAST</code> (any: any AS type) \u2192 type</p> <p>Cast any to type, raises an error if cast is not possible.  Also implemented as individual cast functions.</p> <p><code>INT</code> (num: numeric) \u2192 numeric</p> <p>Alias for <code>INTEGER</code></p> <p><code>INTEGER</code> (num: numeric) \u2192 numeric</p> <p>Convert num to an integer.  <code>INTEGER</code> is a psuedo-type, <code>CAST</code> is not supported and values may be coerced to <code>NUMERIC</code>.</p> <p><code>FLOAT</code> (num: numeric) \u2192 numeric</p> <p>Convert num to a floating point number.  <code>FLOAT</code> is a psuedo-type, <code>CAST</code> is not supported and values may be coerced to <code>NUMERIC</code>.</p> <p><code>NUMERIC</code> any: any \u2192 numeric</p> <p>Cast any to a floating point number, raises an error if cast is not possible. Note <code>NUMERIC</code> does not require parenthesis, however any aliases do.  Alias for <code>CAST</code>(any AS NUMERIC)   </p> <p><code>SAFE_CAST</code> (any: any AS type) \u2192 type</p> <p>Alias for <code>TRY_CAST</code>(any AS type)  </p> <p><code>STR</code> (any: any) \u2192 varchar</p> <p>Alias of <code>VARCHAR</code>(any) and <code>CAST</code>(any AS VARCHAR)   </p> <p><code>STRING</code> (any: any) \u2192 varchar</p> <p>Alias of <code>VARCHAR</code>(any) and <code>CAST</code>(any AS VARCHAR)</p> <p><code>TIMESTAMP</code> iso8601: varchar \u2192 timestamp</p> <p>Cast an ISO 8601 format string to a timestamp, raises an error if cast is not possible. Note <code>TIMESTAMP</code> does not require parenthesis, however any aliases do.   Alias for <code>CAST</code>(iso8601 AS TIMESTAMP)   </p> <p><code>TRY_CAST</code> (any: any AS type) \u2192 type</p> <p>Cast any to type, if cast is not possible, returns <code>null</code>.   </p> <p><code>VARCHAR</code> (any) \u2192 varchar</p> <p>Cast any to a string, raises an error if cast is not possible. Alias for <code>CAST</code>(any AS VARCHAR)</p>"},{"location":"sql-reference/functions/#date-time-functions","title":"Date &amp; Time Functions","text":"<p>For more details, see Working with Timestamps.</p> <p><code>current_date</code> \u2192 timestamp</p> <p>Return the current date, in UTC. Note <code>current_date</code> does not require parenthesis.  </p> <p><code>current_time</code> \u2192 timestamp</p> <p>Return the current date and time, in UTC. Note <code>current_time</code> does not require parenthesis.  </p> <p><code>DATE</code> (ts: timestamp) \u2192 timestamp</p> <p>Remove any time information, leaving just the date part of ts.   </p> <p><code>DATE_FORMAT</code> (ts: timestamp, format: varchar) \u2192 varchar</p> <p>Formats ts as a string using format.   </p> <p><code>DATEPART</code>(unit: varchar, ts: timestamp) \u2192 numeric</p> <p>Alias of <code>EXTRACT</code>(unit FROM ts)</p> <p><code>DATE_TRUNC</code> (unit: varchar, ts: timestamp) \u2192 varchar</p> <p>Returns ts truncated to unit.  </p> <p><code>DATEDIFF</code> (unit: varchar, start: timestamp, end: timestamp) \u2192 numeric</p> <p>Calculate the difference between the start and end timestamps in a given unit.  </p> <p><code>DAY</code> (timestamp) \u2192 numeric</p> <p>Extract day number from a timestamp. See <code>EXTRACT</code>.</p> <p><code>EXTRACT</code> (unit FROM timestamp) \u2192 numeric</p> <p>Extract unit of a timestamp.  Also implemented as individual extraction functions.</p> <p><code>FROM_UNIXTIME</code> (timestamp: numeric) \u2192 timestamp</p> <p>  New in 0.8  Return a timestamp representation of an Unix Timestamp.   Related: <code>UNIXTIME</code> </p> <p><code>NOW</code> () \u2192 timestamp</p> <p>Alias for <code>current_time</code></p> <p><code>TIME</code> () \u2192 timestamp</p> <p>Returns the current iime (UTC).     </p> <p><code>TIME_BUCKET</code> (timestamp, multiple: numeric, unit: varchar) \u2192 timestamp</p> <p>Floor timestamps into fixed time interval buckets. unit is optional and will be <code>day</code> if not provided.</p> <p><code>TODAY</code> () \u2192 timestamp</p> <p>Alias for <code>current_date</code></p> <p><code>HOUR</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the hour of the day from ts. The value ranges from <code>0</code> to <code>23</code>.  Alias for <code>EXTRACT</code>(hour FROM ts)</p> <p><code>MINUTE</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the minute of the hour from ts. The value ranges from <code>0</code> to <code>59</code>. Alias for <code>EXTRACT</code>(minute FROM ts)</p> <p><code>MONTH</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the month of the year from ts. The value ranges from <code>1</code> to <code>12</code>. Alias for <code>EXTRACT</code>(month FROM ts)</p> <p><code>QUARTER</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the quarter of the year from ts. The value ranges from <code>1</code> to <code>4</code>. Alias for <code>EXTRACT</code>(quarter FROM ts)</p> <p><code>SECOND</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the second of the minute from ts. The value ranges from <code>0</code> to <code>59</code>. Alias for <code>EXTRACT</code>(second FROM ts)</p> <p><code>UNIXTIME</code> () \u2192 numeric</p> <p>  New in 0.8  Return the current time as a Unix Timestamp.   Related: <code>FROM_UNIXTIME</code>, <code>current_time</code> </p> <p><code>UNIXTIME</code> (timestamp: timestamp) \u2192 numeric</p> <p>  New in 0.8  Return timestamp in Unix Timestamp representation.   Related: <code>FROM_UNIXTIME</code> </p> <p><code>WEEK</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the week of the year from ts. The value ranges from <code>1</code> to <code>53</code>. Alias for <code>EXTRACT</code>(week FROM ts)</p> <p><code>YEAR</code> (ts: timestamp) \u2192 numeric</p> <p>Returns the year from ts. Alias for <code>EXTRACT</code>(year FROM ts)</p>"},{"location":"sql-reference/functions/#infix-functions","title":"Infix Functions","text":"<p>These are functions that are called similar to comparison operators:</p> <p>numeric <code>+</code> numeric \u2192 numeric</p> <p>Numeric addition</p> <p>timestamp <code>+</code> interval \u2192 timestamp</p> <p>Timestamp and Interval addition</p> <p>numeric <code>-</code> numeric \u2192 numeric</p> <p>Numeric subtraction</p> <p>timestamp <code>-</code> interval \u2192 timestamp</p> <p>Timestamp and Interval subtraction</p> <p>timestamp <code>-</code> timestamp \u2192 interval</p> <p>Timestamp subtraction</p> <p>numeric <code>*</code> numeric \u2192 numeric</p> <p>Numeric multiplication</p> <p>numeric <code>/</code> numeric \u2192 numeric</p> <p>Numeric division</p> <p>numeric <code>%</code> numeric \u2192 numeric</p> <p>Numeric modulo (remainder)</p> <p>varchar <code>||</code> varchar \u2192 varchar</p> <p>String concatenation  </p>"},{"location":"sql-reference/functions/#list-functions","title":"List Functions","text":"<p>For more details, see Working with Lists.</p> <p>array: list<code>[</code>index: numeric<code>]</code> \u2192 value</p> <p>Return the indexth element from array. </p> <p><code>GET</code> (array: list, index: numeric) \u2192 value</p> <p>Alias of array<code>[</code>index<code>]</code> </p> <p><code>GREATEST</code> (array: list) \u2192 value</p> <p>Return the greatest value in array. Related: <code>LEAST</code></p> <p><code>LEAST</code> (array: list) \u2192 value</p> <p>Return the smallest value in array. Related: <code>GREATEST</code></p> <p><code>LEN</code> (array: list) \u2192 numeric</p> <p>Alias of <code>LENGTH</code>(array)</p> <p><code>LENGTH</code> (array: list) \u2192 numeric</p> <p>Returns the number of elements in array.</p> <p><code>LIST_CONTAINS</code> (array: list, value) \u2192 boolean</p> <p>Return <code>true</code> if array contains value. See also <code>SEARCH</code>(array, value)  </p> <p><code>LIST_CONTAINS_ANY</code> (array: list, values: list) \u2192 boolean</p> <p>Return <code>true</code> if array contains any elements in values.</p> <p><code>LIST_CONTAINS_ALL</code> (array: list, values: list) \u2192 boolean</p> <p>Return <code>true</code> if array contains all of elements in values.</p> <p><code>SEARCH</code> (array: list, value) \u2192 boolean</p> <p>Return <code>true</code> if array contains value. </p> <p><code>SORT</code> (array: list) \u2192 list</p> <p>Return array in ascending order. </p>"},{"location":"sql-reference/functions/#numeric-functions","title":"Numeric Functions","text":"<p><code>ABS</code> (x: numeric) \u2192 numeric</p> <p>Alias of <code>ABSOLUTE</code> </p> <p><code>ABSOLUTE</code> (x: numeric) \u2192 numeric</p> <p>Returns the absolute value of x.   </p> <p><code>CEIL</code> (x: numeric) \u2192 numeric</p> <p>Alias of <code>CEILING</code> </p> <p><code>CEILING</code> (x: numeric) \u2192 numeric</p> <p>Returns x rounded up to the nearest integer.  Related: <code>FLOOR</code> </p> <p><code>E</code> () \u2192 numeric</p> <p>Returns the constant e, also known as Euler's number. Related: <code>LN</code>.</p> <p><code>FLOOR</code> (x: numeric) \u2192 numeric</p> <p>Returns x rounded down to the nearest integer.   </p> <p><code>PHI</code> () \u2192 numeric</p> <p>Returns the constant \u03c6 (phi), also known as the golden ratio.  </p> <p><code>PI</code> () \u2192 numeric</p> <p>Returns the constant \u03c0 (pi).  </p> <p><code>POWER</code> (base: numeric, exponent: numeric**) \u2192 _numeric</p> <p>Returns base to the power of exponent.  </p> <p><code>LN</code> (x: numeric) \u2192 numeric</p> <p>Returns the natural logarithm of x. Related: <code>E</code>, <code>LOG</code>, <code>LOG10</code>, <code>LOG2</code></p> <p><code>LOG</code> (x: numeric, base: numeric) \u2192 numeric</p> <p>Returns the logarithm of x for base base.  Related: <code>LN</code>, <code>LOG10</code>, <code>LOG2</code></p> <p><code>LOG10</code> (x: numeric) \u2192 numeric</p> <p>Returns the logarithm for base 10 of x. Related: <code>LN</code>, <code>LOG</code>, <code>LOG2</code></p> <p><code>LOG2</code> (x: numeric) \u2192 numeric</p> <p>Returns the logarithm for base 2 of x. Related: <code>LN</code>, <code>LOG</code>, <code>LOG10</code></p> <p><code>ROUND</code> (x: numeric) \u2192 numeric</p> <p>Returns x rounded to the nearest integer. </p> <p><code>ROUND</code> (x: numeric, places: numeric) \u2192 numeric</p> <p>Returns x rounded to places decimal places.</p> <p><code>SIGN</code> (x: numeric) \u2192 numeric</p> <p>Returns the signum function of x; 0 if x is 0, -1 if x is less than 0 and 1 if x is greater than 0.</p> <p><code>SIGNUM</code> (x: numeric) \u2192 numeric</p> <p>Alias for <code>SIGN</code></p> <p><code>SQRT</code> (x: numeric) \u2192 numeric</p> <p>Returns the square root of x.</p> <p><code>TRUNC</code> (x: numeric) \u2192 numeric</p> <p>Alias of <code>TRUNCATE</code> </p> <p><code>TRUNCATE</code> (x: numeric) \u2192 numeric</p> <p>Returns x rounded to integer by dropping digits after decimal point.    </p>"},{"location":"sql-reference/functions/#string-functions","title":"String Functions","text":"<p>Functions for examining and manipulating string values. </p> <p>str: varchar<code>[</code>index: numeric<code>]</code> \u2192 varchar</p> <p>Subscript operator, return the indexth character from str. </p> <p><code>CONCAT</code> (list: array&lt;varchar&gt;) \u2192 varchar</p> <p>Returns the result of concatenating, or joining, of two or more string values in an end-to-end manner. Related: <code>CONCAT_WS</code></p> <p><code>CONCAT_WS</code> (separator: varchar, list: array&lt;varchar&gt;) \u2192 varchar</p> <p>Returns the result of concatenating, or joining, of two or more string values with a separator used to delimit individual values. Related: <code>CONCAT</code></p> <p><code>ENDS_WITH</code> (str: varchar, value: varchar) \u2192 boolean</p> <p>Return <code>true</code> if str ends with value. Related: <code>STARTS_WITH</code></p> <p><code>GET</code> (str: varchar, index: numeric) \u2192 varchar</p> <p>Alias of str<code>[</code>index<code>]</code> </p> <p><code>LEFT</code> (str: varchar, n: numeric) \u2192 varchar</p> <p>Extract the left-most n characters of str. Related: <code>RIGHT</code></p> <p><code>LEN</code> (str: varchar) \u2192 numeric</p> <p>Alias of <code>LENGTH</code></p> <p><code>LENGTH</code> (str: varchar) \u2192 numeric</p> <p>Returns the length of str in characters.    </p> <p><code>LEVENSHTEIN</code> (str1: varchar, str2: varchar) \u2192 numeric</p> <p>  New in 0.8  Returns the Levenshtein Distance between str1 and str2 </p> <p><code>LOWER</code> (str: varchar) \u2192 varchar</p> <p>Converts str to lowercase.  Related: <code>UPPER</code>, <code>TITLE</code></p> <p><code>LTRIM</code> (str: varchar) \u2192 varchar</p> <p>Remove leading whitespace from str.  Related: <code>RTRIM</code>, <code>TRIM</code></p> <p><code>POSITION</code> (substring: varchar IN string: varchar) \u2192 numeric</p> <p>Returns the starting position of the first instance of substring in string. Positions start with 1. If not found, 0 is returned.   </p> <p><code>REVERSE</code> (str: varchar) \u2192 varchar</p> <p>Returns str with the characters in reverse order.</p> <p><code>RIGHT</code> (str: varchar, n: numeric) \u2192 varchar</p> <p>Extract the right-most n characters of str.  Related: <code>LEFT</code></p> <p><code>RTRIM</code> (str: varchar) \u2192 varchar</p> <p>Remove trailing whitespace from str.  Related: <code>LTRIM</code>, <code>TRIM</code></p> <p><code>SOUNDEX</code> (str: varchar) \u2192 varchar</p> <p>Returns a character string containing the phonetic representation of char. See Soundex \ud83e\udc55.   </p> <p><code>SEARCH</code> (str: varchar, substring: varchar) \u2192 boolean</p> <p>Return <code>true</code> if str contains substring.  </p> <p><code>SUBSTRING</code> (str: varchar, start: numeric) \u2192 varchar</p> <p>Return substring from a string from start position to the end of str.  </p> <p><code>SUBSTRING</code> (str: varchar, start: numeric, length: numeric) \u2192 varchar</p> <p>Return substring from a string from start position for length characters.  </p> <p><code>STARTS_WITH</code> (str: varchar, value: varchar) \u2192 boolean</p> <p>Return <code>true</code> if str starts with value. Related: <code>ENDS_WITH</code></p> <p><code>TITLE</code> (str: varchar) \u2192 varchar</p> <p>Returns str with the first letter of each work in upper case.  Related: <code>LOWER</code>, <code>UPPER</code></p> <p><code>TRIM</code> ( [ LEADING | TRAILING | BOTH ] [ chars: varchar FROM ] str: varchar ) \u2192 varchar</p> <p>  Updated in 0.7  Removes leading and trailing chars from str, if chars is not specified, whitespace is removed. Note that any instance of a character in chars is removed in any order they appear. The LEADING modifier removes chars from the start of str.  The TRAILING modifier removes chars from the end of str.   The BOTH modifier removes chars from both the start and end of str, this is the default behaviour if no positional modifier is supplied.  Related: <code>LTRIM</code>, <code>RTRIM</code></p> <p><code>UPPER</code> (str: varchar) \u2192 varchar</p> <p>Converts str to uppercase.  Related: <code>LOWER</code>, <code>TITLE</code></p>"},{"location":"sql-reference/functions/#struct-functions","title":"Struct Functions","text":"<p>For more details, see Working with Structs.</p> <p>object: struct<code>[</code>key: varchar<code>]</code> \u2192 value</p> <p>Subscript operator, return the value for key from object. </p> <p><code>GET</code> (object: struct, key: varchar) \u2192 value</p> <p>Alias of object<code>[</code>key<code>]</code> </p> <p><code>SEARCH</code> (object: struct, value: varchar) \u2192 boolean</p> <p>Return <code>true</code> if any of the values in object is value. Note <code>SEARCH</code> does not match struct keys.</p>"},{"location":"sql-reference/functions/#system-functions","title":"System Functions","text":"<p><code>VERSION</code> () \u2192 varchar</p> <p>Return the version of the query engine.</p>"},{"location":"sql-reference/functions/#other-functions","title":"Other Functions","text":"<p><code>BASE64_DECODE</code>  (str: varchar) \u2192 varchar</p> <p>Decode BASE64 encoded value, str.  Related: <code>BASE64_ENCODE</code></p> <p><code>BASE64_ENCODE</code> (str: varchar) \u2192 varchar</p> <p>Encode str with BASE64 encoding. Related: <code>BASE64_DECODE</code></p> <p><code>BASE85_DECODE</code> (str: varchar) \u2192 varchar</p> <p>Decode BASE85 encoded value, str.  Related: <code>BASE85_ENCODE</code></p> <p><code>BASE85_ENCODE</code> (str: varchar) \u2192 varchar</p> <p>Encode str with BASE85 encoding. Related: <code>BASE85_DECODE</code></p> <p><code>COALESCE</code> (arg1, arg2, ...) \u2192 value</p> <p>Return the first item from args which is not <code>null</code>.   Related: <code>IFNULL</code></p> <p><code>GENERATE_SERIES</code> (stop: numeric) \u2192 list&lt;numeric&gt;</p> <p>Return a numeric list between 1 and stop, with a step of 1.  </p> <p><code>GENERATE_SERIES</code> (start: numeric, stop: numeric) \u2192 list&lt;numeric&gt;</p> <p>Return a numeric list between start and stop, with a step of 1.</p> <p><code>GENERATE_SERIES</code> (start: numeric, stop: numeric, step: numeric) \u2192 list&lt;numeric&gt;</p> <p>Return a numeric list between start and stop, with an increment of step.</p> <p><code>GENERATE_SERIES</code> (start: timestamp, stop: timestamp, interval) \u2192 list&lt;timestamp&gt;</p> <p>Return a timestamp list between start and stop, with a interval of step.    </p> <p><code>GENERATE_SERIES</code> (cidr: varchar) \u2192 list&lt;varchar&gt;</p> <p>Return a list of IP addresses from a given cidr.   </p> <p><code>HASH</code> (value: any) \u2192 varchar</p> <p>Calculate the CityHash (64 bit) of value.</p> <p><code>HEX_DECODE</code> (str: varchar) \u2192 varchar</p> <p>Decode hexidecimal (BASE16) encoded value, str.   Related: <code>HEX_ENCODE</code></p> <p><code>HEX_ENCODE</code> (str: varchar) \u2192 varchar</p> <p>Encode str with hexadecimal (BASE16) encoding. Related: <code>HEX_DECODE</code></p> <p><code>IIF</code> (condition, true_value, false_value) \u2192 input type</p> <p>Return the true_value if the condition evaluates to <code>True</code>, otherwise return the false_value.</p> <p><code>IFNULL</code> (check_expression: any, replacement_value: any) \u2192 input type</p> <p>Returns check_expression if not <code>null</code>, otherwise returns replacement_value. Related: <code>COALESCE</code> </p> <p><code>NORMAL</code> () \u2192 numeric</p> <p>Random number from a normal (Gaussian) distribution; distribution is centred at 0.0 and has a standard deviation of 1.0. Per record.</p> <p><code>NULLIF</code> (value1: any, value2: any) \u2192 input type</p> <p>Returns <code>null</code> if value1 equals value2, otherwise returns value1. </p> <p><code>MD5</code> (str: varchar) \u2192 varchar</p> <p>Calculate the MD5 hash of str.</p> <p><code>RAND</code> () \u2192 numeric</p> <p>Returns a random number between 0 and 1. Per record.</p> <p><code>RANDOM</code> () \u2192 numeric</p> <p>Alias of <code>RAND</code></p> <p><code>RANDOM_STRING</code> (length: numeric) \u2192 varchar</p> <p>Returns a random string of lowercase alphabetic characters with a length of length. Per record.</p> <p><code>SHA1</code> (str: varchar) \u2192 varchar</p> <p>Calculate the SHA1 hash of str. Related: <code>SHA224</code>, <code>SHA256</code>, <code>SHA384</code>, <code>SHA512</code></p> <p><code>SHA224</code> (str: varchar) \u2192 varchar</p> <p>Calculate the SHA224 hash of str. Related: <code>SHA1</code>, <code>SHA256</code>, <code>SHA384</code>, <code>SHA512</code></p> <p><code>SHA256</code> (str: varchar) \u2192 varchar</p> <p>Calculate the SHA256 hash of str. Related: <code>SHA1</code>, <code>SHA224</code>, <code>SHA384</code>, <code>SHA512</code></p> <p><code>SHA384</code> (str: varchar) \u2192 varchar</p> <p>Calculate the SHA384 hash of stre. Related: <code>SHA1</code>, <code>SHA224</code>, <code>SHA256</code>, <code>SHA512</code></p> <p><code>SHA512</code> (str: varchar) \u2192 varchar</p> <p>Calculate the SHA512 hash of str. Related: <code>SHA1</code>, <code>SHA224</code>, <code>SHA256</code>, <code>SHA384</code></p> <p><code>UNNEST</code> (array: list) \u2192 relation</p> <p>Create a virtual relation with a row for each element in array.</p>"},{"location":"sql-reference/introduction/","title":"SQL Introduction","text":"<p>This tutorial is reworked from the DuckDB tutorial.</p>"},{"location":"sql-reference/introduction/#overview","title":"Overview","text":"<p>This page provides an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an introduction and is not a complete tutorial on SQL.</p> <p>All queries use the internal sample NASA datasets and should work regardless of the data your installation and set up has access to.</p>"},{"location":"sql-reference/introduction/#concepts","title":"Concepts","text":"<p>Opteryx is a system for querying ad hoc data stored in files as relations. A relation is mathematical term for a data table.</p> <p>Each relation is a named collection of rows, organized in columns, each column should be a common datatype. </p> <p>As an ad hoc query engine, the relations and their schema do not need to be predefined, they are determined at the time the query is run. This is one of the reasons Opteryx cannot be considered a RDBMS (relational database management system), even though it can be used to query data using SQL.</p>"},{"location":"sql-reference/introduction/#querying-relations","title":"Querying Relations","text":"<p>To retrieve data from a relation, the relation is queried using a SQL <code>SELECT</code> statement. Basic statements are made of three parts; the list of columns to be returned and the list of relations to retrieve data from, and optional clauses to shape and filter the data that is returned.</p> <pre><code>SELECT *\nFROM $planets;\n</code></pre> <p>The <code>*</code> is shorthand for \"all columns\", by convention keywords are capitalized, and <code>;</code> optionally terminates the query.</p> <pre><code>SELECT id,\nname\nFROM $planets\nWHERE name = 'Earth';\n</code></pre> <p>The output of the above query should be </p> <pre><code> id |  name\n----+-------\n  3 | Earth\n</code></pre> <p>You can write functions, not just simple column references, in the select list. For example, you can write:</p> <pre><code>SELECT id, UPPER(name) AS uppercase_name\nFROM $planets\nWHERE id = 3;\n</code></pre> <p>This should give:</p> <pre><code> id | uppercase_name\n----+----------------\n  3 | EARTH\n</code></pre> <p>Notice how the <code>AS</code> clause is used to relabel the output column. (The <code>AS</code> clause is optional.)</p> <p>A query can be \u201cqualified\u201d by adding a <code>WHERE</code> clause that specifies which rows are wanted. The <code>WHERE</code> clause contains a Boolean (truth value) expression, and only rows for which the Boolean expression is true are returned. The usual Boolean operators (<code>AND</code>, <code>OR</code>, and <code>NOT</code>) are allowed in the qualification. </p> <p>The <code>SELECT</code> clause can be thought of as choosing which columns we want from the relation, and the <code>WHERE</code> clause as choosing which rows we want from the relation.</p> <p></p> <p>For example, the following the planets with fewer than 10 moons and a day longer than 24 hours:</p> <pre><code>SELECT *\nFROM $planets\nWHERE lengthOfDay &gt; 24\nAND numberOfMoons &lt; 10;\n</code></pre> <p>Result:</p> <pre><code>name    | lengthOfDay | numberOfMoons\n--------+-------------+---------------\nMercury |      4222.6 |             0\nVenus   |        2802 |             0\nMars    |        24.7 |             2\nPluto   |       153.3 |             5\n</code></pre> <p>The order of results are not guaranteed and should not be relied upon. If you request the results of the below query, you might get the Mercury or Venus in either order. </p> <p>Note</p> <p>The same query, of the same data in the same version of the query engine will likely to return results in the same order, don't expect to test result order non-determinism by rerunning the query millions of times and looking for differences. These differences may manifest over different versions, or from subtle differences to the query statement or data.</p> <pre><code>SELECT name,\nnumberOfMoons\nFROM $planets\nWHERE numberOfMoons = 0;\n</code></pre> <p>Result:</p> <pre><code>name    | lengthOfDay | numberOfMoons\n--------+-------------+---------------\nMercury |      4222.6 |             0\nVenus   |        2802 |             0\n</code></pre> <p>But you\u2019d always get the results shown above if you do:</p> <pre><code>SELECT name,\nnumberOfMoons\nFROM $planets\nWHERE numberOfMoons = 0\nORDER BY name;\n</code></pre> <p>You can request that duplicate rows be removed from the result of a query:</p> <pre><code>SELECT DISTINCT planetId\nFROM $satellites;\n</code></pre> <p>Result:</p> <pre><code>planetId\n--------\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n</code></pre> <p>Here again, the result row ordering might vary. You can ensure consistent results by using <code>DISTINCT</code> and <code>ORDER BY</code> together:</p> <pre><code>SELECT DISTINCT planetId\nFROM $satellites\nORDER BY planetId;\n</code></pre>"},{"location":"sql-reference/introduction/#joins-between-relations","title":"Joins Between Relations","text":"<p>So far our queries have only accessed one relation at a time. Queries can access multiple relations at once, or access the same relation in such a way that multiple rows of the relation are being processed at the same time. A query that accesses multiple rows of the same or different relations at one time is called a join query. </p> <p>As an example, say you wish to list all the $satellites records together with the planet they orbit. To do that, we need to compare the planetId of each row of the $satellites relation with the id column of all rows in the $planets relation, and return the pairs of rows where these values match.</p> <p>This would be accomplished by the following query:</p> <pre><code>SELECT *\nFROM $satellites, $planets\nWHERE planetId = $planets.id;\n</code></pre> <pre><code>$satellites.id | planetId | $satellites.name | ...\n---------------+----------+------------------+----\n             1 |        3 | Moon             |\n             2 |        4 | Phobos           |\n             3 |        4 | Deimos           |\n             4 |        5 | Io               |\n             5 |        5 | Europa           |\n\n(more rows and columns)\n</code></pre> <p>Observe two things about the result set:</p> <p>There are no result row for the planets of Mercury or Venus (planetIds 1 and 2). This is because there is no matching entry in the $satellites relation for these planets, so the join ignores the unmatched rows in the $planets relation.</p> <p>Each of the relations being joined have an id and a name column, to ensure it is clear which relation the value being displayed is from, columns with clashing names are qualified with the relation name.</p> <p>To avoid abiguity and problems in the future if new columns are added to relations, it is good practice to qualify column names in join conditions:</p> <pre><code>SELECT *\nFROM $satellites, $planets\nWHERE $satellites.planetId = $planets.id;\n</code></pre> <p>Will return the same result as above, but be more resistant to future failure.</p> <p>Join queries of the kind seen thus far can also be written in this alternative form:</p> <pre><code>SELECT *\nFROM $satellites INNER JOIN $planets ON $satellites.planetId = $planets.id;\n</code></pre> <p>The planner currently uses a different execution strategy for these two similar queries, the explicit <code>INNER JOIN</code> style generally executes faster.</p> <p>Now we will figure out how we can get the Mercury and Venus records back in. What we want the query to do is to scan the $planets relation and for each row to find the matching $satellites row(s). If no matching row is found we want some \u201cempty values\u201d to be substituted for the $satellites relations columns. This kind of query is called an outer join. (The joins we have seen so far are inner joins and cross joins.) The command looks like this:</p> <pre><code>SELECT *\nFROM $satellites LEFT OUTER JOIN $planets ON $satellites.planetId = $planets.id;\n</code></pre> <pre><code>$satellites.id | planetId | $satellites.name | ...\n---------------+----------+------------------+----\n               |        1 |                  |\n               |        2 |                  |\n             1 |        3 | Moon             |\n             2 |        4 | Phobos           |\n             3 |        4 | Deimos           |\n             4 |        5 | Io               |\n             5 |        5 | Europa           |\n\n(more rows and columns)\n</code></pre> <p>Using the <code>LEFT OUTER JOIN</code> will mean the relation mentioned on the left of the join operator will have each of its rows in the output at least once, whereas the relation on the right will only have those rows output that match some row of the left relation. When outputting a left-relation row for which there is no right-relation match, empty (<code>null</code>) values are substituted for the right-relation columns. </p> <p>Note</p> <p>How <code>null</code> values are displayed may be different between different systems, common approaches are to display an empty cell or display 'none' or 'null' in an alternate format (e.g. italics or different font color). This is not controlled by the query engine.</p>"},{"location":"sql-reference/introduction/#aggregate-functions","title":"Aggregate Functions","text":"<p>An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the <code>COUNT</code>, <code>SUM</code>, <code>AVG</code> (average), <code>MAX</code> (maximum) and <code>MIN</code> (minimum) over a set of rows.</p>"},{"location":"sql-reference/joins/","title":"Joins","text":"<p>Joins allow you to combine data from multiple relations into a single relation. There are a number of different join types available, each combines relations in different ways.</p>"},{"location":"sql-reference/joins/#cross-join","title":"CROSS JOIN","text":"<p><pre><code>FROM left_relation CROSS JOIN right_relation\n</code></pre> <pre><code>FROM left_relation, right_relation\n</code></pre></p> <p>A <code>CROSS JOIN</code> returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explicit <code>CROSS JOIN</code> syntax or by specifying multiple relations in the <code>FROM</code> clause.</p> <pre><code>SELECT *\nFROM left_relation\nCROSS JOIN right_relation;\n</code></pre> <p></p> <p>The size of the resultant dataset when using <code>CROSS JOIN</code> is length of the two datasets multiplied together (2 x 3 = 6, in the pictorial example), which can easily result in extremely large datasets. When an alternate join approach is possible, it will almost always perform better than a <code>CROSS JOIN</code>.</p>"},{"location":"sql-reference/joins/#inner-join","title":"INNER JOIN","text":"<pre><code>FROM left_relation [ INNER ] JOIN right_relation &lt; ON condition | USING (column) &gt;\n</code></pre> <p>An <code>INNER JOIN</code> returns rows from both relations where the value in the joining column of one relation matches the value in the joining column of the other relation. Inner joins can either be specified using the full <code>INNER JOIN</code> syntax or the shorter <code>JOIN</code> syntax, and the joining logic specified using <code>ON condition</code> or <code>USING(column)</code> syntax.</p> <pre><code>SELECT *\nFROM left_relation\nINNER JOIN right_relation\nON left_relation.column_name = right_relation.column_name;\n</code></pre> <p></p> <p>In this example, the blue column is used as the joining column in both relations. Only the value <code>1</code> occurs in both relations so the resultant dataset is the combination of the row with <code>1</code> in right_relation and the row with <code>1</code> in left_relation.</p>"},{"location":"sql-reference/joins/#left-join","title":"LEFT JOIN","text":"<pre><code>FROM left_relation LEFT [ OUTER ] JOIN right_relation ON condition\n</code></pre> <p>A <code>LEFT JOIN</code> returns all rows from the left relation, and rows from the right relation where there is a matching row, otherwise the fields for the right relation are populated with <code>NULL</code>.</p> <pre><code>SELECT *\nFROM left_relation\nLEFT OUTER JOIN right_relation\nON left_relation.column_name = right_relation.column_name;\n</code></pre> <p></p>"},{"location":"sql-reference/joins/#right-join","title":"RIGHT JOIN","text":"<p>A <code>RIGHT JOIN</code> is the same as a <code>LEFT JOIN</code> with the relations swapped.</p>"},{"location":"sql-reference/joins/#full-join","title":"FULL JOIN","text":"<pre><code>FROM left_relation FULL [ OUTER ] JOIN right_relation ON condition\n</code></pre> <p>The <code>FULL JOIN</code> keyword returns all rows from the left relation, and all rows from the right relation. Where they have a matching value in the joining column, the rows will be aligned, otherwise the fields will be populated with <code>NULL</code>.</p> <pre><code>SELECT *\nFROM left_relation\nFULL OUTER JOIN right_relation\nON left_relation.column_name = right_relation.column_name;\n</code></pre> <p></p>"},{"location":"sql-reference/statements/","title":"Statements","text":"<p>The following statement forms are supported.</p>"},{"location":"sql-reference/statements/#explain","title":"EXPLAIN","text":"<p>Show the logical execution plan of a statement.</p> <pre><code>EXPLAIN\nstatement\n</code></pre> <p>The <code>EXPLAIN</code> clause outputs a summary of the execution plan for the query in the <code>SELECT</code> statement.</p> <p>Warning</p> <p>The data returned by the <code>EXPLAIN</code> statement is intended for interactive usage only and the output format may change between releases. Applications should not depend on the output of the <code>EXPLAIN</code> statement.</p>"},{"location":"sql-reference/statements/#select","title":"SELECT","text":"<p>Retrieve rows from zero or more relations.</p> <pre><code>  WITH &lt;cte&gt; AS &lt;statement&gt; [, ..] SELECT [ DISTINCT ] &lt;expression&gt; [, ..]\nFROM &lt;relation&gt; [AS &lt;alias&gt;]\nFOR &lt;period&gt; [ WITH (NO_CACHE|NO_PARTITION|NO_PUSH_PROJECTION|NO_PUSH_SELECTION) ]\n[ INNER ] JOIN &lt;relation&gt; | &lt;function&gt; | (&lt;subquery&gt;)\nCROSS JOIN &lt;relation&gt; | &lt;function&gt; | (&lt;subquery&gt;)\nLEFT [ OUTER ] JOIN &lt;relation&gt; | &lt;function&gt; | (&lt;subquery&gt;)\nRIGHT [ OUTER ] JOIN &lt;relation&gt; | &lt;function&gt; | (&lt;subquery&gt;)\nFULL [ OUTER ] JOIN &lt;relation&gt; | &lt;function&gt; | (&lt;subquery&gt;)\nON &lt;expression&gt;\nUSING (&lt;columns&gt;)\nWHERE &lt;expression&gt; [ AND | OR | XOR .. ]\nGROUP BY HAVING &lt;expression&gt; [ AND | OR | XOR .. ]\nORDER BY &lt;expression&gt; [, ..]\nOFFSET &lt;offset&gt;\nLIMIT &lt;limit&gt;\n</code></pre>"},{"location":"sql-reference/statements/#with-clause","title":"WITH clause","text":"<pre><code>WITH &lt;cte&gt; AS &lt;statement&gt; [, ..] \n</code></pre> <p>The <code>WITH</code> clause, known as a Common Table Expression, or CTE, is used to define a temporary view which can be referenced within <code>FROM</code> and <code>JOIN</code> clauses. The statement in the CTE supports the full syntax for <code>SELECT</code> statements.</p> <p>Unlike some platforms, Opteryx handles CTEs by inserting them as subqueries into the main statement and not by executing them and referencing the result.</p>"},{"location":"sql-reference/statements/#select-clause","title":"SELECT clause","text":"<pre><code>SELECT [ DISTINCT ] expression [, ...]\n</code></pre> <p>The <code>SELECT</code> clause specifies the list of columns that will be returned by the query. While it appears first in the clause, logically the expressions here are executed after most other clauses. The <code>SELECT</code> clause can contain arbitrary expressions that transform the output, as well as aggregate functions.</p> <p>The <code>DISTINCT</code> modifier is specified, only unique rows are included in the result set. In this case, each output column must be of a type that allows comparison.</p>"},{"location":"sql-reference/statements/#from-join-clauses","title":"FROM / JOIN clauses","text":"<p><pre><code>FROM relation [AS alias] [FOR period] [WITH (NO_CACHE, NO_PARTITION, NO_PUSH_PROJECTION, NO_PUSH_SELECTION)] [, ...] \n</code></pre> <pre><code>FROM relation [AS alias] [FOR period] [ INNER ] JOIN relation [FOR period] &lt; USING (columns) | ON condition &gt;\n</code></pre> <pre><code>FROM relation [AS alias] [FOR period] LEFT [ OUTER ] JOIN relation [FOR period] &lt; USING (columns) | ON condition &gt;\n</code></pre> <pre><code>FROM relation [AS alias] [FOR period] &lt; RIGHT | FULL &gt; [OUTER ] JOIN relation [FOR period]\n</code></pre> <pre><code>FROM relation [AS alias] [FOR period] CROSS JOIN &lt; relation [FOR period] | UNNEST(column) &gt;\n</code></pre></p> <p>The <code>FROM</code> clause specifies the source of the data on which the remainder of the query should operate. Logically, the <code>FROM</code> clause is where the query starts execution. The <code>FROM</code> clause can contain a single relation, a combination of multiple relations that are joined together, or another <code>SELECT</code> query inside a subquery node.</p> <p><code>JOIN</code> clauses allow you to combine data from multiple relations. If no <code>JOIN</code> qualifier is provided, <code>INNER</code> will be used. <code>JOIN</code> qualifiers are mutually exclusive. <code>ON</code> and <code>USING</code> clauses are also mutually exclusive and can only be used with <code>INNER</code> and <code>LEFT</code> joins.</p> <p>See Joins for more information on <code>JOIN</code> syntax and functionality.</p> <p>Hints can be provided as part of the statement to direct the query planner and executor to make decisions. Relation hints are declared as <code>WITH</code> statements following a relation in the <code>FROM</code> and <code>JOIN</code> clauses, for example <code>FROM $astronauts WITH (NO_CACHE)</code>. Reconised hints are:</p> Hint Effect NO_CACHE Ignores any cache configuration NO_PARTITION Do not use partition configuration when reading NO_PUSH_PROJECTION Do not attempt to prune columns when reading NO_PUSH_SELECTION Do not use the source system to prefilter rows <p>Note</p> <p>Hints are not guaranteed to be followed, the query planner and executor may ignore hints in specific circumstances.</p>"},{"location":"sql-reference/statements/#for-clause","title":"FOR clause","text":"<p><pre><code>FOR date\n</code></pre> <pre><code>FOR DATES BETWEEN start AND end\n</code></pre> <pre><code>FOR DATES IN range\n</code></pre></p> <p>The <code>FOR</code> clause is a non-standard clause which filters data by the date it was recorded for. When provided <code>FOR</code> clauses must directly follow the relation in a <code>FROM</code> or <code>JOIN</code> clause. If not provided <code>FOR TODAY</code> is assumed.</p> <p>See Time Travel for more information on <code>FOR</code> syntax and functionality.</p>"},{"location":"sql-reference/statements/#where-clause","title":"WHERE clause","text":"<pre><code>WHERE condition\n</code></pre> <p>The <code>WHERE</code> clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Logically the <code>WHERE</code> clause is applied immediately after the <code>FROM</code> clause.</p>"},{"location":"sql-reference/statements/#group-by-having-clauses","title":"GROUP BY / HAVING clauses","text":"<p><pre><code>GROUP BY expression [, ...]\n</code></pre> <pre><code>HAVING group_filter\n</code></pre></p> <p>The <code>GROUP BY</code> clause specifies which grouping columns should be used to perform any aggregations in the <code>SELECT</code> clause. If the <code>GROUP BY</code> clause is specified, the query is always an aggregate query, even if no aggregations are present in the <code>SELECT</code> clause. The <code>HAVING</code> clause specifies filters to apply to aggregated data, <code>HAVING</code> clauses require a <code>GROUP BY</code> clause.</p> <p><code>GROUP BY</code> expressions may use column numbers, however, this is not recommended for statements intended for reuse. </p>"},{"location":"sql-reference/statements/#order-by-limit-offset-clauses","title":"ORDER BY / LIMIT / OFFSET clauses","text":"<p><pre><code>ORDER BY expression [ ASC | DESC ] [, ...]\n</code></pre> <pre><code>OFFSET count\n</code></pre> <pre><code>LIMIT count\n</code></pre></p> <p><code>ORDER BY</code>, <code>LIMIT</code> and <code>OFFSET</code> are output modifiers. Logically they are applied at the very end of the query. The <code>OFFSET</code> clause discards initial rows from the returned set, the <code>LIMIT</code> clause restricts the amount of rows fetched, and the <code>ORDER BY</code> clause sorts the rows on the sorting criteria in either ascending or descending order.</p> <p><code>ORDER BY</code> expressions may use column numbers, however, this is not recommended for statements intended for reuse.</p>"},{"location":"sql-reference/statements/#set","title":"SET","text":"<p>Specifies the value of a variable, the variable is available to the scope of the executing query batch.</p> <pre><code>SET variable = value\n</code></pre> <p>User defined variable names must be prefixed with an 'at' symbol (<code>@</code>) and the value must be a literal value. The variable can be used within <code>SELECT</code> clauses within the same query batch. A <code>SET</code> statement without a <code>SELECT</code> statement is invalid.</p> <p>System parameters can also be temporarily for a query batch and are prefixed with a dollar sign (<code>$</code>).</p> <p>Related: <code>SHOW VARIABLES</code> and <code>SHOW PARAMETER</code></p>"},{"location":"sql-reference/statements/#show-columns","title":"SHOW COLUMNS","text":"<p>List the columns in a relation along with their data type. Without any modifiers, <code>SHOW COLUMNS</code> only reads a single page of data before returning.</p> <pre><code>SHOW [EXTENDED] [FULL] COLUMNS\nFROM relation\nLIKE pattern\nFOR period\n</code></pre>"},{"location":"sql-reference/statements/#extended-modifier","title":"EXTENDED modifier","text":"<p>Inclusion of the <code>EXTENDED</code> modifier includes summary statistics about the columns which take longer and more memory to create than the standard summary information without the modifier. The summary information varies between column types and values.</p>"},{"location":"sql-reference/statements/#full-modifier","title":"FULL modifier","text":"<p>Inclusion of the <code>FULL</code> modifier uses the entire dataset in order to return complete column information, rather than just the first page from the dataset.</p>"},{"location":"sql-reference/statements/#like-clause","title":"LIKE clause","text":"<pre><code>LIKE pattern\n</code></pre> <p>A case-insensitive <code>LIKE</code> clause to filter the results to the desired subset by the column name. This does not require a left-hand operator, it will always filter by the column name.</p>"},{"location":"sql-reference/statements/#for-clause_1","title":"FOR clause","text":"<p><pre><code>FOR date\n</code></pre> <pre><code>FOR DATES BETWEEN start AND end\n</code></pre> <pre><code>FOR DATES IN range\n</code></pre></p> <p>The <code>FOR</code> clause is a non-standard clause which filters data by the date it was recorded for. When provided <code>FOR</code> clauses must directly follow the relation name the <code>FROM</code> clause. If not provided <code>FOR TODAY</code> is assumed.</p> <p>See Time Travel for more information on <code>FOR</code> syntax and functionality.</p>"},{"location":"sql-reference/statements/#show-create-table","title":"SHOW CREATE TABLE","text":"<p>Show an approximation of the SQL to create a specified relation.</p> <pre><code>SHOW CREATE TABLE table\nFOR period\n</code></pre> <p>The SQL generated by this statement is unlikely to be able to be used with any SQL engine to create the table without some intervention and edits. It is intended to reduce effort to obtain this SQL, not eliminate it.</p>"},{"location":"sql-reference/statements/#for-clause_2","title":"FOR clause","text":"<p><pre><code>FOR date\n</code></pre> <pre><code>FOR DATES BETWEEN start AND end\n</code></pre> <pre><code>FOR DATES IN range\n</code></pre></p> <p>The <code>FOR</code> clause is a non-standard clause which filters data by the date it was recorded for. When provided <code>FOR</code> clauses must directly follow the relation name. If not provided <code>FOR TODAY</code> is assumed.</p>"},{"location":"sql-reference/statements/#show-functions","title":"SHOW FUNCTIONS","text":"<p>List the functions and aggregators supported by the engine.</p> <pre><code>SHOW FUNCTIONS\nLIKE pattern\n</code></pre>"},{"location":"sql-reference/statements/#like-clause_1","title":"LIKE clause","text":"<pre><code>LIKE pattern\n</code></pre> <p>A case-insensitive <code>LIKE</code> clause to filter the results to the desired subset by the function name. This does not require a left-hand operator, it will always filter by the function name.</p>"},{"location":"sql-reference/statements/#show-parameter","title":"SHOW PARAMETER","text":"<p>Display the value of a given configuration setting.</p> <pre><code>SHOW PARAMETER parameter\n</code></pre>"},{"location":"sql-reference/statements/#show-stores","title":"SHOW STORES","text":"<p>Display the set of configured data stores.</p> <pre><code>SHOW STORES\n</code></pre>"},{"location":"sql-reference/statements/#show-variables","title":"SHOW VARIABLES","text":"<p>List the variables set in the query batch.</p> <pre><code>SHOW VARIABLES\nLIKE pattern\n</code></pre>"},{"location":"sql-reference/statements/#like-clause_2","title":"LIKE clause","text":"<pre><code>LIKE pattern\n</code></pre> <p>A case-insensitive <code>LIKE</code> clause to filter the results to the desired subset by the variable name. This does not require a left-hand operator, it will always filter by the variable name.</p> <p>Related: <code>SET</code></p>"}]}