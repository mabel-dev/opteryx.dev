{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"blog/202209-building-our-first-optimizer/","text":"Building our first Optimizer 1st October 2022 Building a SQL query engine is no trivial task, we started building Opteryx about 10 months ago on a complete rewrite of our first query engine. It's only recently we've started to focus on improving performance, initially the performance of the new engine was such a massive improvement over the old, almost purely due to architectural differences, there is a lot of room for improvement. We since users started using the engine about five months ago we've been able to implement some point improvements. Selection before Aggregation, this isn't implemented by an Optimizer, instead it was part of the Aggregator Operator. We've also implemented page merging into the Selection Operator to make the most of SIMD and parallel execution. This week saw the first iteration of plan-based optimization, where the Optimizer receives a query plan and uses rules to rewrite it to run faster. All complex systems are built on something simple. Our first iteration of the Optimizer is simple. It currently does one action - it takes AND conjunctions in the WHERE and HAVING clauses and splits them into individual Selection Operators to run in serial. The effect of doing this, with no intelligence to the order or other factors, has been observed to be up to a 85% reduction in the execution time of the Selection step of real-world queries - i.e. ones we see being written and run by users. The query time is still dominated by the read time in most situations, but saving 6 seconds on a 42 second query is still meaningful. The Optimizer is heuristic, that is, it has some simple rules when it applies. It is not cost-based as we've not built a statistics model and store to compliment the Optimizer yet. This is the expected evolution of the planner, but for now we will focus on extending the rule-based planner with more rules. We look forward to being able to update you on other major updates and improvements to Opteryx .","title":"Building our first Optimizer"},{"location":"blog/202209-building-our-first-optimizer/#building-our-first-optimizer","text":"1st October 2022 Building a SQL query engine is no trivial task, we started building Opteryx about 10 months ago on a complete rewrite of our first query engine. It's only recently we've started to focus on improving performance, initially the performance of the new engine was such a massive improvement over the old, almost purely due to architectural differences, there is a lot of room for improvement. We since users started using the engine about five months ago we've been able to implement some point improvements. Selection before Aggregation, this isn't implemented by an Optimizer, instead it was part of the Aggregator Operator. We've also implemented page merging into the Selection Operator to make the most of SIMD and parallel execution. This week saw the first iteration of plan-based optimization, where the Optimizer receives a query plan and uses rules to rewrite it to run faster. All complex systems are built on something simple. Our first iteration of the Optimizer is simple. It currently does one action - it takes AND conjunctions in the WHERE and HAVING clauses and splits them into individual Selection Operators to run in serial. The effect of doing this, with no intelligence to the order or other factors, has been observed to be up to a 85% reduction in the execution time of the Selection step of real-world queries - i.e. ones we see being written and run by users. The query time is still dominated by the read time in most situations, but saving 6 seconds on a 42 second query is still meaningful. The Optimizer is heuristic, that is, it has some simple rules when it applies. It is not cost-based as we've not built a statistics model and store to compliment the Optimizer yet. This is the expected evolution of the planner, but for now we will focus on extending the rule-based planner with more rules. We look forward to being able to update you on other major updates and improvements to Opteryx .","title":"Building our first Optimizer"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/","text":"Writing a SQL Engine Motivation No-one in their right mind would write a SQL Engine if they didn't need to. There are a lot of options in the space of providing SQL query access to distributed data - with a few players dominating the market like Trino, DuckDB and SQLite. We had a problem where we wanted a SQL interface to our data, but none of the existing tools were a good fit for our situation. We could change ourselves to fit an existing toolset, but wanted to explore other options before committing to vendor-defined design. Prior Attempts The data store we're working with was designed to be transctional (read a row of data, process it, save the result, repeat). We use JSON lines files, which for this use case we were unable to find anything better in the sweet spot of human and machine readable, and performance to read and write. With this as the datastore, our first attempt at a SQL engine was also transactional, following what is known as the Volcano Model. This aligned well with the tools that we had written to process the data so most of the effort was with translating the SQL syntax to filters that the existing tools could understand. Functionality like GROUP BY was added to make it feel more like a database and less like a log-viewer. This provided an acceptable level of functionality for single-table queries (the existing tools only ever read from one table and write to one table) and the engine was implemented into user-facing systems. As data grew, we started to hit problems. Reading tens of million of rows, constraints outside the control of the system meant that jobs that ran for longer than 180 seconds were terminated. This generally meant that queries with more than about 30 million records (or far fewer records but with calculations) timed out. A lot of queries were still able to be run as not everything hit these thresholds, but it couldn't be used for large data analysis. Redesign The decision to write the SQL Engine as a new library separate from the existing data handling library was made early it freed us from some of the implementation decisions of the existing solution. XXX leveraging Apache Arrow (using Parquet) to help improve performance. Parquet was assessed for the transactional use case but the optimized JSONL implementation in Mabel consistently outperformed Parquet. However, reassessing performance for a SQL engine, Parquet out performs JSONL. The previous SQL Engine had a fixed execution plan, this meant that no matter what your query was it followed the same steps, with some steps doing nothing. This was simplier to write, but will have affected performance. Opteryx creates a query plan, the initial version doesn't optimize this plan by doing things like running selections and projections early, but it does only add steps to the plan that are required. Writing a robust SQL parser is hard, there are a considerable number of rules and exceptions that need to be followed. We chose SqlOxide to parse the statements to an AST. There is still a lot of work to convert the AST to a query plan - but the AST removes ambiguity, something like SELECT SELECT FROM FROM FROM would be difficult to parse with a regex-based parser.","title":"Writing a SQL Engine"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#writing-a-sql-engine","text":"","title":"Writing a SQL Engine"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#motivation","text":"No-one in their right mind would write a SQL Engine if they didn't need to. There are a lot of options in the space of providing SQL query access to distributed data - with a few players dominating the market like Trino, DuckDB and SQLite. We had a problem where we wanted a SQL interface to our data, but none of the existing tools were a good fit for our situation. We could change ourselves to fit an existing toolset, but wanted to explore other options before committing to vendor-defined design.","title":"Motivation"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#prior-attempts","text":"The data store we're working with was designed to be transctional (read a row of data, process it, save the result, repeat). We use JSON lines files, which for this use case we were unable to find anything better in the sweet spot of human and machine readable, and performance to read and write. With this as the datastore, our first attempt at a SQL engine was also transactional, following what is known as the Volcano Model. This aligned well with the tools that we had written to process the data so most of the effort was with translating the SQL syntax to filters that the existing tools could understand. Functionality like GROUP BY was added to make it feel more like a database and less like a log-viewer. This provided an acceptable level of functionality for single-table queries (the existing tools only ever read from one table and write to one table) and the engine was implemented into user-facing systems. As data grew, we started to hit problems. Reading tens of million of rows, constraints outside the control of the system meant that jobs that ran for longer than 180 seconds were terminated. This generally meant that queries with more than about 30 million records (or far fewer records but with calculations) timed out. A lot of queries were still able to be run as not everything hit these thresholds, but it couldn't be used for large data analysis.","title":"Prior Attempts"},{"location":"blog/unpublished/20220205%20Writing%20a%20Query%20Engine/#redesign","text":"The decision to write the SQL Engine as a new library separate from the existing data handling library was made early it freed us from some of the implementation decisions of the existing solution. XXX leveraging Apache Arrow (using Parquet) to help improve performance. Parquet was assessed for the transactional use case but the optimized JSONL implementation in Mabel consistently outperformed Parquet. However, reassessing performance for a SQL engine, Parquet out performs JSONL. The previous SQL Engine had a fixed execution plan, this meant that no matter what your query was it followed the same steps, with some steps doing nothing. This was simplier to write, but will have affected performance. Opteryx creates a query plan, the initial version doesn't optimize this plan by doing things like running selections and projections early, but it does only add steps to the plan that are required. Writing a robust SQL parser is hard, there are a considerable number of rules and exceptions that need to be followed. We chose SqlOxide to parse the statements to an AST. There is still a lot of work to convert the AST to a query plan - but the AST removes ambiguity, something like SELECT SELECT FROM FROM FROM would be difficult to parse with a regex-based parser.","title":"Redesign"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/","text":"Introduction to Query Engines Scope Just the Data Query Language aspects - that\u2019s more or less the bit that handles SELECT statements. Will cover generic aspects of implementation, but will include detail relating to Opteryx. Key Steps Query Language Interpretation Query Planning and Optimization Execution Engine Files / Storage Key Steps SQL -> Query Language Interpretation Abstract Syntax Tree -> Query Planning and Optimization Query Plan -> Execution Engine Resource Access -> Files / Storage Result Creation Key Components Parser / Lexer Interprets SQL into a semantic representation (AST) Abstract Syntax Tree (AST) First machine processable representation of the query (we can rewrite the query here) Query Plan Describes the steps to take to fulfil the request Optimizer Reworks the Query Plan to improve performance Executor Runs the Query Plan Fixed Query Plan Based on Relational Algrebra. This is the order items are processed before optimizations. Has implications, e.g. can\u2019t GROUP BY aliases defined in the SELECT clause. Naive Plan Order SELECT (5) [project] DISTINCT (6) [distinct] FROM (1) WHERE (2) [select] GROUP BY (3) [aggregate] HAVING (4) [select] ORDER BY (7) [sort] OFFSET (8) LIMIT (9) Plan Optimization Optimized plan has to create the same result as naive plan. Get rid of data (rows and columns) as quickly as possible Selection ( WHERE ) and Projection ( SELECT ) push-downs LIMIT push-downs Algorithm Decisions Choose JOIN order and algorithm (HASH or SORT MERGE) Execution Models Row Processing (Volcano) Mabel Block/Column Processing (Vectorized) Opteryx Volcano Model 1) The step at the end of our plan tries to return a record 1) It asks the previous step, which asks the previous step 1) Until we get to the files, which we read line-by-line All calculations are done on each line, one at a time Block/Column Processing 1) The step at the end of our plan tries to return a block 1) It asks the previous step, which asks the previous step 1) Util we get to the files, which we read an entire file/block All calculations are done block at a time.","title":"**Introduction to Query Engines**"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#introduction-to-query-engines","text":"","title":"Introduction to Query Engines"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#scope","text":"Just the Data Query Language aspects - that\u2019s more or less the bit that handles SELECT statements. Will cover generic aspects of implementation, but will include detail relating to Opteryx.","title":"Scope"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-steps","text":"Query Language Interpretation Query Planning and Optimization Execution Engine Files / Storage","title":"Key Steps"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-steps_1","text":"SQL -> Query Language Interpretation Abstract Syntax Tree -> Query Planning and Optimization Query Plan -> Execution Engine Resource Access -> Files / Storage Result Creation","title":"Key Steps"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#key-components","text":"Parser / Lexer Interprets SQL into a semantic representation (AST) Abstract Syntax Tree (AST) First machine processable representation of the query (we can rewrite the query here) Query Plan Describes the steps to take to fulfil the request Optimizer Reworks the Query Plan to improve performance Executor Runs the Query Plan","title":"Key Components"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#fixed-query-plan","text":"Based on Relational Algrebra. This is the order items are processed before optimizations. Has implications, e.g. can\u2019t GROUP BY aliases defined in the SELECT clause.","title":"Fixed Query Plan"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#naive-plan-order","text":"SELECT (5) [project] DISTINCT (6) [distinct] FROM (1) WHERE (2) [select] GROUP BY (3) [aggregate] HAVING (4) [select] ORDER BY (7) [sort] OFFSET (8) LIMIT (9)","title":"Naive Plan Order"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#plan-optimization","text":"Optimized plan has to create the same result as naive plan. Get rid of data (rows and columns) as quickly as possible Selection ( WHERE ) and Projection ( SELECT ) push-downs LIMIT push-downs Algorithm Decisions Choose JOIN order and algorithm (HASH or SORT MERGE)","title":"Plan Optimization"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#execution-models","text":"Row Processing (Volcano) Mabel Block/Column Processing (Vectorized) Opteryx","title":"Execution Models"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#volcano-model","text":"1) The step at the end of our plan tries to return a record 1) It asks the previous step, which asks the previous step 1) Until we get to the files, which we read line-by-line All calculations are done on each line, one at a time","title":"Volcano Model"},{"location":"blog/unpublished/202204%20Introduction%20to%20Query%20Engines/#blockcolumn-processing","text":"1) The step at the end of our plan tries to return a block 1) It asks the previous step, which asks the previous step 1) Util we get to the files, which we read an entire file/block All calculations are done block at a time.","title":"Block/Column Processing"},{"location":"blog/unpublished/20220901%20Lessons%20Learnt%20so%20Far/","text":"The conventions in SQL Engines generally make good sense, follow them Have real systems and real users as your beta testers Unit testing is fine, but write hundreds of tests cases which run real SQL queries You can't fabricate test data for all your test scenarios Storage read speed will kill any performance boosts from algorithmic improvements If you don't control the writing of the data - assume the worst PyArrow is awesome, but it has bugs, odd limitations and some parts are so slow it hurts. bugs - date diff just doesn't work for months odd limitations - can't join on tables with arrays or structs so slow it hurts - Abstraction == Slow, if you want fast, you need to get as close to the bare metal or raw API as possible","title":"20220901 Lessons Learnt so Far"},{"location":"blog/unpublished/20221001%20Storage/","text":"there's the classic diagram that shows that on CPU cache is fast, through to spinning rust is slow and it shows scaled timeframes - if it take a second to get data out of registers on the CPU, HDD would take a lifetime. The reference user for Opteryx uses a cloud storage provider for their data - I wish I had spinning rust. There's two broad approaches to speeding up disk access: make disk access faster don't do it","title":"20221001 Storage"},{"location":"contributing/code-of-conduct/","text":"Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via Slack (@joocer) or GitHub (@joocer). All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of, or an extreme violation, of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Code of Conduct"},{"location":"contributing/code-of-conduct/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"contributing/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"contributing/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"contributing/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"contributing/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via Slack (@joocer) or GitHub (@joocer). All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"contributing/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Attribution"},{"location":"contributing/contributing/","text":"Contributor Guide Welcome to the Opteryx Contributor Guide. In this section you will find information to help you to bring your unique skills and experience and join us in building Opteryx. Opteryx is primarily written in Python, but you don't need to be a Python developer to contribute. All contributions, bug reports , documentation improvements, and ideas are welcome. Our Tools GitHub Join us on Github We use GitHub to host the code, track feature requests, perform testing, report bugs and maintain documentation. All submissions, including submissions by core project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Pull requests that add support for, or fix a bug in, a feature in a popular RDBMS, or address deficiency in documentation, will likely be accepted after a brief review. For more substative changes you should start a discussion to coordinate with maintainers. We have included a number of tests which run automatically when code is submitted to help maintain consistency and quality of the codebase, note that these may not be automatically triggered for new contributors. Pull requests which do not meet the quality criteria will not be reviewed or merged. Changes should have: Corresponding unit and regression tests A clean execution of the CI tests Updated documentation (code-level, module-level and docs/ folder as appropriate) Attributed external sources Gitter Gitter is our target channel for interactive conversations and support. Join us on Gitter Licence All code is licensed under the Apache Software License 2.0, unless clearly stated otherwise. Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions. Any submissions with incompatible licence or licence conditions may be rejected from inclusion.","title":"Contributor Guide"},{"location":"contributing/contributing/#contributor-guide","text":"Welcome to the Opteryx Contributor Guide. In this section you will find information to help you to bring your unique skills and experience and join us in building Opteryx. Opteryx is primarily written in Python, but you don't need to be a Python developer to contribute. All contributions, bug reports , documentation improvements, and ideas are welcome.","title":"Contributor Guide"},{"location":"contributing/contributing/#our-tools","text":"","title":"Our Tools"},{"location":"contributing/contributing/#licence","text":"All code is licensed under the Apache Software License 2.0, unless clearly stated otherwise. Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions. Any submissions with incompatible licence or licence conditions may be rejected from inclusion.","title":"Licence"},{"location":"contributing/etiquette/","text":"Community Etiquette Be nice to everyone Assume everyone has good intentions. Check off your resolved questions If you have received a useful reply to your question, please drop a \u2705 reaction or a reply for affirmation. Try not to repost question If you have asked a question and have not got a response in 24hrs, please review your question for clarity and revise it. Post in public Please don't direct message any individual member of Mabel community without their explicit permission, independent of reason. Your question might be helpful for other community members. Don't spam tags Mabel and Opteryx are supported by volunteers, avoid tagging members unless it is urgent. Use threads for discussion To keep the main channel area clear, we request to use threads to keep an ongoing conversation organized.","title":"Etiquette"},{"location":"contributing/etiquette/#community-etiquette","text":"","title":"Community Etiquette"},{"location":"contributing/version-goals/","text":"Version Goals Version goals are set out to provide a view of a road map. Goals are intended to demonstrate direction of enhancements and evolution of the engine, inclusion on the list does not guarantee delivery at a particular point in time, or at all. As always, conditions on the road may require you to consider your current path. Version 1.0 Version 1.0 goals will be delivered across various minor versions building toward v1.0. These minor releases will also include bug fixes, performance improvements and functional completeness. The items listed below are major pieces of functionality or milestones. Connection Python PEP249 compatibility Planner ANSI SQL92 compatibility Planner CTEs ( WITH ) statements supported Planner Read across multiple data sources (e.g. GCS and Postgres in the same query) [v0.2] Planner Support different plaform data sources (e.g. FireStore [v0.3] and BigQuery) Planner Rule-based query optimizer [v0.5] Planner Metastore used in planning an optimizing Execution JOIN statements supported [v0.1] Execution Functions using the result of Functions (e.g. LENGTH(LIST(field)) ) [v0.3] Execution Inline operators (e.g. firstname || surname ) [v0.3] Execution Local Buffer Cache implemented [v0.6] Operation Correctness benchmarks written [v0.6] and acceptable pass-rate obtained Operation Performance benchmarks written [v0.5] and monitored Version 2.0 Version 2.0 goals indicate which items are considered important for the engine to support, but we are willing to lower the priority against other items. Operation Persisted materialized views Execution Distributed execution","title":"Version Goals"},{"location":"contributing/version-goals/#version-goals","text":"Version goals are set out to provide a view of a road map. Goals are intended to demonstrate direction of enhancements and evolution of the engine, inclusion on the list does not guarantee delivery at a particular point in time, or at all. As always, conditions on the road may require you to consider your current path.","title":"Version Goals"},{"location":"contributing/version-goals/#version-10","text":"Version 1.0 goals will be delivered across various minor versions building toward v1.0. These minor releases will also include bug fixes, performance improvements and functional completeness. The items listed below are major pieces of functionality or milestones. Connection Python PEP249 compatibility Planner ANSI SQL92 compatibility Planner CTEs ( WITH ) statements supported Planner Read across multiple data sources (e.g. GCS and Postgres in the same query) [v0.2] Planner Support different plaform data sources (e.g. FireStore [v0.3] and BigQuery) Planner Rule-based query optimizer [v0.5] Planner Metastore used in planning an optimizing Execution JOIN statements supported [v0.1] Execution Functions using the result of Functions (e.g. LENGTH(LIST(field)) ) [v0.3] Execution Inline operators (e.g. firstname || surname ) [v0.3] Execution Local Buffer Cache implemented [v0.6] Operation Correctness benchmarks written [v0.6] and acceptable pass-rate obtained Operation Performance benchmarks written [v0.5] and monitored","title":"Version 1.0"},{"location":"contributing/version-goals/#version-20","text":"Version 2.0 goals indicate which items are considered important for the engine to support, but we are willing to lower the priority against other items. Operation Persisted materialized views Execution Distributed execution","title":"Version 2.0"},{"location":"contributing/internals/project-structure/","text":"Project Structure Folder Structure Opteryx's repository folder structure is described below: opteryx/ <- main opteryx library \u251c\u2500\u2500 connectors/ <- modules to connect to data sources \u251c\u2500\u2500 functions/ <- modules to execute functions within SQL statements \u251c\u2500\u2500 managers/ <- libraries responsible for key functional units \u2502 \u251c\u2500\u2500 expression/ <- modules implementing expression evaluation \u2502 \u251c\u2500\u2500 kvstore/ <- modules implementing interfacing with KV Stores \u2502 \u251c\u2500\u2500 planner/ <- modules implementing query planning and optimizing \u2502 \u2514\u2500\u2500 schemes/ <- modules implementing storage schemes \u251c\u2500\u2500 models/ <- internal data models \u251c\u2500\u2500 operators/ <- modules implementing steps in the query plan \u251c\u2500\u2500 samples/ <- sample data \u251c\u2500\u2500 shared/ <- global resources \u251c\u2500\u2500 third_party/ <- third party code \u2502 \u251c\u2500\u2500 distogram/ \u2502 \u251c\u2500\u2500 fuzzy/ \u2502 \u251c\u2500\u2500 hyperloglog/ \u2502 \u251c\u2500\u2500 pyarrow_ops/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 utils/ <- helper libraries \u2514\u2500\u2500 ...","title":"Project Structure"},{"location":"contributing/internals/project-structure/#project-structure","text":"","title":"Project Structure"},{"location":"contributing/internals/project-structure/#folder-structure","text":"Opteryx's repository folder structure is described below: opteryx/ <- main opteryx library \u251c\u2500\u2500 connectors/ <- modules to connect to data sources \u251c\u2500\u2500 functions/ <- modules to execute functions within SQL statements \u251c\u2500\u2500 managers/ <- libraries responsible for key functional units \u2502 \u251c\u2500\u2500 expression/ <- modules implementing expression evaluation \u2502 \u251c\u2500\u2500 kvstore/ <- modules implementing interfacing with KV Stores \u2502 \u251c\u2500\u2500 planner/ <- modules implementing query planning and optimizing \u2502 \u2514\u2500\u2500 schemes/ <- modules implementing storage schemes \u251c\u2500\u2500 models/ <- internal data models \u251c\u2500\u2500 operators/ <- modules implementing steps in the query plan \u251c\u2500\u2500 samples/ <- sample data \u251c\u2500\u2500 shared/ <- global resources \u251c\u2500\u2500 third_party/ <- third party code \u2502 \u251c\u2500\u2500 distogram/ \u2502 \u251c\u2500\u2500 fuzzy/ \u2502 \u251c\u2500\u2500 hyperloglog/ \u2502 \u251c\u2500\u2500 pyarrow_ops/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 utils/ <- helper libraries \u2514\u2500\u2500 ...","title":"Folder Structure"},{"location":"contributing/internals/query-engine/","text":"Query Engine If you are interested in how databases work, I recommend the resources from The CMU Database Group and the collection of resources at Awesome Database Learning . The Opteryx query engine has the following key components and processing queries follows this high-level series of steps: \u2003 Parser & Lexer recieves the user SQL and builds an Abstract Syntax Tree (AST). \u2003 Binder maps contextual information to the AST. \u2003 Planner recieves the AST and builds a Query Plan. \u2003 Optimizer recieves a Query Plan and rewrites it to improve performance. \u2003 Executor recieves the Query Plan and returns the result dataset. Parser & Lexer The primary goal of the Parser and Lexer (which in some engines is two separate components) is to interpret the SQL provided by the user. This is generally done in two steps, the first is the break the query into separate tokens (or words) and the second is to understand the meaning of those tokens. For example for this statement SELECT SELECT FROM FROM The Parser and Lexer will understand that we're requesting the field SELECT from the relation FROM . Opteryx uses sqlparser-rs as its Parser and Lexer, as a Rust library, Opteryx creates Python bindings for sqlparser-rs (derived from sqloxide ). Opteryx does not support all features and functionality provided by this library. This sqlparser-rs interprets all SQL except for the Temporal FOR clause which are handled separately. Binder The Binder's primary goal is to embelish and replace information in the AST with details which the Parser & Lexer did not have. This is used for replacing variables in queries with their literal equivalents and adding temporal information to relations. Query Planner The Query Planner's primary goal is to convert the AST into a plan to respond to the query. The Query Plan is described in a Directed Acyclic Graph (DAG) with the nodes that acquire the raw data, usually from storage, at the start of the flow and the node that forms the data to return to the user (usually the SELECT step) at the end. The DAG is made of different nodes which process the data as they pass through then node. Different node types exist for processing actions like Aggregations ( GROUP BY ), Selection ( WHERE ) and Distinct ( DISTINCT ). Query plans follow a generally accepted order of execution. This does not match the order queries are usually written in, instead it follows this order: The planner ensures the processes to be applied to the data reflect this order and creates the most convenient plan to achieve this. The Query Plan can be seen for a given query using the EXPLAIN query. Query Optimizer The goal of the Query Optimizer is to rewrite the Query Plan to a plan which will return result to users faster. This is generally achieved through reducing the data being handled as early in the query as possible (such as projection push-down), reducing the complexity of steps (such as using logical equivelences to make expressions simpler) or combining steps (such as sort and limit into a heap sort). The current optimizer in Opteryx is immature with very few rules and requires hand-tuning of the input query for the optimizer to achieve best results. Query Executor The goal of the Query Executor is to produce the results for the user. It takes the Plan and executes the steps in the plan. Opteryx implements a vectorized Volcano model executor. This means that the planner starts at the node closest to the end of the plan (e.g. LIMIT ) and asks it for a page of data. This node asks its preceeding node for a page of data, etc etc until it gets to the node which aquires data from source. The data is then processed by each node until it is returned to the LIMIT node at the end. Performance Features The following features are build into the query engine to improve performance Small pages are merged together (referred to as 'defragmentation') before activities which operate on the entire page-at-a-time (such as selections) Projections are pushed to the blob parser, either to prevent parsing of unwanted fields (Parquet), or before passing to the next operation A buffer pool is used to maintain an in-memory cache of blobs A shared page cache can be used (e.g. memcached) to reduce reads to attached or remote storage An LRU-K cache eviction strategy with a fixed eviction budget per query to help ensure effective use of the page cache Aggressive pruning of date partitioned datasets SIMD and vectorized execution where available (via Numpy and PyArrow ) Projection before GROUP BY to reduce data handled by the aggregators null values are eliminated from filters before they are executed, and added back in after values have been compared, reducing the pointless work of comparing null values","title":"Query Engine"},{"location":"contributing/internals/query-engine/#query-engine","text":"If you are interested in how databases work, I recommend the resources from The CMU Database Group and the collection of resources at Awesome Database Learning . The Opteryx query engine has the following key components and processing queries follows this high-level series of steps: \u2003 Parser & Lexer recieves the user SQL and builds an Abstract Syntax Tree (AST). \u2003 Binder maps contextual information to the AST. \u2003 Planner recieves the AST and builds a Query Plan. \u2003 Optimizer recieves a Query Plan and rewrites it to improve performance. \u2003 Executor recieves the Query Plan and returns the result dataset.","title":"Query Engine"},{"location":"contributing/internals/query-engine/#parser-lexer","text":"The primary goal of the Parser and Lexer (which in some engines is two separate components) is to interpret the SQL provided by the user. This is generally done in two steps, the first is the break the query into separate tokens (or words) and the second is to understand the meaning of those tokens. For example for this statement SELECT SELECT FROM FROM The Parser and Lexer will understand that we're requesting the field SELECT from the relation FROM . Opteryx uses sqlparser-rs as its Parser and Lexer, as a Rust library, Opteryx creates Python bindings for sqlparser-rs (derived from sqloxide ). Opteryx does not support all features and functionality provided by this library. This sqlparser-rs interprets all SQL except for the Temporal FOR clause which are handled separately.","title":"Parser &amp; Lexer"},{"location":"contributing/internals/query-engine/#binder","text":"The Binder's primary goal is to embelish and replace information in the AST with details which the Parser & Lexer did not have. This is used for replacing variables in queries with their literal equivalents and adding temporal information to relations.","title":"Binder"},{"location":"contributing/internals/query-engine/#query-planner","text":"The Query Planner's primary goal is to convert the AST into a plan to respond to the query. The Query Plan is described in a Directed Acyclic Graph (DAG) with the nodes that acquire the raw data, usually from storage, at the start of the flow and the node that forms the data to return to the user (usually the SELECT step) at the end. The DAG is made of different nodes which process the data as they pass through then node. Different node types exist for processing actions like Aggregations ( GROUP BY ), Selection ( WHERE ) and Distinct ( DISTINCT ). Query plans follow a generally accepted order of execution. This does not match the order queries are usually written in, instead it follows this order: The planner ensures the processes to be applied to the data reflect this order and creates the most convenient plan to achieve this. The Query Plan can be seen for a given query using the EXPLAIN query.","title":"Query Planner"},{"location":"contributing/internals/query-engine/#query-optimizer","text":"The goal of the Query Optimizer is to rewrite the Query Plan to a plan which will return result to users faster. This is generally achieved through reducing the data being handled as early in the query as possible (such as projection push-down), reducing the complexity of steps (such as using logical equivelences to make expressions simpler) or combining steps (such as sort and limit into a heap sort). The current optimizer in Opteryx is immature with very few rules and requires hand-tuning of the input query for the optimizer to achieve best results.","title":"Query Optimizer"},{"location":"contributing/internals/query-engine/#query-executor","text":"The goal of the Query Executor is to produce the results for the user. It takes the Plan and executes the steps in the plan. Opteryx implements a vectorized Volcano model executor. This means that the planner starts at the node closest to the end of the plan (e.g. LIMIT ) and asks it for a page of data. This node asks its preceeding node for a page of data, etc etc until it gets to the node which aquires data from source. The data is then processed by each node until it is returned to the LIMIT node at the end.","title":"Query Executor"},{"location":"contributing/internals/query-engine/#performance-features","text":"The following features are build into the query engine to improve performance Small pages are merged together (referred to as 'defragmentation') before activities which operate on the entire page-at-a-time (such as selections) Projections are pushed to the blob parser, either to prevent parsing of unwanted fields (Parquet), or before passing to the next operation A buffer pool is used to maintain an in-memory cache of blobs A shared page cache can be used (e.g. memcached) to reduce reads to attached or remote storage An LRU-K cache eviction strategy with a fixed eviction budget per query to help ensure effective use of the page cache Aggressive pruning of date partitioned datasets SIMD and vectorized execution where available (via Numpy and PyArrow ) Projection before GROUP BY to reduce data handled by the aggregators null values are eliminated from filters before they are executed, and added back in after values have been compared, reducing the pointless work of comparing null values","title":"Performance Features"},{"location":"contributing/internals/storage-engine/","text":"Storage Formats This document primarily applies to the Blob and File stores, such as GCS, S3 and local disk. Supported Data Files Parquet Parquet is the preferred file format for Opteryx and use of Parquet offers optimizations not available with other formats. If a datasource has query performance issues or is hot in terms of query use, converting to Parquet is likely to improve performance. Performance testing suggests Parquet with zStandard compression provides best balance of IO to read the files and CPU to to the files. As will all guidance on performance tuning - this appears to be generally correct but test for your specific circumstances. ORC & Feather Opteryx support ORC and Feather, but not all optimizations implemented for Parquet are implemented for these formats. These will still provide better performance than traditional data formats. JSONL JSONL and zStandard compressed JSONL files. Files don't need an explicit schema, but each partition must have the same columns in the same order in every row of every file. Data types are inferred from the records, where data types are not consistent, the read will fail. Opteryx supports zStandard Compressed JSONL files as created by Mabel, these perform approximately 20% faster than raw JSONL files primarily due to reduced IO. CSV Comma-separated files can be used with Opteryx, however this is only provided to meet the base expectation that the system can support CSV. It is not recommended and limited regression tests are written relating to CSV handling. Storage Layout For Blob/File stores, the path of the data is used as the name of the relation in the query. There are currently two built in data schemas, none (or flat) and Mabel. Flat The flat schema is where the data files are stored in the folder which names the relation, such as: customer/ preferences/ file_1 file_2 file_3 This would be available to query with a query such as: SELECT * FROM customer . preferences ; Which would read the three files to return the query. Mabel The Mabel schema is where data is structured in date labelled folders customer/ preferences/ year_2020/ month_03/ day_04/ file_1 file_2 file_3 The date components of the folder structure are part of the temporal processing, and are not directly referenced as part of the query, instead they form part of the temporal clause ( FOR ) SELECT * FROM customer . preferences FOR '2020-03-04' This approach enables data to be partitioned by date and pruned using temporal filters.","title":"Storage Engine"},{"location":"contributing/internals/storage-engine/#storage-formats","text":"This document primarily applies to the Blob and File stores, such as GCS, S3 and local disk.","title":"Storage Formats"},{"location":"contributing/internals/storage-engine/#supported-data-files","text":"","title":"Supported Data Files"},{"location":"contributing/internals/storage-engine/#storage-layout","text":"For Blob/File stores, the path of the data is used as the name of the relation in the query. There are currently two built in data schemas, none (or flat) and Mabel.","title":"Storage Layout"},{"location":"contributing/internals/testing/","text":"How Opteryx is Tested Opteryx utilizes a number of test approaches to help ensure the system is performant, secure and correct. The key test harnesses which are used to test Opteryx are listed here. Most testing is part of the main Opteryx repository on GitHub, however, some testing is located in other respositories. Unit Testing Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Part of the CI process. Tests specific aspects of the internals. Combined with the SQL Battery test, the aim is for 95% coverage (with explicit exceptions). Whilst 95% coverage does not ensure the tests are 'good', it does help ensures any material changes to the function of the system are captured early. SQL Battery Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Part of the CI process. Executes hundreds of hand-crafted SQL statements against the engine. The SQL Battery helps to ensure the entire system performs as expected and when used in tandem with Unit Testing, which primarily focuses on ensuring parts work as they should, this provides a level of confidence that the system continues to perform as expected. The battery essentially has four variations: Does the Query run - with no checking or validation of the outputs Does the Query fail - for scenarios when the query is expected to fail Is the shape of the results as expected - only the row and column counts are checked Does the Query return the right results - the returned dataset is checked The SQL Battery the most effective test to identify when functionality has been broken or changed by updates. The shape testing is currently considered the best value of this suite - it is fast and easy to write new tests for this suite, and the execution give reasonable considence in the correctness of the result in most situations. Performance Testing Frequency : Ad hoc Maturity : Low Location : mabel-dev/wrenchy-bench The performance testing framework is only able to be run ad hoc, and there is currently no meaningful treatment or tracking out outcomes. It us currently used to confirm optimizations do have the impact of reducing query execution times. SQL Logic Test Frequency : Ad hoc Maturity : Low Location : mabel-dev/wrenchy-bench Runs SQL statements in both Operyx and DuckDB to verify Opteryx returns the same answer as DuckDB. This has a growing set of tests which are executed, but as how relations are referenced in these systems, most queries require some hand-tuning and many are not possible with the framework as it currently is written. What has been able to be tested has demonstrated some deviation between these systems, so is a valuable and useful test, even in its current form. Fuzzing Frequency : On Commit to GitHub & Nightly Maturity : Low Location : mabel-dev/opteryx As part of the CI process, executes 100 iterations of random inputs. As part of a nightly test, executes 100,000 iterations of random inputs. Fuzzing supplies some key functions with random data to try to capture scenarios which are unexpected or unhandled. Security & Code Quality Testing Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Various other tests are performed to help ensure code quality is maintained, these include security, form, typing, secret detection and test coverage checks using the following tools: Bandit, Semgrep, Black, MyPy, PyLint, PerfLint, Fides, SonarCloud, and Coverage.","title":"Testing Approach"},{"location":"contributing/internals/testing/#how-opteryx-is-tested","text":"Opteryx utilizes a number of test approaches to help ensure the system is performant, secure and correct. The key test harnesses which are used to test Opteryx are listed here. Most testing is part of the main Opteryx repository on GitHub, however, some testing is located in other respositories.","title":"How Opteryx is Tested"},{"location":"contributing/internals/testing/#unit-testing","text":"Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Part of the CI process. Tests specific aspects of the internals. Combined with the SQL Battery test, the aim is for 95% coverage (with explicit exceptions). Whilst 95% coverage does not ensure the tests are 'good', it does help ensures any material changes to the function of the system are captured early.","title":"Unit Testing"},{"location":"contributing/internals/testing/#sql-battery","text":"Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Part of the CI process. Executes hundreds of hand-crafted SQL statements against the engine. The SQL Battery helps to ensure the entire system performs as expected and when used in tandem with Unit Testing, which primarily focuses on ensuring parts work as they should, this provides a level of confidence that the system continues to perform as expected. The battery essentially has four variations: Does the Query run - with no checking or validation of the outputs Does the Query fail - for scenarios when the query is expected to fail Is the shape of the results as expected - only the row and column counts are checked Does the Query return the right results - the returned dataset is checked The SQL Battery the most effective test to identify when functionality has been broken or changed by updates. The shape testing is currently considered the best value of this suite - it is fast and easy to write new tests for this suite, and the execution give reasonable considence in the correctness of the result in most situations.","title":"SQL Battery"},{"location":"contributing/internals/testing/#performance-testing","text":"Frequency : Ad hoc Maturity : Low Location : mabel-dev/wrenchy-bench The performance testing framework is only able to be run ad hoc, and there is currently no meaningful treatment or tracking out outcomes. It us currently used to confirm optimizations do have the impact of reducing query execution times.","title":"Performance Testing"},{"location":"contributing/internals/testing/#sql-logic-test","text":"Frequency : Ad hoc Maturity : Low Location : mabel-dev/wrenchy-bench Runs SQL statements in both Operyx and DuckDB to verify Opteryx returns the same answer as DuckDB. This has a growing set of tests which are executed, but as how relations are referenced in these systems, most queries require some hand-tuning and many are not possible with the framework as it currently is written. What has been able to be tested has demonstrated some deviation between these systems, so is a valuable and useful test, even in its current form.","title":"SQL Logic Test"},{"location":"contributing/internals/testing/#fuzzing","text":"Frequency : On Commit to GitHub & Nightly Maturity : Low Location : mabel-dev/opteryx As part of the CI process, executes 100 iterations of random inputs. As part of a nightly test, executes 100,000 iterations of random inputs. Fuzzing supplies some key functions with random data to try to capture scenarios which are unexpected or unhandled.","title":"Fuzzing"},{"location":"contributing/internals/testing/#security-code-quality-testing","text":"Frequency : On Commit to GitHub Maturity : Medium Location : mabel-dev/opteryx Various other tests are performed to help ensure code quality is maintained, these include security, form, typing, secret detection and test coverage checks using the following tools: Bandit, Semgrep, Black, MyPy, PyLint, PerfLint, Fides, SonarCloud, and Coverage.","title":"Security &amp; Code Quality Testing"},{"location":"contributing/set-up-guides/debian/","text":"Debian/Ubuntu This guide will help you to set up a Debian or Ubuntu workstation to work with the code and develop Opteryx. Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly. Setting Up 1. Install Python 3.10 recommended We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2. Install pip python3 -m ensurepip --upgrade 3. Install Git sudo apt-get update sudo apt-get install git 4. Install Rust curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh 5. Clone the Repository git clone https://github.com/mabel-dev/opteryx 6. Install Dependencies python3 -m pip install --upgrade -r requirements.txt 7. Build Binaries python3 setup.py build_ext --inplace Running Tests To run the regression and unit tests: First, install the optional dependencies, on most devices: python3 -m pip install --upgrade -r tests/requirements.txt On ARM-based devices (like Raspberry Pi): python3 -m pip install --upgrade -r tests/requirements_arm.txt Then run the regression tests. python3 -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"Debian/Ubuntu"},{"location":"contributing/set-up-guides/debian/#debianubuntu","text":"This guide will help you to set up a Debian or Ubuntu workstation to work with the code and develop Opteryx. Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly.","title":"Debian/Ubuntu"},{"location":"contributing/set-up-guides/debian/#setting-up","text":"","title":"Setting Up"},{"location":"contributing/set-up-guides/debian/#running-tests","text":"To run the regression and unit tests: First, install the optional dependencies, on most devices: python3 -m pip install --upgrade -r tests/requirements.txt On ARM-based devices (like Raspberry Pi): python3 -m pip install --upgrade -r tests/requirements_arm.txt Then run the regression tests. python3 -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"Running Tests"},{"location":"contributing/set-up-guides/macos/","text":"MacOS This guide will help you to set up a MacOS workstation to work with the code and develop Opteryx. Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly. Setting Up 1. Install Python 3.10 recommended We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2. Install pip python3 -m ensurepip --upgrade 3. Install Git sudo apt-get update sudo apt-get install git 4. Install Rust curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh 5. Clone the Repository git clone https://github.com/mabel-dev/opteryx 6. Install Dependencies python3 -m pip install --upgrade -r requirements.txt 7. Build Binaries python3 setup.py build_ext --inplace Running Tests To run the regression and unit tests: First, install the optional dependencies, on intel-based Macs: python3 -m pip install --upgrade -r tests/requirements.txt On M (ARM) CPU Macs: python3 -m pip install --upgrade -r tests/requirements_arm.txt Then run the regression tests. python3 -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"MacOS"},{"location":"contributing/set-up-guides/macos/#macos","text":"This guide will help you to set up a MacOS workstation to work with the code and develop Opteryx. Intel/x86 is the recommended environment, however Opteryx does run on ARM and some parts of the guide may require additional steps in order to work correctly.","title":"MacOS"},{"location":"contributing/set-up-guides/macos/#setting-up","text":"","title":"Setting Up"},{"location":"contributing/set-up-guides/macos/#running-tests","text":"To run the regression and unit tests: First, install the optional dependencies, on intel-based Macs: python3 -m pip install --upgrade -r tests/requirements.txt On M (ARM) CPU Macs: python3 -m pip install --upgrade -r tests/requirements_arm.txt Then run the regression tests. python3 -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"Running Tests"},{"location":"contributing/set-up-guides/windows/","text":"Windows This guide will help you to set up a Windows workstation to work with the code and develop Opteryx. If using WSL, refer to the Debian/Ubuntu set up guide. Initial set up of the WSL component is not covered in these guides. Intel/x86 is the recommended environment, it has not been confirmed that Opteryx operates as expected on ARM Windows - it does operate on Linux and Mac ARM. Setting Up 1. Install Python 3.10 recommended We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2. Install pip python -m ensurepip --upgrade 3. Install Git sudo apt-get update sudo apt-get install git 4. Install Rust Follow the instructions at https://rustup.rs/ 5. Clone the Repository git clone https://github.com/mabel-dev/opteryx 5. Install Dependencies python -m pip install --upgrade -r requirements.txt 6. Build Binaries python setup.py build_ext --inplace Running Tests To run the regression and unit tests: First, install the optional dependencies: python -m pip install --upgrade -r tests/requirements.txt Then run the regression tests. python -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"Windows"},{"location":"contributing/set-up-guides/windows/#windows","text":"This guide will help you to set up a Windows workstation to work with the code and develop Opteryx. If using WSL, refer to the Debian/Ubuntu set up guide. Initial set up of the WSL component is not covered in these guides. Intel/x86 is the recommended environment, it has not been confirmed that Opteryx operates as expected on ARM Windows - it does operate on Linux and Mac ARM.","title":"Windows"},{"location":"contributing/set-up-guides/windows/#setting-up","text":"","title":"Setting Up"},{"location":"contributing/set-up-guides/windows/#running-tests","text":"To run the regression and unit tests: First, install the optional dependencies: python -m pip install --upgrade -r tests/requirements.txt Then run the regression tests. python -m pytest Note Some tests require external services like GCS and Memcached and may fail if these have not been configured.","title":"Running Tests"},{"location":"get-started/clients/","text":"Client Connectivity Overview Opteryx supports two methods of invocation; as an importable Python library and as a command line tool. Python Embedded Opteryx is an embeddable package into Python applications, scripts and Notebooks which implements a partial Python DBAPI ( PEP 249 ) interface. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( 'SELECT * FROM $planets' ) rows = cur . fetchall () The results of the query are availble via the cursor using fetchone() which returns a dictionary, fetchmany(size) and fetchall() which return generators of dictionaries, or arrow() which returns an Arrow Table . Command Line Interface Opteryx Command Line Interface (CLI) provides a terminal-based interactive shell for running queries. The CLI is a Python script usually run by invoking Python, like this: python -m opteryx --o planets.csv \"SELECT * FROM \\$planets\" Note that CLI will have character escaping requirements, such as a backslash before dollar signs. Abridged usage guidance is available below: Usage: python -m opteryx [OPTIONS] [SQL] --ast --no-ast Display the AST for the query. [default: no-ast] --o <target> Where to output the results. [default: console] --help Show the full help details. To see the full help and usage details for the CLI use the --help option: python -m opteryx --help","title":"Client Overview"},{"location":"get-started/clients/#client-connectivity-overview","text":"Opteryx supports two methods of invocation; as an importable Python library and as a command line tool.","title":"Client Connectivity Overview"},{"location":"get-started/clients/#python-embedded","text":"Opteryx is an embeddable package into Python applications, scripts and Notebooks which implements a partial Python DBAPI ( PEP 249 ) interface. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( 'SELECT * FROM $planets' ) rows = cur . fetchall () The results of the query are availble via the cursor using fetchone() which returns a dictionary, fetchmany(size) and fetchall() which return generators of dictionaries, or arrow() which returns an Arrow Table .","title":"Python Embedded"},{"location":"get-started/clients/#command-line-interface","text":"Opteryx Command Line Interface (CLI) provides a terminal-based interactive shell for running queries. The CLI is a Python script usually run by invoking Python, like this: python -m opteryx --o planets.csv \"SELECT * FROM \\$planets\" Note that CLI will have character escaping requirements, such as a backslash before dollar signs. Abridged usage guidance is available below: Usage: python -m opteryx [OPTIONS] [SQL] --ast --no-ast Display the AST for the query. [default: no-ast] --o <target> Where to output the results. [default: console] --help Show the full help details. To see the full help and usage details for the CLI use the --help option: python -m opteryx --help","title":"Command Line Interface"},{"location":"get-started/configuration-guide/","text":"Configuration Guide Configuration File Configuration values are set a opteryx.yaml file in the directory the application is run from. Key Default Description INTERNAL_BATCH_SIZE 500 Batch size for left-table of a join processes MAX_JOIN_SIZE 10000 Maximum records created in a CROSS JOIN frame MEMCACHED_SERVER not set Address of Memcached server, in IP:PORT format DATASET_PREFIX_MAPPING not set Data store prefix mapping PARTITION_SCHEME none How the blob/file data is partitioned MAX_SIZE_SINGLE_CACHE_ITEM 1048576 The maximum size of an item to store in the cache MAX_CACHE_EVICTIONS 25 The maximum number of evictions from in-memory read cache per query PAGE_SIZE 67108864 The size to try to make data pages as they are processed LOCAL_BUFFER_POOL_SIZE 256 The size of the in-memory Buffer Pool (blob size) DISABLE_HIGH_PRIORITY False Disable trying to set the process priority Environment Variables The environment is the preferred location for secrets, although the engine will read .env files if dotenv has also been installed. MONGO_CONNECTION MINIO_END_POINT MINIO_ACCESS_KEY MINIO_SECRET_KEY MINIO_SECURE Caching for Blob Stores Read Cache The observed bottleneck for query performance is almost always IO. It is not uncommon for 90% of the execution time is initial load of data - this can vary considerably by storage and query. The Read Cache currently has two implementations, In Memory Cache and Memcached Cache. When your main storage is local disk, using Memcached as your Read Cache is unlikely to provide significant performance improvement, however the In Memory Cache may; when using remote storage such as S3 or GCS, Memcached Cache can provide significant improvements. However, as will all optimization, test in your unique set of circumstances before assuming it to be true. Buffer Pool The Buffer Pool is similar to the Read Cache with two key differences; the Buffer Pool must be held in local memory, Memcache or external system cannot be used and the Buffer Pool is a read through to the Read Cache. That means that reads to the Read Cache check if the items is in the Buffer Pool before checking the Read Cache. This creates a storage heirarchy, where frequently read blobs are likely to be in memory, and never, or infrequently accessed blobs are fetched from storage. If your Read Cache is in memory, you are unlikely to see considerable benefits from the Buffer Pool. In Memory Cache Uses the main memory of the host machine to cache pages. This is usually fastest, but most limiting and volatile. This is a good fit for high specification hosts. The size of the cache is set by the number of pages to hold in memory. No checks are made if the pages actually fit in memory and setting the cache too large, or running on a host where there is high contention for memory where memory is swapped to disk, may result in negative performance. Memcached Cache Uses a Memcached instance to cache pages. Is a good option when remote reads are slow, for example from GCS or S3. This is also recommended in an environment where multiple servers, or container instances, may be serving customers. Here, the shared cache allows users to benefit from caching even on their first query if another user's query has populated the cache with the files being read.","title":"Configuration Guide"},{"location":"get-started/configuration-guide/#configuration-guide","text":"","title":"Configuration Guide"},{"location":"get-started/configuration-guide/#configuration-file","text":"Configuration values are set a opteryx.yaml file in the directory the application is run from. Key Default Description INTERNAL_BATCH_SIZE 500 Batch size for left-table of a join processes MAX_JOIN_SIZE 10000 Maximum records created in a CROSS JOIN frame MEMCACHED_SERVER not set Address of Memcached server, in IP:PORT format DATASET_PREFIX_MAPPING not set Data store prefix mapping PARTITION_SCHEME none How the blob/file data is partitioned MAX_SIZE_SINGLE_CACHE_ITEM 1048576 The maximum size of an item to store in the cache MAX_CACHE_EVICTIONS 25 The maximum number of evictions from in-memory read cache per query PAGE_SIZE 67108864 The size to try to make data pages as they are processed LOCAL_BUFFER_POOL_SIZE 256 The size of the in-memory Buffer Pool (blob size) DISABLE_HIGH_PRIORITY False Disable trying to set the process priority","title":"Configuration File"},{"location":"get-started/configuration-guide/#environment-variables","text":"The environment is the preferred location for secrets, although the engine will read .env files if dotenv has also been installed. MONGO_CONNECTION MINIO_END_POINT MINIO_ACCESS_KEY MINIO_SECRET_KEY MINIO_SECURE","title":"Environment Variables"},{"location":"get-started/configuration-guide/#caching-for-blob-stores","text":"","title":"Caching for Blob Stores"},{"location":"get-started/deployment-guide/","text":"Deployment Guide Requirements Host Specifications Minimum : 1 CPU, 1 Gb RAM (Intel/x86 CPU) Recommended : 4 CPUs, 8 Gb RAM (Intel/x86 CPU) Opteryx balances memory consumption with performance, however, being able to process large datasets will require larger memory specifications compared to what is needed to process smaller datasets. The reference implementation of Opteryx regularly processes 100Gb of data in a container with 4 CPUs and 8Gb of memory allocated. Note This is a general recommendation and is a good place to start, your environment and specific problem may require, or perform significantly better, with a different configuration. Warning Non x86 environments, such as Raspberry Pi or the M1 Macs, may require additional set up steps. Operating System Support Opteryx is confirmed to support the following operating systems, the below table shows the regression suite coverage: OS Python 3.8 Python 3.9 Python 3.10 Python 3.11 PyPy 3.9 MacOS (Intel) Partial Partial Partial None None MacOS (M1) None None None None None Windows (x86) Partial Partial Partial None None Windows (ARM) None None None None None Ubuntu (x86) Full Full Full Failing Failing Debian (ARM) None Partial None None None Full - no tests are excluded from the test suite - coverage statistics are from Full tests. Partial - some tests are excluded from the test suite or that some tests fail. None - there is no automated test for this configuration. Note Windows (x86) regression suite fails some tests due to issues with Apache Arrow. PyPy regression suite fails due to issues with Apache Arrow. Python 3.11 regression suite fails due to lack of 3.11 support on the test platform. MacOs (M1) is not included in the regression suite due to lack of support on the test platform, but there is known usage on this configuration. Windows (ARM) is not included in the regression suite due to lack of support on the test platform. Most tests are excluded due to testing platform constraints, not compatibility issues. Python Environment Recommended Version : 3.9 Opteryx supports Python versions 3.8, 3.9 and 3.10. Opteryx has builds for Python 3.8, 3.9 and 3.10 on 64-bit (x86) versions of Windows, MacOS and Linux. The full regression suite is run on Ubuntu (Ubuntu 20.04) for Python version 3.8, 3.9 and 3.10. Opteryx is primarily developed on workstations running Python 3.10 (Debian, MacOS), is known to be deployed in production environments running Python 3.9 (Debian). Python 3.9 also has the greatest test coverage due to it being supported on more platforms. Jupyter Notebooks Opteryx can run in Jupyter Notebooks to access data locally or, if configured, remotely on systems like GCS and S3. This approach will result in raw data required to respond to the query being moved from the data platform (GCS or S3) to the host running Jupyter in order to be processed. This is most practical when the connection to the data platform is fast - such as running Vertex AI Notebooks on GCP, or querying local files. Docker & Kubernetes There is no Docker image for Opteryx, this is because Opteryx is an embedded Python library. However, system built using Opteryx can be deployed via Docker or Kubernetes. Google Cloud Cloud Run Opteryx is well-suited for running data manipulation tasks in Cloud Run as this was the target platform for the initial development. Running in the Generation 2 container environment is likely to result in faster query processing, but has a slower start-up time. Opteryx runs in Generation 1 container, taking approximately 10% longer to execute queries. Note Opteryx contains no specific optimiations to make use of multiple CPUs, although multiple CPUs may be beneficial to allow higher memory allocations and libraries Opteryx is built on may use multiple CPUs. Data Storage Connectors Built-In Connectors Platform Connector Name Implementation Google Cloud Storage GcpCloudStorageConnector Blob/File Store AWS S3 AwsS3Connector Blob/File Store MinIo AwsS3Connector Blob/File Store Google FireStore GcpFireStoreConnector Document Store MongoDB MongoDbConnector Document Store Local Disk DiskConnector Blob/File Store Connectors are registered with the storage engine using the register_store method. Multiple prefixes can be added, using different connectors - multiple storage types can be combined into a single query. opteryx . storage . register_store ( \"tests\" , DiskConnector ) A more complete example using the register_store method to set up a connector to Google Cloud Storage (GCS) and then query data on GCS is below: import opteryx from opteryx.connectors import GcpCloudStorageConnector # Tell the storage engine that datasets with the prefix 'your_bucket' # are to be read using the GcpCloudStorageConnector connector. # Multiple prefixes can be added and do not need to be the same # connector. opteryx . register_store ( \"your_bucket\" , GcpCloudStorageConnector ) connextion = opteryx . connect () cursor = connection . cursor () cursor . execute ( \"SELECT * FROM your_bucket.folder;\" ) print ( cursor . fetchone ()) Blob/File Stores Datasets Opteryx references datasets using their relative path as the table name. For example in the following folder structure / \u251c\u2500 products/ \u251c\u2500 customers/ \u2502 \u251c\u2500 profiles/ \u2502 \u2514\u2500 preferences/ \u2502 \u251c\u2500 marketing/ \u2502 \u2514\u2500 site/ \u2514\u2500\u2500 purchases/ Would have the following datasets available (assuming leaf folders have data files within them) products customers.profiles customers.preferences.marketing customers.preferences.site purchases These are queryable like this: SELECT * FROM customers . profiles Temporal Structures To enable temporal queries, data must be structured into date hierarchy folders below the dataset folder. Using just the products dataset from the above example, below the products folder must be year, month and day folders like this: / \u2514\u2500 products/ \u2514\u2500 year_2022/ \u2514\u2500 month_05/ \u2514\u2500 day_01/ To query the data for today with this structure, you can execute: SELECT * FROM products To query just the folder shown in the example (1st May 2022), you can execute: SELECT * FROM products FOR '2022-05-01' This is the default structure created by Mabel and within Opteryx this is called Mabel Partitioning. File Types Opteryx is primarily designed for use with Parquet to store data, Parquet is fast to process and offers optimizations not available for other formats, however, in some benchmarks ORC out performs Parquet. Opteryx supports: Parquet formatted files ( .parquet ) CSV formatted files ( .csv ) JSONL formatted files ( .jsonl ) JSONL formatted files which have been Zstandard compressed ( .zstd ) ORC formatted files ( .orc ) Feather (Arrow) formatted files ( .arrow ) File Sizes Opteryx loads entire files (pages) into memory one at a time, this requires the following to be considered: Reading one record from a file loads the entire page. If you regularly only read a few records, prefer smaller pages. Reading each page, particularly from Cloud Storage (S3/GCS), incurs a per-read overhead. If you have large datasets, prefer larger pages. If you are unsure where to start, 64Mb (before compression) is a recommended general-purpose page size.","title":"Deployment Guide"},{"location":"get-started/deployment-guide/#deployment-guide","text":"","title":"Deployment Guide"},{"location":"get-started/deployment-guide/#requirements","text":"","title":"Requirements"},{"location":"get-started/deployment-guide/#data-storage","text":"","title":"Data Storage"},{"location":"get-started/deployment-guide/#blobfile-stores","text":"","title":"Blob/File Stores"},{"location":"get-started/get-started/","text":"Get Started Installation Install from PyPI (recommended) This will install the latest release version. pip install --upgrade opteryx Install from GitHub The lastest version, including pre-release and beta versions can be installed, this is not recommended for production environments. pip install git+https://github.com/mabel-dev/opteryx Your First Query You can quickly test your installation is working as expected by querying one of the internal sample datasets. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( \"SELECT * FROM $planets;\" ) for row in cur . fetchall (): print ( row [ \"name\" ])","title":"Installation"},{"location":"get-started/get-started/#get-started","text":"","title":"Get Started"},{"location":"get-started/get-started/#installation","text":"Install from PyPI (recommended) This will install the latest release version. pip install --upgrade opteryx Install from GitHub The lastest version, including pre-release and beta versions can be installed, this is not recommended for production environments. pip install git+https://github.com/mabel-dev/opteryx","title":"Installation"},{"location":"get-started/get-started/#your-first-query","text":"You can quickly test your installation is working as expected by querying one of the internal sample datasets. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( \"SELECT * FROM $planets;\" ) for row in cur . fetchall (): print ( row [ \"name\" ])","title":"Your First Query"},{"location":"get-started/home/","text":"Overview Star on Github Opteryx is a SQL query engine to query large data sets designed to run in low-cost serverless environments. Opteryx is not a database, it does understand and respond to a subset of SQL statements like a database, but it does not support any activities which insert, update or delete data. It is not a replacement for databases like SQL Server, MySQL or Postgres, it is designed to allow querying of data sources as part of an data analytics process. Use Cases Where you need to query data across different data platforms but don't want the cost or effort to move this data to a common platform. Great for use in cost-optimized environments, where a traditional data solution like Hadoop is out of reach. Querying ad hoc data stores for third-party systems, such as querying logs output to storage. Where you have many different environments, and each would require their own database to query static data. Where you occassionally query data, and don't want the effort of loading into a database or the cost of maintaining a database for infrequent use. Where time to respond to queries is not time sensitive.","title":"Overview"},{"location":"get-started/home/#overview","text":"Star on Github Opteryx is a SQL query engine to query large data sets designed to run in low-cost serverless environments. Opteryx is not a database, it does understand and respond to a subset of SQL statements like a database, but it does not support any activities which insert, update or delete data. It is not a replacement for databases like SQL Server, MySQL or Postgres, it is designed to allow querying of data sources as part of an data analytics process.","title":"Overview"},{"location":"get-started/home/#use-cases","text":"Where you need to query data across different data platforms but don't want the cost or effort to move this data to a common platform. Great for use in cost-optimized environments, where a traditional data solution like Hadoop is out of reach. Querying ad hoc data stores for third-party systems, such as querying logs output to storage. Where you have many different environments, and each would require their own database to query static data. Where you occassionally query data, and don't want the effort of loading into a database or the cost of maintaining a database for infrequent use. Where time to respond to queries is not time sensitive.","title":"Use Cases"},{"location":"get-started/python-client/","text":"Python Client class Connection () cursor () return a cursor object close () exists for interface compatibility only commit () exists for interface compatibility only rollback () exists for interface compatibility only class Cursor (connection) id () The unique internal reference for this query execute (operation, params) rowcount () shape () stats () execution statistics messages () list of run-time warnings fetchone (as_dicts) Fetch one record only. Parameters as_dicts: boolean (optional) Return a dictionary, default is False, return a tuple fetchmany (size, as_dicts) fetch a given number of records fetchall (as_dicts) fetch all matching records arrow (size) Fetch the resultset as a pyarrow table, this is generally the fastest way to get the entire set of results. Parameters size: int (optional) Return the head 'size' number of records. Returns pyarrow.Table: close () close the connection head (size) This file has been automatically generated from the source code.","title":"Python Client"},{"location":"get-started/python-client/#python-client","text":"","title":"Python Client"},{"location":"get-started/schema-evolution/","text":"Schema Evolution Opteryx has support for in-place relation evolution. You can evolve a table schema or change a partition layout without requiring existing data to be rewritten or migrated to a new dataset. Schema of the data is determined by the first page read to respond to a query, new columns are removed and missing columns are null-filled. This allows graceful handling of pages with different schemas, but may result in the appearance of missing data as columns not found in the first page are removed. Opteryx supports the following schema evolution changes: Add - new columns can be added - these are removed if not present on the first page read Remove - removed columns are null-filled Reorder - the order of columns can be changed Partitioning - partition resolution can be changed Note Renamed columns will behave like the column has been removed and a new column added. Opteryx has limited support for column types changing, some changes within the same broad type (e.g. between numeric types and date resolutions) are supported, but these are not all supported and changing between types is not supported. Partitioning Changes to partition schemes are handled transparently. For example, data using Mabel partitioning moving from a daily to an hourly partition layout can occur without requiring any other changes to the configuration of the query engine. However, moving between no partition and partitioning (or vise-versa) is not supported.","title":"Schema Evolution"},{"location":"get-started/schema-evolution/#schema-evolution","text":"Opteryx has support for in-place relation evolution. You can evolve a table schema or change a partition layout without requiring existing data to be rewritten or migrated to a new dataset. Schema of the data is determined by the first page read to respond to a query, new columns are removed and missing columns are null-filled. This allows graceful handling of pages with different schemas, but may result in the appearance of missing data as columns not found in the first page are removed. Opteryx supports the following schema evolution changes: Add - new columns can be added - these are removed if not present on the first page read Remove - removed columns are null-filled Reorder - the order of columns can be changed Partitioning - partition resolution can be changed Note Renamed columns will behave like the column has been removed and a new column added. Opteryx has limited support for column types changing, some changes within the same broad type (e.g. between numeric types and date resolutions) are supported, but these are not all supported and changing between types is not supported.","title":"Schema Evolution"},{"location":"get-started/external-standards/pep249/","text":"Python PEP-249 Conformity Opteryx is not a DBMS so only aims for confirmity to PEP-249 for the featureset it supports - this is primarily support related to the querying of data. Module Interface Feature Imperative Supported Module Interface uknown connect constructor must unknown apilevel global must unknown threadsafety global must unknown paramstyle global must unknown Warning exception should unknown Error exception should unknown InterfaceError exception should unknown DatabaseError exception should unknown DataError exception should unknown OperationalError exception should unknown IntegrityError exception should unknown InternalError exception should unknown ProgrammingError exception should unknown NotSupportedError exception should unknown Connection Object unknown close method should unknown commit method should unknown rollback method should unknown cursor method should unknown messages attribute optional no errorhandler attribute optional no Cursor Object unknown description attribute should no rowcount attribute should unknown callproc method optional n/a close method should unknown execute method should unknown executemany method should no fetchmany method should unknown fetchall method should unknown nextset method optional n/a arraysize attribute should unknown setinputsizes method should no setoutputsize method should no rownumber attribute optional unknown connection attribute optional unknown scroll method optional no messages attribute optional no next method optional unknown __iter__ method optional no lastrowid attribute optional no errorhandler attribute optional no Type Constructors unknown Date optional unknown Time optional unknown Timestamp optional unknown DateFromTicks optional unknown TimeFromTicks optional unknown TimestampFromTicks optional unknown Binary optional unknown STRING optional unknown BINARY optional unknown NUMBER optional unknown DATETIME optional unknown ROWID optional unknown Two-Phase Commit Extensions n/a xid method optional n/a tpc_begin method optional n/a tpc_prepare method optional n/a tpc_commit method optional n/a tpc_rollback method optional n/a tpc_recover method optional n/a Support statuses in these tables: \u2003 yes The feature is supported and conformance is part of the test suite. \u2003 no The feature is not supported. \u2003 partial Some features are supported. \u2003 n/a The feature relates to a feature not supported by Opteryx. \u2003 unknown No test exists to confirm conformance.","title":"Python PEP249"},{"location":"get-started/external-standards/pep249/#python-pep-249-conformity","text":"Opteryx is not a DBMS so only aims for confirmity to PEP-249 for the featureset it supports - this is primarily support related to the querying of data.","title":"Python PEP-249 Conformity"},{"location":"get-started/external-standards/pep249/#module-interface","text":"Feature Imperative Supported Module Interface uknown connect constructor must unknown apilevel global must unknown threadsafety global must unknown paramstyle global must unknown Warning exception should unknown Error exception should unknown InterfaceError exception should unknown DatabaseError exception should unknown DataError exception should unknown OperationalError exception should unknown IntegrityError exception should unknown InternalError exception should unknown ProgrammingError exception should unknown NotSupportedError exception should unknown Connection Object unknown close method should unknown commit method should unknown rollback method should unknown cursor method should unknown messages attribute optional no errorhandler attribute optional no Cursor Object unknown description attribute should no rowcount attribute should unknown callproc method optional n/a close method should unknown execute method should unknown executemany method should no fetchmany method should unknown fetchall method should unknown nextset method optional n/a arraysize attribute should unknown setinputsizes method should no setoutputsize method should no rownumber attribute optional unknown connection attribute optional unknown scroll method optional no messages attribute optional no next method optional unknown __iter__ method optional no lastrowid attribute optional no errorhandler attribute optional no Type Constructors unknown Date optional unknown Time optional unknown Timestamp optional unknown DateFromTicks optional unknown TimeFromTicks optional unknown TimestampFromTicks optional unknown Binary optional unknown STRING optional unknown BINARY optional unknown NUMBER optional unknown DATETIME optional unknown ROWID optional unknown Two-Phase Commit Extensions n/a xid method optional n/a tpc_begin method optional n/a tpc_prepare method optional n/a tpc_commit method optional n/a tpc_rollback method optional n/a tpc_recover method optional n/a Support statuses in these tables: \u2003 yes The feature is supported and conformance is part of the test suite. \u2003 no The feature is not supported. \u2003 partial Some features are supported. \u2003 n/a The feature relates to a feature not supported by Opteryx. \u2003 unknown No test exists to confirm conformance.","title":"Module Interface"},{"location":"get-started/external-standards/sql92/","text":"ANSI SQL-92 Conformity For a system to attest to supporting SQL it must have good conformance to the ANSI SQL-92 standard. This standard is also known as ISO/IEC 9075:1992 . Opteryx is not a DBMS, so only aims for confirmity to the subset of SQL-92 for to featureset it supports - this is primarily related to SELECT statements. Function Description Support E011 Numeric data types partial E011-01 INTEGER and SMALLINT data types yes E011-02 REAL , DOUBLE PRECISION , and FLOAT data types partial E011-03 DECIMAL and NUMERIC data unknown E011-04 Arithmetic yes E011-05 Numeric comparison yes E011-06 Implicit casting among the numeric data types yes E021 Character string types unknown E021-01 CHARACTER data type unknown E021-02 CHARACTER VARYING data type unknown E021-03 Character literals unknown E021-04 CHARACTER_LENGTH function no E021-05 OCTET_LENGTH no E021-06 SUBSTRING function partial E021-07 Character concatenation yes E021-08 UPPER and LOWER functions yes E021-09 TRIM function unknown E021-10 Implicit casting among the fixed-length and variable-length character string types unknown E021-11 POSITION function no E021-12 Character comparison yes E031 Identifiers unknown E031-01 Delimited identifiers unknown E031-02 Lower case identifiers unknown E031-03 Trailing underscore unknown E051 Basic query specification unknown E051-01 SELECT DISTINCT unknown E051-02 GROUP BY clause unknown E051-04 GROUP BY can contain columns not in select-list unknown E051-05 Select list items can be renamed unknown E051-06 HAVING clause unknown E051-07 Qualified * in select list unknown E051-08 Correlation names in the FROM clause unknown E051-09 Rename columns in the FROM clause unknown E061 Basic predicates and search conditions unknown E061-01 Comparison predicate unknown E061-02 BETWEEN predicate unknown E061-03 IN predicate with list of values unknown E061-04 LIKE predicate unknown E061-05 LIKE predicate: ESCAPE clause unknown E061-06 NULL predicate unknown E061-07 Quantified comparison predicate unknown E061-08 EXISTS predicate no E061-09 Subqueries in comparison predicate unknown E061-11 Subqueries in IN predicate unknown E061-12 Subqueries in quantified comparison predicate unknown E061-13 Correlated subqueries unknown E061-14 Search condition unknown E071 Basic query expressions unknown E071-01 UNION DISTINCT table operator no E071-02 UNION ALL table operator no E071-03 EXCEPT DISTINCT table operator no E071-05 Columns combined via table operators need not have exactly the same data type unknown E071-06 Table operators in subqueries unknown E081 Basic Privileges unknown E081-01 SELECT privilege at the table level unknown E081-02 DELETE privilege n/a E081-03 INSERT privilege at the table level n/a E081-04 UPDATE privilege at the table level n/a E081-05 UPDATE privilege at the column level n/a E081-06 REFERENCES privilege at the table level unknown E081-07 REFERENCES privilege at the column level unknown E081-08 WITH GRANT OPTION no E081-09 USAGE privilege unknown E081-10 EXECUTE privilege unknown E091 Set functions unknown E091-01 AVG unknown E091-02 COUNT unknown E091-03 MAX unknown E091-04 MIN unknown E091-05 SUM unknown E091-06 ALL quantifier unknown E091-07 DISTINCT quantifier unknown E101 Basic data manipulation n/a E101-01 INSERT statement n/a E101-03 Searched UPDATE statement n/a E101-04 Searched DELETE statement n/a E111 Single row SELECT statement unknown E121 Basic cursor support unknown E121-01 DECLARE CURSOR no E121-02 ORDER BY columns need not be in select unknown E121-03 Value expressions in ORDER BY clause unknown E121-04 OPEN statement unknown E121-06 Positioned UPDATE statement no E121-07 Positioned DELETE statement no E121-08 CLOSE statement no E121-10 FETCH statement: implicit NEXT no E121-17 WITH HOLD cursors no E131 Null value support unknown E141 Basic integrity constraints unknown E141-01 NOT NULL constraints unknown E141-02 UNIQUE constraints of NOT NULL unknown E141-03 PRIMARY KEY constraints unknown E141-04 Basic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action unknown E141-06 CHECK constraints unknown E141-07 Column defaults unknown E141-08 NOT NULL inferred on PRIMARY KEY unknown E141-10 Names in a foreign key can be specified in any order unknown E151 Transaction support n/a E151-01 COMMIT statement n/a E151-02 ROLLBACK statement n/a E152 Basic SET TRANSACTION statement n/a E152-01 SET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause n/a E152-02 SET TRANSACTION statement: READ ONLY and READ WRITE clauses n/a E+ Other unknown E153 Updatable queries with subqueries unknown E161 SQL comments using leading double minus yes E171 SQLSTATE support unknown E182 Host language binding unknown F021 Basic information schema unknown F021-01 COLUMNS view unknown F021-02 TABLES view unknown F021-03 VIEWS view unknown F021-04 TABLE_CONSTRAINTS view unknown F021-05 REFERENTIAL_CONSTRAINTS view unknown F021-06 CHECK_CONSTRAINTS view unknown F031 Basic schema manipulation unknown F031-01 CREATE TABLE statement to create persistent base tables n/a F031-02 CREATE VIEW statement n/a F031-03 GRANT statement unknown F031-04 ALTER TABLE statement: ADD COLUMN clause n/a F031-13 DROP TABLE statement: RESTRICT clause n/a F031-16 DROP VIEW statement: RESTRICT clause n/a F031-19 REVOKE statement: RESTRICT clause unknown F041 Basic joined table unknown F041-01 Inner join (but not necessarily the INNER keyword) unknown F041-02 INNER keyword unknown F041-03 LEFT OUTER JOIN unknown F041-04 RIGHT OUTER JOIN unknown F041-05 Outer joins can be nested unknown F041-07 The inner table in a left or right outer join can also be used in an inner join unknown F041-08 All comparison operators are supported (rather than just = ) unknown F051 Basic date and time unknown F051-01 DATE data type (including support of DATE literal) unknown F051-02 TIME data type (including support of TIME literal) with fractional seconds unknown F051-03 TIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6 unknown F051-04 Comparison predicate on DATE , TIME , and TIMESTAMP data types partial F051-05 Explicit CAST between datetime types and character string types unknown F051-06 CURRENT_DATE yes F051-07 LOCALTIME unknown F051-08 LOCALTIMESTAMP unknown F081 UNION and EXCEPT in views unknown F131 Grouped operations unknown F131-01 WHERE , GROUP BY , and HAVING clauses supported in queries with grouped views unknown F131-02 Multiple tables supported in queries with grouped views unknown F131-03 Set functions supported in queries with grouped views unknown F131-04 Subqueries with GROUP BY and HAVING clauses and grouped views unknown F131-05 Single row SELECT with GROUP BY and HAVING clauses and grouped views unknown F181 Multiple module support unknown F201 CAST function unknown F221 Explicit defaults unknown F261 CASE expression no F261-01 Simple CASE no F261-02 Searched CASE no F261-03 NULLIF yes F261-04 COALESCE yes F311 Schema definition statement n/a F311-01 CREATE SCHEMA n/a F311-02 CREATE TABLE n/a F311-03 CREATE VIEW n/a F311-04 CREATE VIEW : WITH CHECK OPTION n/a F311-05 GRANT statement n/a F471 Scalar subquery values unknown F481 Expanded NULL predicate unknown F501 Features and conformance views unknown F501-01 SQL_FEATURES view unknown F501-02 SQL_SIZING view unknown F501-03 SQL_LANGUAGES view unknown F812 Basic flagging unknown S011 Distinct data types unknown S011-01 USER_DEFINED_TYPES view no T321 Basic SQL-invoked routines unknown T321-01 User-defined functions with no overloading unknown T321-02 User-defined stored procedures with no overloading unknown T321-03 Function invocation unknown T321-04 CALL statement unknown T321-05 RETURN statement unknown T321-06 ROUTINES view unknown T321-07 PARAMETERS view unknown T631 IN predicate with one list element unknown Support statuses in this table: \u2003 yes The feature is supported and conformance is part of the test suite. \u2003 no The feature is not supported. \u2003 partial Some features are supported. \u2003 n/a The feature relates to a feature not supported by Opteryx. \u2003 unknown No test exists to confirm conformance.","title":"ANSI SQL-92"},{"location":"get-started/external-standards/sql92/#ansi-sql-92-conformity","text":"For a system to attest to supporting SQL it must have good conformance to the ANSI SQL-92 standard. This standard is also known as ISO/IEC 9075:1992 . Opteryx is not a DBMS, so only aims for confirmity to the subset of SQL-92 for to featureset it supports - this is primarily related to SELECT statements. Function Description Support E011 Numeric data types partial E011-01 INTEGER and SMALLINT data types yes E011-02 REAL , DOUBLE PRECISION , and FLOAT data types partial E011-03 DECIMAL and NUMERIC data unknown E011-04 Arithmetic yes E011-05 Numeric comparison yes E011-06 Implicit casting among the numeric data types yes E021 Character string types unknown E021-01 CHARACTER data type unknown E021-02 CHARACTER VARYING data type unknown E021-03 Character literals unknown E021-04 CHARACTER_LENGTH function no E021-05 OCTET_LENGTH no E021-06 SUBSTRING function partial E021-07 Character concatenation yes E021-08 UPPER and LOWER functions yes E021-09 TRIM function unknown E021-10 Implicit casting among the fixed-length and variable-length character string types unknown E021-11 POSITION function no E021-12 Character comparison yes E031 Identifiers unknown E031-01 Delimited identifiers unknown E031-02 Lower case identifiers unknown E031-03 Trailing underscore unknown E051 Basic query specification unknown E051-01 SELECT DISTINCT unknown E051-02 GROUP BY clause unknown E051-04 GROUP BY can contain columns not in select-list unknown E051-05 Select list items can be renamed unknown E051-06 HAVING clause unknown E051-07 Qualified * in select list unknown E051-08 Correlation names in the FROM clause unknown E051-09 Rename columns in the FROM clause unknown E061 Basic predicates and search conditions unknown E061-01 Comparison predicate unknown E061-02 BETWEEN predicate unknown E061-03 IN predicate with list of values unknown E061-04 LIKE predicate unknown E061-05 LIKE predicate: ESCAPE clause unknown E061-06 NULL predicate unknown E061-07 Quantified comparison predicate unknown E061-08 EXISTS predicate no E061-09 Subqueries in comparison predicate unknown E061-11 Subqueries in IN predicate unknown E061-12 Subqueries in quantified comparison predicate unknown E061-13 Correlated subqueries unknown E061-14 Search condition unknown E071 Basic query expressions unknown E071-01 UNION DISTINCT table operator no E071-02 UNION ALL table operator no E071-03 EXCEPT DISTINCT table operator no E071-05 Columns combined via table operators need not have exactly the same data type unknown E071-06 Table operators in subqueries unknown E081 Basic Privileges unknown E081-01 SELECT privilege at the table level unknown E081-02 DELETE privilege n/a E081-03 INSERT privilege at the table level n/a E081-04 UPDATE privilege at the table level n/a E081-05 UPDATE privilege at the column level n/a E081-06 REFERENCES privilege at the table level unknown E081-07 REFERENCES privilege at the column level unknown E081-08 WITH GRANT OPTION no E081-09 USAGE privilege unknown E081-10 EXECUTE privilege unknown E091 Set functions unknown E091-01 AVG unknown E091-02 COUNT unknown E091-03 MAX unknown E091-04 MIN unknown E091-05 SUM unknown E091-06 ALL quantifier unknown E091-07 DISTINCT quantifier unknown E101 Basic data manipulation n/a E101-01 INSERT statement n/a E101-03 Searched UPDATE statement n/a E101-04 Searched DELETE statement n/a E111 Single row SELECT statement unknown E121 Basic cursor support unknown E121-01 DECLARE CURSOR no E121-02 ORDER BY columns need not be in select unknown E121-03 Value expressions in ORDER BY clause unknown E121-04 OPEN statement unknown E121-06 Positioned UPDATE statement no E121-07 Positioned DELETE statement no E121-08 CLOSE statement no E121-10 FETCH statement: implicit NEXT no E121-17 WITH HOLD cursors no E131 Null value support unknown E141 Basic integrity constraints unknown E141-01 NOT NULL constraints unknown E141-02 UNIQUE constraints of NOT NULL unknown E141-03 PRIMARY KEY constraints unknown E141-04 Basic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action unknown E141-06 CHECK constraints unknown E141-07 Column defaults unknown E141-08 NOT NULL inferred on PRIMARY KEY unknown E141-10 Names in a foreign key can be specified in any order unknown E151 Transaction support n/a E151-01 COMMIT statement n/a E151-02 ROLLBACK statement n/a E152 Basic SET TRANSACTION statement n/a E152-01 SET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause n/a E152-02 SET TRANSACTION statement: READ ONLY and READ WRITE clauses n/a E+ Other unknown E153 Updatable queries with subqueries unknown E161 SQL comments using leading double minus yes E171 SQLSTATE support unknown E182 Host language binding unknown F021 Basic information schema unknown F021-01 COLUMNS view unknown F021-02 TABLES view unknown F021-03 VIEWS view unknown F021-04 TABLE_CONSTRAINTS view unknown F021-05 REFERENTIAL_CONSTRAINTS view unknown F021-06 CHECK_CONSTRAINTS view unknown F031 Basic schema manipulation unknown F031-01 CREATE TABLE statement to create persistent base tables n/a F031-02 CREATE VIEW statement n/a F031-03 GRANT statement unknown F031-04 ALTER TABLE statement: ADD COLUMN clause n/a F031-13 DROP TABLE statement: RESTRICT clause n/a F031-16 DROP VIEW statement: RESTRICT clause n/a F031-19 REVOKE statement: RESTRICT clause unknown F041 Basic joined table unknown F041-01 Inner join (but not necessarily the INNER keyword) unknown F041-02 INNER keyword unknown F041-03 LEFT OUTER JOIN unknown F041-04 RIGHT OUTER JOIN unknown F041-05 Outer joins can be nested unknown F041-07 The inner table in a left or right outer join can also be used in an inner join unknown F041-08 All comparison operators are supported (rather than just = ) unknown F051 Basic date and time unknown F051-01 DATE data type (including support of DATE literal) unknown F051-02 TIME data type (including support of TIME literal) with fractional seconds unknown F051-03 TIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6 unknown F051-04 Comparison predicate on DATE , TIME , and TIMESTAMP data types partial F051-05 Explicit CAST between datetime types and character string types unknown F051-06 CURRENT_DATE yes F051-07 LOCALTIME unknown F051-08 LOCALTIMESTAMP unknown F081 UNION and EXCEPT in views unknown F131 Grouped operations unknown F131-01 WHERE , GROUP BY , and HAVING clauses supported in queries with grouped views unknown F131-02 Multiple tables supported in queries with grouped views unknown F131-03 Set functions supported in queries with grouped views unknown F131-04 Subqueries with GROUP BY and HAVING clauses and grouped views unknown F131-05 Single row SELECT with GROUP BY and HAVING clauses and grouped views unknown F181 Multiple module support unknown F201 CAST function unknown F221 Explicit defaults unknown F261 CASE expression no F261-01 Simple CASE no F261-02 Searched CASE no F261-03 NULLIF yes F261-04 COALESCE yes F311 Schema definition statement n/a F311-01 CREATE SCHEMA n/a F311-02 CREATE TABLE n/a F311-03 CREATE VIEW n/a F311-04 CREATE VIEW : WITH CHECK OPTION n/a F311-05 GRANT statement n/a F471 Scalar subquery values unknown F481 Expanded NULL predicate unknown F501 Features and conformance views unknown F501-01 SQL_FEATURES view unknown F501-02 SQL_SIZING view unknown F501-03 SQL_LANGUAGES view unknown F812 Basic flagging unknown S011 Distinct data types unknown S011-01 USER_DEFINED_TYPES view no T321 Basic SQL-invoked routines unknown T321-01 User-defined functions with no overloading unknown T321-02 User-defined stored procedures with no overloading unknown T321-03 Function invocation unknown T321-04 CALL statement unknown T321-05 RETURN statement unknown T321-06 ROUTINES view unknown T321-07 PARAMETERS view unknown T631 IN predicate with one list element unknown Support statuses in this table: \u2003 yes The feature is supported and conformance is part of the test suite. \u2003 no The feature is not supported. \u2003 partial Some features are supported. \u2003 n/a The feature relates to a feature not supported by Opteryx. \u2003 unknown No test exists to confirm conformance.","title":"ANSI SQL-92 Conformity"},{"location":"get-started/release-notes/change-log/","text":"Changelog All notable changes to this project will be documented in this file, where appropriate the GitHub issue reference will be noted along with the change. Breaking changes will be clearly indicated with the icon. The format is based on Keep a Changelog . [Unreleased] Fixed [ #653 ] LIKE and FOR clauses cannot coexist in SHOW queries. @joocer Changed Added [ #629 ] Evaluate constants in expressions during optimization. @joocer [ #439 ] Support SHOW STORES . @joocer [0.6.0] - 2022-11-08 Fixed [ #568 ] Unable to perform aggregates on literals. @joocer [ #592 ] Dates not always handled correctly. @joocer [ #600 ] Parameterization when used on query batches fails. @joocer [ #580 ] Empty result sets have no column information. @joocer [ #548 ] 'did you mean' message restored for dataset WITH hints. @joocer [ #640 ] COUNT(*) shortcut only used when in uppercase. @joocer [ #645 ] (correction) null values not handled correctly in comparisions. @joocer Problem installing on M1 Mac. @joocer Support AND , OR , and XOR in SELECT statement. @joocer [ #646 ] Temporal clauses in incorrect place were ignored @joocer Changed [ #566 ] Change from using SQLite3 to DuckDB for SQL comparision tests in Wrenchy-Bench . @joocer [ #584 ] (clarity) enable_page_management configuration and parameter renamed enable_page_defragmentation with some minor refactoring of approach to defragmentation. @joocer (alignment) TIMESTAMP casting no longer supports casting from a number. @joocer [ #588 ] Integrate sqloxide into Opteryx to reduce lag with sqlparser-rs updates. @joocer [ #619 ] Page defragmentation moved to an Operator and positioned by the Optimizer. @joocer (correction) cursor 'fetch*' methods return Python tuple, rather than Python lists. @joocer Added [ #533 ] Support LIKE on SHOW FUNCTIONS , see sqlparser-rs/#620 . @joocer [ #570 ] Query Optimizer rule to reduce steps in expression evaluation by partial elimination of negatives. @joocer [ #129 ] Support FOR clauses for all datasets. @joocer [ #543 ] Support 'type string' notation for casting values. @joocer [ #596 ] Optimizer replaces ORDER BY and LIMIT plan steps with a single 'HeapSort' plan step. @joocer [ #515 ] NULLIF function. @joocer [ #581 ] New SQL Battery test that tests results, and initial set of tests. @joocer [ #577 ] Hierarchical buffer pool and configuration. @joocer [0.5.0] - 2022-10-02 Fixed [ #528 ] .shape and .count not working as expected. @joocer Numbers expressed in the form +n not parsed correctly. @joocer Changed (alignment) as_arrow renamed to arrow to align to DuckDB naming. @joocer (consistency) SHOW COLUMNS returns the column name in the name column, previously column_name @joocer (correction) cursor 'fetch*' methods returns tuples rather than dictionaries as defaults, this is correcting a bug in PEP249 compatibility. @joocer [ #517 ] (security) Placeholder changed from '%s' to '?'. @joocer [ #522 ] Implementation of LRU-K(2) for cache evictions. @joocer [ #537 ] Significant refactor of Query Planner. @joocer Added [ #397 ] Time Travel with '$planets' dataset. @joocer [ #519 ] Introduce a size limit on as_arrow() . @joocer [ #324 ] Support IN UNNEST() . @joocer [ #386 ] Support SET statements. @joocer [ #531 ] Support SHOW VARIABLES and SHOW PARAMETERS . @joocer [ #464 ] Support LEFT JOIN <relation> USING @joocer [ #402 ] INNER JOIN ON supports multiple conditions @joocer [ #551 ] Document stores (MongoDb + FireStore) return '_id' column holding string version of document ID. @joocer [ #532 ] Runtime parameters are able to be altered using the SET statement. @joocer [ #524 ] Query Optimizer - conjunctive predicate splitter. @joocer [0.4.1] - 2022-09-12 Fixed Fixed missing __init__ file. @joocer [0.4.0] - 2022-09-12 Added [ #366 ] Implement 'function not found' suggestions. @joocer [ #443 ] Introduce a CLI. @joocer [ #351 ] Support SHOW FUNCTIONS . @joocer [ #442 ] Various functions. @joocer [ #483 ] Support SHOW CREATE TABLE . @joocer [ #375 ] Results to an Arrow Table. @joocer [ #486 ] Support functions on aggregates and aggregates on functions. @joocer Initial support for INTERVAL s. @joocer [ #395 ] Support reading CSV files. @joocer [ #498 ] CLI support writing CSV/JSONL/Parquet. @joocer Changed [ #457 ] (correction) null values are removed before performing INNER JOIN USING . @joocer Fixed [ #448 ] VERSION() failed and missing from regression suite. @joocer [ #404 ] COALESCE fails for NaN values. @joocer [ #453 ] PyArrow bug with long lists creating new columns. @joocer [ #444 ] Very low cardinality INNER JOINS exceed memory allocation. @joocer [ #459 ] Functions lose some detail on non-first page. @joocer [ #465 ] Pages aren't matched to schema for simple queries. @joocer [ #468 ] Parquet reader shows some fields as \"item\". @joocer [ #471 ] Column aliases not correctly applied when the relation has an alias. @joocer [ #489 ] Intermittent behaviour on hash JOIN algorithm. @joocer [0.3.0] - 2022-08-28 Added [ #196 ] Partial implementation of projection pushdown (Parquet Only). @joocer [ #41 ] Enable the results of functions to be used as parameters for other functions. @joocer [ #42 ] Enable inline operations. @joocer [ #330 ] Support SIMILAR TO alias for RegEx match. @joocer [ #331 ] Support SAFE_CAST alias for TRY_CAST . @joocer [ #419 ] Various simple functions ( SIGN , SQRT , TITLE , REVERSE ). @joocer [ #364 ] Support SOUNDEX function. @joocer [ #401 ] Support SHA-based hash algorithm functions. @joocer Changed (alignment) Paths to storage adapters has been updated to reflect 'connector' terminology. (sensible defaults) Default behaviour changed from Mabel partitioning to no partitioning. (correction) - Use of aliases defined in the SELECT clause can no longer be used in WHERE and GROUP BY clauses - this is a correction to align to standard SQL behaviour. (correction) - Use of 'None' as an alias for null is no longer supported - this is a correction to align to standard SQL behaviour. [ #326 ] Prefer pyarrow's 'promote' over manually handling missing fields. @joocer [ #39 ] Rewrite Aggregation Node to use Pyarrow group_by . @joocer [ #338 ] Remove Evaluation Node. @joocer [ #58 ] Performance of ORDER BY RAND() improved. @joocer Fixed [ #334 ] All lists should be cast to lists of strings. ( @joocer ) [ #382 ] INNER JOIN on UNNEST relation. ( @joocer ) [ #320 ] Can't execute functions on results of GROUP BY . ( @joocer ) [ #399 ] Strings in double quotes aren't parsed. ( @joocer ) [0.2.0] - 2022-07-31 Added [ #232 ] Support DATEPART and EXTRACT date functions. @joocer [ #63 ] Estimate row counts when reading blobs. ( @joocer ) [ #231 ] Implement DATEDIFF function. ( @joocer ) [ #301 ] Optimizations for IS conditions. ( @joocer ) [ #229 ] Support TIME_BUCKET function. ( @joocer ) Changed [ #35 ] Table scan planning done during query planning. @joocer [ #173 ] Data not found raises different errors under different scenarios. ( @joocer ) Implementation of LEFT and RIGHT functions to reduce execution time. ( @joocer ) [ #258 ] Code release approach. ( @joocer ) [ #295 ] Removed redundant projection when SELECT * . ( @joocer ) [ #297 ] Filters on SHOW COLUMNS execute before profiling. ( @joocer ) Fixed [ #252 ] Planner should gracefully convert byte strings to ascii strings. ( @joocer ) [ #184 ] Schema changes cause unexpected and unhelpful failures. ( @joocer ) [ #261 ] Read fails if buffer cache is unavailable. ( @joocer ) [ #277 ] Cache errors should be transparent. ( @joocer ) [ #285 ] DISTINCT on nulls throws error. ( @joocer ) [ #281 ] SELECT on empty aggregates reports missing columns. ( @joocer ) [ #312 ] Invalid dates in FOR clauses treated as TODAY . ( @joocer ) [0.1.0] - 2022-07-02 Added [ #165 ] Support S3/MinIO data stores for blobs. ( @joocer ) FAKE dataset constructor (part of #179 ). ( @joocer ) [ #177 ] Support SHOW FULL COLUMNS to read entire datasets rather than just the first blob. ( @joocer ) [ #194 ] Functions that are abbreviations, should have the full name as an alias. ( @joocer ) [ #201 ] generate_series supports CIDR expansion. ( @joocer ) [ #175 ] Support WITH (NO_CACHE) hint to disable using cache. ( @joocer ) [ #203 ] When reporting that a column doesn't exist, it should suggest likely correct columns. ( @joocer ) 'Not' Regular Expression match operator, !~ added to supported set of operators. ( @joocer ) [ #226 ] Implement DATE_TRUNC function. ( @joocer ) [ #230 ] Allow addressing fields as numbers. ( @joocer ) [ #234 ] Implement SEARCH function. ( @joocer ) [ #237 ] Implement COALESCE function. ( @joocer ) Changed Blob-based readers (disk & GCS) moved from 'local' and 'network' paths to a new 'blob' path. ( @joocer ) Query Execution rewritten. ( @joocer ) [ #20 ] Split query planner and query plan into different modules. ( @joocer ) [ #164 ] Split dataset reader into specific types. ( @joocer ) Expression evaluation short-cuts execution when executing evaluations against an array of null . ( @joocer ) [ #244 ] Improve performance of IN test against literal lists. ( @joocer ) Fixed [ #172 ] LIKE on non string column gives confusing error ( @joocer ) [ #179 ] Aggregate Node creates new metadata for each chunk ( @joocer ) [ #183 ] NOT doesn't display in plan correctly ( @joocer ) [ #182 ] Unable to evaluate valid filters ( @joocer ) [ #178 ] SHOW COLUMNS returns type OTHER when it can probably work out the type ( @joocer ) [ #128 ] JOIN fails, using PyArrow .join() ( @joocer ) [ #189 ] Explicit JOIN algorithm exceeds memory ( @joocer ) [ #199 ] SHOW EXTENDED COLUMNS blows memory allocations on large tables ( @joocer ) [ #169 ] Selection nodes in EXPLAIN have nested parentheses. ( @joocer ) [ #220 ] LIKE clause fails for columns that contain nulls. ( @joocer ) [ #222 ] Column of NULL detects as VARCHAR . ( @joocer ) [ #225 ] UNNEST does not assign a type to the column when all of the values are NULL . ( @joocer ) [0.0.2] - 2022-06-03 Added [ #72 ] Configuration is now read from opteryx.yaml rather than the environment. ( @joocer ) [ #139 ] Gather statistics on planning reading of segements. ( @joocer ) [ #151 ] Implement SELECT table.* . ( @joocer ) [ #137 ] GENERATE_SERIES function. ( @joocer ) Fixed [ #106 ] ORDER BY on qualified fields fails ( @joocer ) [ #103 ] ORDER BY after JOIN errors ( @joocer ) [ #110 ] SubQueries AS statement ignored ( @joocer ) [ #112 ] SHOW COLUMNS doesn't work for non sample datasets ( @joocer ) [ #113 ] Sample data has \"NaN\" as a string, rather than the value NaN ( @joocer ) [ #111 ] CROSS JOIN UNNEST should return a NONE when the list is empty (or NONE ) ( @joocer ) [ #119 ] 'NoneType' object is not iterable error on UNNEST ( @joocer ) [ #127 ] Reading from segments appears to only read the first segment ( @joocer ) [ #132 ] Multiprocessing regressed Caching functionality ( @joocer ) [ #140 ] Appears to have read both frames rather than the latest frame ( @joocer ) [ #144 ] Multiple JOINS in one query aren't recognized ( @joocer ) [0.0.1] - 2022-05-09 Added Additional statistics recording the time taken to scan partitions ( @joocer ) Support for FULL JOIN and RIGHT JOIN ( @joocer ) Changed Use PyArrow implementation for INNER JOIN and LEFT JOIN ( @joocer ) Fixed [ #99 ] Grouping by a list gives an unhelpful error message ( @joocer ) [ #100 ] Projection ignores field qualifications ( @joocer ) [0.0.0] Initial Version","title":"Change Log"},{"location":"get-started/release-notes/change-log/#changelog","text":"All notable changes to this project will be documented in this file, where appropriate the GitHub issue reference will be noted along with the change. Breaking changes will be clearly indicated with the icon. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"get-started/release-notes/change-log/#unreleased","text":"","title":"[Unreleased]"},{"location":"get-started/release-notes/change-log/#060-2022-11-08","text":"","title":"[0.6.0] - 2022-11-08"},{"location":"get-started/release-notes/change-log/#050-2022-10-02","text":"","title":"[0.5.0] - 2022-10-02"},{"location":"get-started/release-notes/change-log/#041-2022-09-12","text":"","title":"[0.4.1] - 2022-09-12"},{"location":"get-started/release-notes/change-log/#040-2022-09-12","text":"","title":"[0.4.0] - 2022-09-12"},{"location":"get-started/release-notes/change-log/#030-2022-08-28","text":"","title":"[0.3.0] - 2022-08-28"},{"location":"get-started/release-notes/change-log/#020-2022-07-31","text":"","title":"[0.2.0] - 2022-07-31"},{"location":"get-started/release-notes/change-log/#010-2022-07-02","text":"","title":"[0.1.0] - 2022-07-02"},{"location":"get-started/release-notes/change-log/#002-2022-06-03","text":"","title":"[0.0.2] - 2022-06-03"},{"location":"get-started/release-notes/change-log/#001-2022-05-09","text":"","title":"[0.0.1] - 2022-05-09"},{"location":"get-started/release-notes/change-log/#000","text":"Initial Version","title":"[0.0.0]"},{"location":"get-started/release-notes/notices/","text":"Notices Incorporated Components Opteryx is built on the shoulders of other great libraries and components: Library Disposition Copyright Licence cityhash Installed Bespoke cython Installed Apache 2.0 datasketch Integrated 2015 Eric Zhu MIT datetime_truncate Integrated 2020 Media Pop MIT distogram Integrated 2020 Romain Picard MIT fuzzy Integrated Jason R. Coombs MIT mbleven Integrated 2018 Fujimoto Seiji Public Domain numpy Installed BSD-3 orjson Installed Apache 2.0 pyarrow Installed Apache 2.0 pyarrow_ops Integrated TomScheffers (assumed) Apache 2.0 pyyaml Installed MIT sqloxide Integrated 2020 Will Eaton MIT sqlparser-rs Installed Apache 2.0 typer Installed MIT \u2003 Installed components are installed from PyPI. \u2003 Integrated components have their source code (or significant parts of) included in the Opteryx codebase. This list does not include transitive dependencies nor is guaranteed to be complete. Only components which have been integrated have copyright information noted, best efforts have been made to ensure this information is correct. Integrated components may differ from their original form. Cosmetic changes are not generally noted however where functionality has been added or altered, this is recorded in comments. pyarrow_ops has had significant changes to the original code and it is no longer practical to annotate all differences in the code. Note License information was correct as at 2022-06-03, or when updated in this document if later. Other Assets The Icarus Opteryx image based on ' Evening: Fall of Day ' by William Rimmer (Public Domain), more commonly associated with Led Zepplin's Swan Song . The Icarus Opteryx image is created using visual components from 'Archaeopteryx' fossil image by Caro Asercion ( CC BY 3.0 ). Satellite and Planet datasets acquired from devstronomy . Astronaut dataset acquired from Kaggle . Diagrams created using ASCII Flow or draw.io . Website build using mkdocs-material . SQL-92 conformity tests are based on sqltest .","title":"Notices"},{"location":"get-started/release-notes/notices/#notices","text":"","title":"Notices"},{"location":"get-started/release-notes/notices/#incorporated-components","text":"Opteryx is built on the shoulders of other great libraries and components: Library Disposition Copyright Licence cityhash Installed Bespoke cython Installed Apache 2.0 datasketch Integrated 2015 Eric Zhu MIT datetime_truncate Integrated 2020 Media Pop MIT distogram Integrated 2020 Romain Picard MIT fuzzy Integrated Jason R. Coombs MIT mbleven Integrated 2018 Fujimoto Seiji Public Domain numpy Installed BSD-3 orjson Installed Apache 2.0 pyarrow Installed Apache 2.0 pyarrow_ops Integrated TomScheffers (assumed) Apache 2.0 pyyaml Installed MIT sqloxide Integrated 2020 Will Eaton MIT sqlparser-rs Installed Apache 2.0 typer Installed MIT \u2003 Installed components are installed from PyPI. \u2003 Integrated components have their source code (or significant parts of) included in the Opteryx codebase. This list does not include transitive dependencies nor is guaranteed to be complete. Only components which have been integrated have copyright information noted, best efforts have been made to ensure this information is correct. Integrated components may differ from their original form. Cosmetic changes are not generally noted however where functionality has been added or altered, this is recorded in comments. pyarrow_ops has had significant changes to the original code and it is no longer practical to annotate all differences in the code. Note License information was correct as at 2022-06-03, or when updated in this document if later.","title":"Incorporated Components"},{"location":"get-started/release-notes/notices/#other-assets","text":"The Icarus Opteryx image based on ' Evening: Fall of Day ' by William Rimmer (Public Domain), more commonly associated with Led Zepplin's Swan Song . The Icarus Opteryx image is created using visual components from 'Archaeopteryx' fossil image by Caro Asercion ( CC BY 3.0 ). Satellite and Planet datasets acquired from devstronomy . Astronaut dataset acquired from Kaggle . Diagrams created using ASCII Flow or draw.io . Website build using mkdocs-material . SQL-92 conformity tests are based on sqltest .","title":"Other Assets"},{"location":"sql-reference/adv-engine-configuration/","text":"Engine Configuration Query Variables Variables can be used when a value within a query would benefit from being configurable by the user running the query. For example pre-built queries which perform the same core statement, but with a variable input. Variables are defined using the SET statement. These variables are available to SELECT statements as part of the same query batch. For example: -- set the planet id, change for different planets SET @ id = 3 ; SELECT name FROM $ planets WHERE id = @ id ; The above query batch contains two statements, the SET and the SELECT separated by a semicolon ( ; ). The variable is defined in the SET statement and must start with an at symbol ( @ ). The variable is then used within a filter in the WHERE clause of the SELECT statement. Query Parameters Query parameters which affect the execution of the query can be tuned on a per-query basis using the SET statement. enable_optimizer : boolean = True Use the query optimizer. internal_batch_size : int = 500 The maximum input frame size for JOIN s. max_join_size : int = 10000 The maximum number of records to create in a CROSS JOIN frame. page_size : int = 67108864 Approximate Page Size in bytes - default is 64Mb. enable_page_defragmentation : boolean = True Use the internal page defragmentation. WITH hints Hints are used to direct the planner, optimizer or the executor to make specific decisions. If a hint is not recognized, it is ignored by the planner and executor and is reported in the messages. Note Hints use the keyword WITH which is also the keyword for CTEs, this information relates to hints and not CTEs. FROM dataset WITH(NO_CACHE) Instructs blob/file reader to not use cache, regardless of other settings. This is almost always followed but only applies to the local Buffer Pool and any remote cache - Operating System, CDN or other caches which may be used are not affected by this hint. FROM dataset WITH(NO_PARTITION) Instructs the blob/file reader to not use partitioning, regardless of other settings. This is always followed. FROM dataset WITH(NO_PUSH_PROJECTION) Instructs the blob/file reader not to try to prune columns at read time. The decision if to follow this hint is made by the reader.","title":"Engine Configuration"},{"location":"sql-reference/adv-engine-configuration/#engine-configuration","text":"","title":"Engine Configuration"},{"location":"sql-reference/adv-engine-configuration/#query-variables","text":"Variables can be used when a value within a query would benefit from being configurable by the user running the query. For example pre-built queries which perform the same core statement, but with a variable input. Variables are defined using the SET statement. These variables are available to SELECT statements as part of the same query batch. For example: -- set the planet id, change for different planets SET @ id = 3 ; SELECT name FROM $ planets WHERE id = @ id ; The above query batch contains two statements, the SET and the SELECT separated by a semicolon ( ; ). The variable is defined in the SET statement and must start with an at symbol ( @ ). The variable is then used within a filter in the WHERE clause of the SELECT statement.","title":"Query Variables"},{"location":"sql-reference/adv-engine-configuration/#query-parameters","text":"Query parameters which affect the execution of the query can be tuned on a per-query basis using the SET statement. enable_optimizer : boolean = True Use the query optimizer. internal_batch_size : int = 500 The maximum input frame size for JOIN s. max_join_size : int = 10000 The maximum number of records to create in a CROSS JOIN frame. page_size : int = 67108864 Approximate Page Size in bytes - default is 64Mb. enable_page_defragmentation : boolean = True Use the internal page defragmentation.","title":"Query Parameters"},{"location":"sql-reference/adv-engine-configuration/#with-hints","text":"Hints are used to direct the planner, optimizer or the executor to make specific decisions. If a hint is not recognized, it is ignored by the planner and executor and is reported in the messages. Note Hints use the keyword WITH which is also the keyword for CTEs, this information relates to hints and not CTEs. FROM dataset WITH(NO_CACHE) Instructs blob/file reader to not use cache, regardless of other settings. This is almost always followed but only applies to the local Buffer Pool and any remote cache - Operating System, CDN or other caches which may be used are not affected by this hint. FROM dataset WITH(NO_PARTITION) Instructs the blob/file reader to not use partitioning, regardless of other settings. This is always followed. FROM dataset WITH(NO_PUSH_PROJECTION) Instructs the blob/file reader not to try to prune columns at read time. The decision if to follow this hint is made by the reader.","title":"WITH hints"},{"location":"sql-reference/adv-null-semantics/","text":"NULL Semantics Most comparisons to null return null . Exceptions are generally in functions or comparisons specifically to handle null , such as IS NULL . When the outcome of a comparison is null , this will be coerced to false when used in a filter ( WHERE or HAVING ) but return as null in a SELECT statement. To demonstrate, first a null comparison in a SELECT statement: SELECT name = null FROM $ planets ; This returns null for all values. name=null ----------- null null null null null null null null null Then a null comparison in a WHERE statement: SELECT name FROM $ planets WHERE name = null ; Returns an empty set. Note null comparison returning null holds true even for null = null . Do not test for null using an equals condition, use IS NULL .","title":"Null Semantics"},{"location":"sql-reference/adv-null-semantics/#null-semantics","text":"Most comparisons to null return null . Exceptions are generally in functions or comparisons specifically to handle null , such as IS NULL . When the outcome of a comparison is null , this will be coerced to false when used in a filter ( WHERE or HAVING ) but return as null in a SELECT statement. To demonstrate, first a null comparison in a SELECT statement: SELECT name = null FROM $ planets ; This returns null for all values. name=null ----------- null null null null null null null null null Then a null comparison in a WHERE statement: SELECT name FROM $ planets WHERE name = null ; Returns an empty set. Note null comparison returning null holds true even for null = null . Do not test for null using an equals condition, use IS NULL .","title":"NULL Semantics"},{"location":"sql-reference/adv-query-optimization/","text":"Query Optimization Adapted from 15 Best Practices for SQL Optimization . No optimization technique is universally true, these recommendations should improve performance in most cases. As will all optimization, test in your unique set of circumstances before assuming it to be true. 1. Avoid using SELECT * Selecting only the fields you need to be returned improves query performance by reducing the amount of data that is processed internally. A principle the Query Optimizer uses is to eliminate rows and columns to process as early as possible, SELECT * removes the option to remove columns from the data being processed. 2. Prune Early Where available, use temporal filters ( FOR DATE ) to limit the date range over will limit the number of partitions that need need to be read. Not reading the record is faster than reading and working out if it needs to be filtered out of the result set. 3. GROUP BY field selection VARCHAR Grouping by VARCHAR columns is usually slower than grouping by NUMERIC columns, if you have an option of grouping by a username or a numeric user id, prefer the user id. cardinality Grouping by columns with high cardinality (mostly unique) is generally slower than grouping where there is a lot of duplication in the groups. 4. Avoid CROSS JOIN Cross join will very likely create a lot of records that are not required - if you then filter these records from the two source tables using a WHERE clause, it's likely you should use an INNER JOIN instead. 5. Small table drives big table Most JOIN s require iterating over two relations, the left relation, which is the one in the FROM clause, and the right relation which is the one in the JOIN clause ( SELECT * FROM left JOIN right ). It is generally faster to put the smaller relation to the left . Note This advice may be contradictory to how other database engines optimize queries and may change in the future. 6. Use LIKE when comparing strings LIKE can be used for pattern matching but it can also be used for comparisions without wildcards and generally performs faster than = comparisons. 7. Use the correct JOIN A CROSS JOIN can quickly generate millions of records to be filtered, if you can use any join other than the CROSS JOIN , do that. 8. Use LIMIT LIMIT stops a query when it has returned the desired number of results; if you do not want the full dataset, using LIMIT can reduce the time taken to process a statement. However, some operations are 'greedy', that is, they need all of the data for their operation (for example ORDER BY , and GROUP BY ) - LIMIT does not have the same impact on these queries. 9. Use WHERE to filter before GROUP BY Only using HAVING to filter the aggregation results of GROUP BY . GROUP BY is a relatively expensive operation in terms of memory and compute, filter as much before the GROUP BY by using the WHERE clause and only use HAVING to filter the by aggregation function (e.g. COUNT , SUM ). 10. IS filters are generally faster than = IS comparisons are optimized for a specific check and perform up to twice as fast as = comparisons. However, they are only available for a limited set of checks: IS NONE IS NOT NONE IS TRUE IS FALSE","title":"Query Optimization"},{"location":"sql-reference/adv-query-optimization/#query-optimization","text":"Adapted from 15 Best Practices for SQL Optimization . No optimization technique is universally true, these recommendations should improve performance in most cases. As will all optimization, test in your unique set of circumstances before assuming it to be true.","title":"Query Optimization"},{"location":"sql-reference/adv-query-optimization/#1-avoid-using-select","text":"Selecting only the fields you need to be returned improves query performance by reducing the amount of data that is processed internally. A principle the Query Optimizer uses is to eliminate rows and columns to process as early as possible, SELECT * removes the option to remove columns from the data being processed.","title":"1. Avoid using SELECT *"},{"location":"sql-reference/adv-query-optimization/#2-prune-early","text":"Where available, use temporal filters ( FOR DATE ) to limit the date range over will limit the number of partitions that need need to be read. Not reading the record is faster than reading and working out if it needs to be filtered out of the result set.","title":"2. Prune Early"},{"location":"sql-reference/adv-query-optimization/#3-group-by-field-selection","text":"VARCHAR Grouping by VARCHAR columns is usually slower than grouping by NUMERIC columns, if you have an option of grouping by a username or a numeric user id, prefer the user id. cardinality Grouping by columns with high cardinality (mostly unique) is generally slower than grouping where there is a lot of duplication in the groups.","title":"3. GROUP BY field selection"},{"location":"sql-reference/adv-query-optimization/#4-avoid-cross-join","text":"Cross join will very likely create a lot of records that are not required - if you then filter these records from the two source tables using a WHERE clause, it's likely you should use an INNER JOIN instead.","title":"4. Avoid CROSS JOIN"},{"location":"sql-reference/adv-query-optimization/#5-small-table-drives-big-table","text":"Most JOIN s require iterating over two relations, the left relation, which is the one in the FROM clause, and the right relation which is the one in the JOIN clause ( SELECT * FROM left JOIN right ). It is generally faster to put the smaller relation to the left . Note This advice may be contradictory to how other database engines optimize queries and may change in the future.","title":"5. Small table drives big table"},{"location":"sql-reference/adv-query-optimization/#6-use-like-when-comparing-strings","text":"LIKE can be used for pattern matching but it can also be used for comparisions without wildcards and generally performs faster than = comparisons.","title":"6. Use LIKE when comparing strings"},{"location":"sql-reference/adv-query-optimization/#7-use-the-correct-join","text":"A CROSS JOIN can quickly generate millions of records to be filtered, if you can use any join other than the CROSS JOIN , do that.","title":"7. Use the correct JOIN"},{"location":"sql-reference/adv-query-optimization/#8-use-limit","text":"LIMIT stops a query when it has returned the desired number of results; if you do not want the full dataset, using LIMIT can reduce the time taken to process a statement. However, some operations are 'greedy', that is, they need all of the data for their operation (for example ORDER BY , and GROUP BY ) - LIMIT does not have the same impact on these queries.","title":"8. Use LIMIT"},{"location":"sql-reference/adv-query-optimization/#9-use-where-to-filter-before-group-by","text":"Only using HAVING to filter the aggregation results of GROUP BY . GROUP BY is a relatively expensive operation in terms of memory and compute, filter as much before the GROUP BY by using the WHERE clause and only use HAVING to filter the by aggregation function (e.g. COUNT , SUM ).","title":"9. Use WHERE to filter before GROUP BY"},{"location":"sql-reference/adv-query-optimization/#10-is-filters-are-generally-faster-than","text":"IS comparisons are optimized for a specific check and perform up to twice as fast as = comparisons. However, they are only available for a limited set of checks: IS NONE IS NOT NONE IS TRUE IS FALSE","title":"10. IS filters are generally faster than ="},{"location":"sql-reference/adv-sample-data/","text":"Sample Data There are three built-in relations for demonstration and testing. \u2003 $satellites (8 columns, 177 rows) \u2003 $planets (20 columns, 9 rows) #plutoisaplanet \u2003 $astronauts (19 columns, 357 rows) Satellite and Planet datasets acquired from this source . Astronaut dataset acquired from Kaggle . These relations are prefixed with a dollar sign ( $ ) and can be accessed as per user datasets. For example: SELECT * FROM $ planets ; Note A dataset called $no_table is used internally to represent no table has been specified, this is not intended for end-users and should not be used.","title":"Sample Data"},{"location":"sql-reference/adv-sample-data/#sample-data","text":"There are three built-in relations for demonstration and testing. \u2003 $satellites (8 columns, 177 rows) \u2003 $planets (20 columns, 9 rows) #plutoisaplanet \u2003 $astronauts (19 columns, 357 rows) Satellite and Planet datasets acquired from this source . Astronaut dataset acquired from Kaggle . These relations are prefixed with a dollar sign ( $ ) and can be accessed as per user datasets. For example: SELECT * FROM $ planets ; Note A dataset called $no_table is used internally to represent no table has been specified, this is not intended for end-users and should not be used.","title":"Sample Data"},{"location":"sql-reference/adv-temp-tables/","text":"Relation Constructors There are multiple options to create temporary relations as part of query definitions. These relations exist only for the execution of the query that defines them. Using VALUES VALUES allows you to create a multi-column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM ( VALUES ( 'High' , 3 ), ( 'Medium' , 2 ), ( 'Low' , 1 ) ) AS ratings ( name , rating ); Result: name | rating --------+-------- High | 3 Medium | 2 Low | 1 Using UNNEST UNNEST allows you to create a single column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM UNNEST (( 1 , 2 , 3 )); Result: unnest -------- 1 2 3 Note The values in the UNNEST function are in two sets of parenthesis. The function accepts a list of values, parenthesis is used to wrap parameters to functions and also used to define lists. Using generate_series generate_series allows you to create series by defining the bounds of the series, and optionally, an interval to step between values in the created series. generate_series supports the following variations: Form Types Description generate_series(stop) NUMERIC Generate a NUMERIC series between 1 and 'stop', with a step of 1 generate_series(start, stop) NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with a step of 1 generate_series(start, stop, step) NUMERIC, NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with an explicit step size generate_series(start, stop, interval) TIMESTAMP, TIMESTAMP, INTERVAL Generate a TIMESTAMP series between 'start' and 'stop', with a given interval generate_series(cidr) VARCHAR Generate set of IP addresses from a given CIDR (e.g. 192.168.0.0/24 ) Single Parameter Example (NUMERIC): SELECT * FROM generate_series ( 3 ) generate_series ----------------- 1 2 3 Single Parameter Example (VARCHAR): SELECT * FROM generate_series ( '192.168.1.0/30' ) generate_series ----------------- 192.168.1.0 192.168.1.1 192.168.1.2 192.168.1.3 Two parameter Example: SELECT * FROM generate_series ( 2 , 4 ) generate_series ----------------- 2 3 4 Three parameter NUMERIC Example: SELECT * FROM generate_series ( - 5 , 5 , 5 ) generate_series ----------------- -5 0 5 Three parameter TIMESTAMP example: SELECT * FROM generate_series ( '2020-01-01' , '2025-12-31' , '1y' ) generate_series ------------------ 2020-01-01 00:00 2021-01-01 00:00 2022-01-01 00:00 2023-01-01 00:00 2024-01-01 00:00 2025-01-01 00:00 Interval Definitions Intervals are defined quantifying one or more periods which make up the interval, supported periods and their notation are: Recognized interval parts for the GENERATE_SERIES function are: Period Symbol Aliases Years year / years y / yr / yrs Months month / months mo / mon / mons / mth / mths Weeks week / weeks w / wk / wks Days day / days d Hours hour / hours h / hr / hrs Minutes minute / minutes m / min / mins Seconds second / seconds s / sec / secs Where required, periods can be combined to define more complex intervals, for example 1h30m represents one hour and 30 minutes. Using FAKE FAKE creates a table of random integers from provided row and column counts. This functionality has limited application outside of creating datasets for testing. A simple example is as follows: SELECT * FROM FAKE ( 3 , 2 ); Result: column_0 \u2502 column_1 ------------\u253c------------ 32981 \u2502 50883 5037 \u2502 42087 51741 \u2502 49456","title":"Temporary Tables"},{"location":"sql-reference/adv-temp-tables/#relation-constructors","text":"There are multiple options to create temporary relations as part of query definitions. These relations exist only for the execution of the query that defines them.","title":"Relation Constructors"},{"location":"sql-reference/adv-temp-tables/#using-values","text":"VALUES allows you to create a multi-column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM ( VALUES ( 'High' , 3 ), ( 'Medium' , 2 ), ( 'Low' , 1 ) ) AS ratings ( name , rating ); Result: name | rating --------+-------- High | 3 Medium | 2 Low | 1","title":"Using VALUES"},{"location":"sql-reference/adv-temp-tables/#using-unnest","text":"UNNEST allows you to create a single column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM UNNEST (( 1 , 2 , 3 )); Result: unnest -------- 1 2 3 Note The values in the UNNEST function are in two sets of parenthesis. The function accepts a list of values, parenthesis is used to wrap parameters to functions and also used to define lists.","title":"Using UNNEST"},{"location":"sql-reference/adv-temp-tables/#using-generate_series","text":"generate_series allows you to create series by defining the bounds of the series, and optionally, an interval to step between values in the created series. generate_series supports the following variations: Form Types Description generate_series(stop) NUMERIC Generate a NUMERIC series between 1 and 'stop', with a step of 1 generate_series(start, stop) NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with a step of 1 generate_series(start, stop, step) NUMERIC, NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with an explicit step size generate_series(start, stop, interval) TIMESTAMP, TIMESTAMP, INTERVAL Generate a TIMESTAMP series between 'start' and 'stop', with a given interval generate_series(cidr) VARCHAR Generate set of IP addresses from a given CIDR (e.g. 192.168.0.0/24 ) Single Parameter Example (NUMERIC): SELECT * FROM generate_series ( 3 ) generate_series ----------------- 1 2 3 Single Parameter Example (VARCHAR): SELECT * FROM generate_series ( '192.168.1.0/30' ) generate_series ----------------- 192.168.1.0 192.168.1.1 192.168.1.2 192.168.1.3 Two parameter Example: SELECT * FROM generate_series ( 2 , 4 ) generate_series ----------------- 2 3 4 Three parameter NUMERIC Example: SELECT * FROM generate_series ( - 5 , 5 , 5 ) generate_series ----------------- -5 0 5 Three parameter TIMESTAMP example: SELECT * FROM generate_series ( '2020-01-01' , '2025-12-31' , '1y' ) generate_series ------------------ 2020-01-01 00:00 2021-01-01 00:00 2022-01-01 00:00 2023-01-01 00:00 2024-01-01 00:00 2025-01-01 00:00","title":"Using generate_series"},{"location":"sql-reference/adv-temp-tables/#using-fake","text":"FAKE creates a table of random integers from provided row and column counts. This functionality has limited application outside of creating datasets for testing. A simple example is as follows: SELECT * FROM FAKE ( 3 , 2 ); Result: column_0 \u2502 column_1 ------------\u253c------------ 32981 \u2502 50883 5037 \u2502 42087 51741 \u2502 49456","title":"Using FAKE"},{"location":"sql-reference/adv-time-travel/","text":"Time Travel The engine support temporality, the ability to view things as they were at a different point in time. For datasets which are snapshots, this allows you to recall the data of that snapshop as at a data in the past. For datasets which are logs, this allows you to prune queries to just the dates which contain relevant data. Note Data must be Mabel partitioned or using a custom partition schema which supports data partitioning. Data returned for previous days with be the latest data as at today. For example if a backfill updates data from seven days ago, when querying that data today the backfilled data will be returned. There is no implicit deduplication of records as they are returned. Partition schemes that supports temporal queries allow you to view data from a different date by using a FOR clause after the dateset name in the SQL statement. FOR clauses state the date, or date range, a query should retrieve results for. If no temporal clause is provided and the schema supports it, FOR TODAY is assumed. Warning Temporal clauses operate on calendar days in UTC. For example, from midnight FOR TODAY will return no data until data is written for that day. Single Dates Data from a specific, single, date can be obtained using the FOR date syntax. FOR date Date values in FOR clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example. FOR TODAY FOR YESTERDAY FOR '2022-02-14' Date Ranges Data within a range of dates can be specified using FOR DATES BETWEEN or FOR DATES IN syntax. Where data is retrieved for multiple dates, the datasets for each day have an implicit UNION ALL applied to them. FOR DATES BETWEEN start AND end FOR DATES IN range Date values in BETWEEN clauses must either be in 'YYYY-MM-DD' format or a recognized date placeholder, for example: FOR DATES BETWEEN '2000-01-01' AND TODAY FOR DATES BETWEEN '2020-04-01' AND '2020-04-30' Date range values in IN clauses must be recognized date range placeholders, for example: FOR DATES IN LAST_MONTH Placeholders Placeholder Applicability Description TODAY FOR, BETWEEN This calendar day YESTERDAY FOR, BETWEEN The previous calendar day THIS_MONTH IN Since the first of the current month LAST_MONTH IN The previous calendar month (also PREVIOUS_MONTH ) Caution FOR clauses cannot contain comments or reference column values or aliases Dates can not include times and must be in the format 'YYYY-MM-DD' The default partition scheme does not support Temporal queries Temporal clauses must follow the relation name they relate to, and they only apply to that relation. Time Travel You can query dates or date ranges using a FOR clause in your query. For example to view the contents of partition SELECT * FROM $ planets FOR YESTERDAY ; This technique is well suited to viewing snapshotted datasets from a previoud point in time. The '$planets' sample dataset has special handling to respond to temporal queries; Uranus was discovered in 1846 and Pluto was discovered in 1930, we and use the FOR clause to query the '$planets' relation from before those planets were discovered like this: SELECT name FROM $ planets FOR '1846-01-01' ; Returns (order may differ): name ------- Mercury Venus Earth Mars Jupiter Saturn Neptune Accumulation For datasets which are continually added to, such as logs, the FOR clause can be used to quickly filter ranges of records to search over. The FOR clause will most likely record the date the record was written (the 'SYSTEM_TIME' for the record) which may not be the same as the logical or effective date for a record, especially in situations where there is a lag in the records being recorded. The BETWEEN keyword can be used to describe ranges of records, this is useful for querying logged data between two dates. SELECT name FROM $ planets FOR DATES BETWEEN '2021-01-01' and '2022-12-31' ;","title":"Time Travel"},{"location":"sql-reference/adv-time-travel/#time-travel","text":"The engine support temporality, the ability to view things as they were at a different point in time. For datasets which are snapshots, this allows you to recall the data of that snapshop as at a data in the past. For datasets which are logs, this allows you to prune queries to just the dates which contain relevant data. Note Data must be Mabel partitioned or using a custom partition schema which supports data partitioning. Data returned for previous days with be the latest data as at today. For example if a backfill updates data from seven days ago, when querying that data today the backfilled data will be returned. There is no implicit deduplication of records as they are returned. Partition schemes that supports temporal queries allow you to view data from a different date by using a FOR clause after the dateset name in the SQL statement. FOR clauses state the date, or date range, a query should retrieve results for. If no temporal clause is provided and the schema supports it, FOR TODAY is assumed. Warning Temporal clauses operate on calendar days in UTC. For example, from midnight FOR TODAY will return no data until data is written for that day.","title":"Time Travel"},{"location":"sql-reference/adv-time-travel/#single-dates","text":"Data from a specific, single, date can be obtained using the FOR date syntax. FOR date Date values in FOR clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example. FOR TODAY FOR YESTERDAY FOR '2022-02-14'","title":"Single Dates"},{"location":"sql-reference/adv-time-travel/#date-ranges","text":"Data within a range of dates can be specified using FOR DATES BETWEEN or FOR DATES IN syntax. Where data is retrieved for multiple dates, the datasets for each day have an implicit UNION ALL applied to them. FOR DATES BETWEEN start AND end FOR DATES IN range Date values in BETWEEN clauses must either be in 'YYYY-MM-DD' format or a recognized date placeholder, for example: FOR DATES BETWEEN '2000-01-01' AND TODAY FOR DATES BETWEEN '2020-04-01' AND '2020-04-30' Date range values in IN clauses must be recognized date range placeholders, for example: FOR DATES IN LAST_MONTH","title":"Date Ranges"},{"location":"sql-reference/adv-time-travel/#placeholders","text":"Placeholder Applicability Description TODAY FOR, BETWEEN This calendar day YESTERDAY FOR, BETWEEN The previous calendar day THIS_MONTH IN Since the first of the current month LAST_MONTH IN The previous calendar month (also PREVIOUS_MONTH ) Caution FOR clauses cannot contain comments or reference column values or aliases Dates can not include times and must be in the format 'YYYY-MM-DD' The default partition scheme does not support Temporal queries Temporal clauses must follow the relation name they relate to, and they only apply to that relation.","title":"Placeholders"},{"location":"sql-reference/adv-time-travel/#time-travel_1","text":"You can query dates or date ranges using a FOR clause in your query. For example to view the contents of partition SELECT * FROM $ planets FOR YESTERDAY ; This technique is well suited to viewing snapshotted datasets from a previoud point in time. The '$planets' sample dataset has special handling to respond to temporal queries; Uranus was discovered in 1846 and Pluto was discovered in 1930, we and use the FOR clause to query the '$planets' relation from before those planets were discovered like this: SELECT name FROM $ planets FOR '1846-01-01' ; Returns (order may differ): name ------- Mercury Venus Earth Mars Jupiter Saturn Neptune","title":"Time Travel"},{"location":"sql-reference/adv-time-travel/#accumulation","text":"For datasets which are continually added to, such as logs, the FOR clause can be used to quickly filter ranges of records to search over. The FOR clause will most likely record the date the record was written (the 'SYSTEM_TIME' for the record) which may not be the same as the logical or effective date for a record, especially in situations where there is a lag in the records being recorded. The BETWEEN keyword can be used to describe ranges of records, this is useful for querying logged data between two dates. SELECT name FROM $ planets FOR DATES BETWEEN '2021-01-01' and '2022-12-31' ;","title":"Accumulation"},{"location":"sql-reference/adv-working-with-lists/","text":"Working with Lists A list is an ordered collection of zero or more VARCHAR values. Actions Accessing list[index] Testing value IN list Searching SEARCH(list, value) IN UNNEST(list) LIST_CONTAINS LIST_CONTAINS_ANY LIST_CONTAINS_ALL Transforms SORT Converting Lists to Relations Using UNNEST UNNEST allows you to create a single column table either as a list of literals, or from a column of LIST type in a dataset. SELECT * FROM UNNEST (( True , False )) AS Booleans ; Limitations Lists have the following limitations Statements cannot ORDER BY a list column Statements cannot contain DISTINCT and JOIN when the relations include list columns Lists cannot be used in comparisons Note Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove list columns as part of optimization. However, you should not rely on the optimizer to take any particular action.","title":"Working with Lists"},{"location":"sql-reference/adv-working-with-lists/#working-with-lists","text":"A list is an ordered collection of zero or more VARCHAR values.","title":"Working with Lists"},{"location":"sql-reference/adv-working-with-lists/#actions","text":"","title":"Actions"},{"location":"sql-reference/adv-working-with-lists/#converting-lists-to-relations","text":"","title":"Converting Lists to Relations"},{"location":"sql-reference/adv-working-with-lists/#limitations","text":"Lists have the following limitations Statements cannot ORDER BY a list column Statements cannot contain DISTINCT and JOIN when the relations include list columns Lists cannot be used in comparisons Note Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove list columns as part of optimization. However, you should not rely on the optimizer to take any particular action.","title":"Limitations"},{"location":"sql-reference/adv-working-with-structs/","text":"Working with Structs A struct is a collection of zero or more key, value pairs. Keys must be VARCHAR , values can be different types. Actions Reading struct[key] Values within structs can be accessed by key using subscript notation, putting the key in square brackets following the struct. Example: SELECT birth_place [ 'town' ] FROM $ astronauts Searching `SEARCH(struct, value)` All values in a struct can be searched for a given value using the SEARCH function. Example: SELECT name , SEARCH ( birth_place , 'Italy' ) FROM $ astronauts Limitations Structs have the following limitations Statements cannot ORDER BY a struct column Statements cannot contain DISTINCT and JOIN when the relations include struct columns Structs cannot be used in comparisons Note Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove struct columns as part of optimization. However, you should not rely on the optimizer to take any particular action.","title":"Working with Structs"},{"location":"sql-reference/adv-working-with-structs/#working-with-structs","text":"A struct is a collection of zero or more key, value pairs. Keys must be VARCHAR , values can be different types.","title":"Working with Structs"},{"location":"sql-reference/adv-working-with-structs/#actions","text":"","title":"Actions"},{"location":"sql-reference/adv-working-with-structs/#limitations","text":"Structs have the following limitations Statements cannot ORDER BY a struct column Statements cannot contain DISTINCT and JOIN when the relations include struct columns Structs cannot be used in comparisons Note Some restrictions may be resolved by the query optimizer, for example, Projection Pushdown may remove struct columns as part of optimization. However, you should not rely on the optimizer to take any particular action.","title":"Limitations"},{"location":"sql-reference/adv-working-with-timestamps/","text":"Working with Timestamps Working with Timestamps often involves working with Intervals. Actions Add/Subtract timestamp + interval \u2192 timestamp timestamp - interval \u2192 timestamp timestamp - timestamp \u2192 interval DATEDIFF ( unit : varchar , start : timestamp , end : timestamp ) \u2192 numeric Note INTERVAL may not support all functions in all circumstances. Construct INTERVAL values units Extract EXTRACT(part FROM timestamp) DATE(timestamp) Format DATE_FORMAT(timestamp, format) Parse CAST(field AS TIMESTAMP) TIMESTAMP(field) Truncate DATE_TRUNC(part, timestamp) TIME_BUCKET(timestamp, multiple, unit) Generate current_date current_time YESTERDAY() TIME() generate_series() Note that current_date and current_time support being called without parenthesis. Recognized date parts and periods and support across various functions: Part DATE_TRUNC EXTRACT DATEDIFF TIME_BUCKET Notes second \u2713 \u2713 \u2713 \u2713 minute \u2713 \u2713 \u2713 \u2713 hour \u2713 \u2713 \u2713 \u2713 day \u2713 \u2713 \u2713 \u2713 dow \u2718 \u2713 \u2718 \u2718 day of week week \u2713 \u2713 \u2713 \u2713 iso week i.e. to monday month \u2713 \u2713 \u25b2 \u2713 DATEFIFF unreliable calculating months quarter \u2713 \u2713 \u2713 \u2713 doy \u2718 \u2713 \u2718 \u2718 day of year year \u2713 \u2713 \u2713 \u2713 Implicit Casting In many situation where a timestamp is expected, if an ISO1806 formatted string is provided, the engine will interpret as a timestamp. Timezones The engine is opinionated to run in UTC - all instances where the system time is requested, UTC is used.","title":"Working with Timestamps"},{"location":"sql-reference/adv-working-with-timestamps/#working-with-timestamps","text":"Working with Timestamps often involves working with Intervals.","title":"Working with Timestamps"},{"location":"sql-reference/adv-working-with-timestamps/#actions","text":"","title":"Actions"},{"location":"sql-reference/adv-working-with-timestamps/#implicit-casting","text":"In many situation where a timestamp is expected, if an ISO1806 formatted string is provided, the engine will interpret as a timestamp.","title":"Implicit Casting"},{"location":"sql-reference/adv-working-with-timestamps/#timezones","text":"The engine is opinionated to run in UTC - all instances where the system time is requested, UTC is used.","title":"Timezones"},{"location":"sql-reference/aggregates/","text":"Aggregates Aggregates are functions that combine multiple rows into a single value. Aggregates can only be used in the SELECT and HAVING clauses of a SQL query. When the ORDER BY clause is provided, the values being aggregated are sorted after applying the function. Aggregate functions generally ignore null values when performing calculations. General Functions ANY_VALUE ( column ) \u2192 any Select any single value from the grouping. APPROXIMATE_MEDIAN ( column : numeric ) \u2192 numeric Approximate median of a column with T-Digest algorithm. AVG ( column : numeric ) \u2192 numeric The mean average of a numeric column. Also MEAN and AVERAGE . COUNT (*) \u2192 numeric Count the number of rows. COUNT ( column ) \u2192 numeric Count the number of non null values in column . COUNT_DISTINCT ( column ) \u2192 numeric Count the number of unique values. LIST ( column ) \u2192 array The complete list of values. MAX ( column ) \u2192 any The maximum value in column . Also MAXIMUM . MIN ( column ) \u2192 any The minimum value in column . Also MINIMUM . MIN_MAX ( column ) \u2192 struct The minimum and maximum values in column . ONE ( column ) \u2192 any Alias for 'ANY_VALUE'() PRODUCT ( column : numeric ) \u2192 numeric The product of values in column . STDDEV ( column : numeric ) \u2192 numeric The standard deviation of values in column . SUM ( column : numeric ) \u2192 numeric The sum of values in column . VARIANCE ( column : numeric ) \u2192 numeric The variance of values in column .","title":"Aggregates"},{"location":"sql-reference/aggregates/#aggregates","text":"Aggregates are functions that combine multiple rows into a single value. Aggregates can only be used in the SELECT and HAVING clauses of a SQL query. When the ORDER BY clause is provided, the values being aggregated are sorted after applying the function. Aggregate functions generally ignore null values when performing calculations.","title":"Aggregates"},{"location":"sql-reference/aggregates/#general-functions","text":"ANY_VALUE ( column ) \u2192 any Select any single value from the grouping. APPROXIMATE_MEDIAN ( column : numeric ) \u2192 numeric Approximate median of a column with T-Digest algorithm. AVG ( column : numeric ) \u2192 numeric The mean average of a numeric column. Also MEAN and AVERAGE . COUNT (*) \u2192 numeric Count the number of rows. COUNT ( column ) \u2192 numeric Count the number of non null values in column . COUNT_DISTINCT ( column ) \u2192 numeric Count the number of unique values. LIST ( column ) \u2192 array The complete list of values. MAX ( column ) \u2192 any The maximum value in column . Also MAXIMUM . MIN ( column ) \u2192 any The minimum value in column . Also MINIMUM . MIN_MAX ( column ) \u2192 struct The minimum and maximum values in column . ONE ( column ) \u2192 any Alias for 'ANY_VALUE'() PRODUCT ( column : numeric ) \u2192 numeric The product of values in column . STDDEV ( column : numeric ) \u2192 numeric The standard deviation of values in column . SUM ( column : numeric ) \u2192 numeric The sum of values in column . VARIANCE ( column : numeric ) \u2192 numeric The variance of values in column .","title":"General Functions"},{"location":"sql-reference/data-types/","text":"Data Types The engine supports a reduced set of types compared to full DBMS platforms. Types Name Description BOOLEAN Logical boolean (True/False). NUMERIC All numeric types. LIST An ordered sequence of strings. VARCHAR Variable-length character string. STRUCT A dictionary of multiple named values, where each key is a string, but the value can be a different type for each key. TIMESTAMP Combination of date and time. INTERVAL The difference between two TIMESTAMP values Note INTERVAL may not support all functions in all circumstances. Type Hinting Intervals Intervals require definition by type hints, using the type name before providing a literal description of the value. INTERVAL 'value' unit Where unit can be 'Year', 'Month', 'Day', 'Hour', 'Minute' or 'Second'. Casting Values can be cast using the CAST function, its form is CAST(any AS type) . Where values are incompatible, an error will be thrown, to avoid errors TRY_CAST can be used instead which will return NULL instead of error. BOOLEAN , NUMERIC and TIMESTAMP also support 'type string' notation ( SELECT TIMESTAMP '2022-01-01'; ) to perform casting to the desired type. Coercion Timestamps Literal values in quotes may be in interpreted as a TIMESTAMP when they match a valid date in ISO 8601 format (e.g. YYYY-MM-DD and YYYY-MM-DD HH:MM ). All TIMESTAMP and date values read from datasets are coerced to nanosecond precision timestamps. Numbers All numeric values included in SQL statements and read from datasets are coerced to 64bit floats.","title":"Data Types"},{"location":"sql-reference/data-types/#data-types","text":"The engine supports a reduced set of types compared to full DBMS platforms.","title":"Data Types"},{"location":"sql-reference/data-types/#types","text":"Name Description BOOLEAN Logical boolean (True/False). NUMERIC All numeric types. LIST An ordered sequence of strings. VARCHAR Variable-length character string. STRUCT A dictionary of multiple named values, where each key is a string, but the value can be a different type for each key. TIMESTAMP Combination of date and time. INTERVAL The difference between two TIMESTAMP values Note INTERVAL may not support all functions in all circumstances.","title":"Types"},{"location":"sql-reference/data-types/#type-hinting","text":"","title":"Type Hinting"},{"location":"sql-reference/data-types/#casting","text":"Values can be cast using the CAST function, its form is CAST(any AS type) . Where values are incompatible, an error will be thrown, to avoid errors TRY_CAST can be used instead which will return NULL instead of error. BOOLEAN , NUMERIC and TIMESTAMP also support 'type string' notation ( SELECT TIMESTAMP '2022-01-01'; ) to perform casting to the desired type.","title":"Casting"},{"location":"sql-reference/data-types/#coercion","text":"","title":"Coercion"},{"location":"sql-reference/expressions/","text":"Expressions An expression is a combination of values, operators and functions. Expressions are highly composable, and range from very simple to arbitrarily complex. They can be found in many different parts of SQL statements. In this section, we provide the different types of operators that can be used within expressions. Logical Operators Logical Operators are used within Expressions to express how predicates combine. The following logical operators are available: NOT , AND , OR , and XOR . a b a AND b a OR b a XOR b NOT a true true true true false false true false false true true false false false false false false true null true null null null null null false null null null null The operators AND , OR , and XOR are commutative, that is, you can switch the left and right operand without affecting the result. Comparison Operators Comparison Operators are used within Expressions to compare values, usually involving comparing a field within the datasets against a literal value - although comparisons can be used against two fields, or two literals. Usually when one of the values involved in the comparison is null , the result is null . Operator Description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to IN value in list NOT IN value not in list LIKE pattern match NOT LIKE inverse results of LIKE ILIKE case-insensitive pattern match NOT ILIKE inverse results of ILIKE ~ regular expression match (also SIMILAR TO ) !~ inverse results of ~ (also NOT SIMILAR TO ) ~* case insensitive regular expression match !~* inverse results of ~* IS special comparison for non-values true , false and null Other Comparisons Predicate Description a BETWEEN x AND y equivalent to a >= x AND a <= y a NOT BETWEEN x AND y equivalent to a < x OR a > y Warning Using BETWEEN with other predicates, especially when used with an AND conjunction, can cause the query parser to fail. Subqueries The IN operator can reference a sub query, this sub query cannot include a temporal clause ( FOR ), but otherwise the full syntax for SELECT queries are supported. For example, to find the planets without any satellites. SELECT name FROM $ planets WHERE id NOT IN ( SELECT DISTINCT planetId FROM $ satellites );","title":"Expressions"},{"location":"sql-reference/expressions/#expressions","text":"An expression is a combination of values, operators and functions. Expressions are highly composable, and range from very simple to arbitrarily complex. They can be found in many different parts of SQL statements. In this section, we provide the different types of operators that can be used within expressions.","title":"Expressions"},{"location":"sql-reference/expressions/#logical-operators","text":"Logical Operators are used within Expressions to express how predicates combine. The following logical operators are available: NOT , AND , OR , and XOR . a b a AND b a OR b a XOR b NOT a true true true true false false true false false true true false false false false false false true null true null null null null null false null null null null The operators AND , OR , and XOR are commutative, that is, you can switch the left and right operand without affecting the result.","title":"Logical Operators"},{"location":"sql-reference/expressions/#comparison-operators","text":"Comparison Operators are used within Expressions to compare values, usually involving comparing a field within the datasets against a literal value - although comparisons can be used against two fields, or two literals. Usually when one of the values involved in the comparison is null , the result is null . Operator Description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to IN value in list NOT IN value not in list LIKE pattern match NOT LIKE inverse results of LIKE ILIKE case-insensitive pattern match NOT ILIKE inverse results of ILIKE ~ regular expression match (also SIMILAR TO ) !~ inverse results of ~ (also NOT SIMILAR TO ) ~* case insensitive regular expression match !~* inverse results of ~* IS special comparison for non-values true , false and null","title":"Comparison Operators"},{"location":"sql-reference/expressions/#other-comparisons","text":"Predicate Description a BETWEEN x AND y equivalent to a >= x AND a <= y a NOT BETWEEN x AND y equivalent to a < x OR a > y Warning Using BETWEEN with other predicates, especially when used with an AND conjunction, can cause the query parser to fail.","title":"Other Comparisons"},{"location":"sql-reference/expressions/#subqueries","text":"The IN operator can reference a sub query, this sub query cannot include a temporal clause ( FOR ), but otherwise the full syntax for SELECT queries are supported. For example, to find the planets without any satellites. SELECT name FROM $ planets WHERE id NOT IN ( SELECT DISTINCT planetId FROM $ satellites );","title":"Subqueries"},{"location":"sql-reference/functions/","text":"Functions This document describes the supported SQL functions and operators. Generally functions will return null on null input, although note that this is not true in all circumstances, especially for null-aware functions like COALESCE and IFNULL . Definitions noted with a accept different input arguments. New functions for this version are annotated with the icon. Conversion Functions BOOLEAN any : any \u2192 boolean Cast any to a BOOLEAN , raises an error if cast is not possible. Note BOOLEAN does not require parenthesis, however any aliases do. Alias for CAST ( any AS BOOLEAN). CAST ( any : any AS type ) \u2192 [type] Cast any to type , raises an error if cast is not possible. Also implemented as individual cast functions. INT ( num : numeric ) \u2192 numeric Alias for INTEGER . INTEGER ( num : numeric ) \u2192 numeric Convert num to an integer. INTEGER is a psuedo-type, CAST is not supported and values may be coerced to NUMERIC . FLOAT ( num : numeric ) \u2192 numeric Convert num to a floating point number. FLOAT is a psuedo-type, CAST is not supported and values may be coerced to NUMERIC . NUMERIC any : any \u2192 numeric Cast any to a floating point number, raises an error if cast is not possible. Note NUMERIC does not require parenthesis, however any aliases do. Alias for CAST ( any AS NUMERIC). SAFE_CAST ( any : any AS type ) \u2192 [type] Alias for TRY_CAST ( any AS type ). STR ( any : any ) \u2192 varchar Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) New in 0.6 STRING ( any : any ) \u2192 varchar Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) TIMESTAMP iso8601 : varchar \u2192 timestamp Cast an ISO 8601 format string to a timestamp, raises an error if cast is not possible. Note TIMESTAMP does not require parenthesis, however any aliases do. Alias for CAST ( iso8601 AS TIMESTAMP). TRY_CAST ( any : any AS type ) \u2192 [type] Cast any to type , if cast is not possible, returns null . VARCHAR ( any ) \u2192 varchar Cast any to a string, raises an error if cast is not possible. Alias for CAST ( any AS VARCHAR). Date & Time Functions For more details, see Working with Timestamps . current_date \u2192 timestamp Return the current date, in UTC. Note current_date does not require parenthesis. current_time \u2192 timestamp Return the current date and time, in UTC. Note current_time does not require parenthesis. DATE ( ts : timestamp ) \u2192 timestamp Remove any time information, leaving just the date part of ts . DATE_FORMAT ( ts : timestamp , format : varchar ) \u2192 varchar Formats ts as a string using format . DATEPART ( unit : varchar , ts : timestamp ) \u2192 numeric Alias of EXTRACT ( unit FROM ts ). DATE_TRUNC ( unit : varchar , ts : timestamp ) \u2192 varchar Returns ts truncated to unit . DATEDIFF ( unit : varchar , start : timestamp , end : timestamp ) \u2192 numeric Calculate the difference between the start and end timestamps in a given unit . DAY ( timestamp ) \u2192 numeric Extract day number from a timestamp. See EXTRACT . EXTRACT ( unit FROM timestamp ) \u2192 numeric Extract unit of a timestamp. Also implemented as individual extraction functions. NOW () \u2192 timestamp Alias for current_time . TIME () \u2192 timestamp Current Time (UTC). TIME_BUCKET ( timestamp , multiple : numeric , unit : varchar ) \u2192 timestamp Floor timestamps into fixed time interval buckets. unit is optional and will be day if not provided. TODAY () \u2192 timestamp Alias for current_date . HOUR ( ts : timestamp ) \u2192 numeric Returns the hour of the day from ts . The value ranges from 0 to 23 . Alias for EXTRACT (hour FROM ts ). MINUTE ( ts : timestamp ) \u2192 numeric Returns the minute of the hour from ts . The value ranges from 0 to 59 . Alias for EXTRACT (minute FROM ts ) MONTH ( ts : timestamp ) \u2192 numeric Returns the month of the year from ts . The value ranges from 1 to 12 . Alias for EXTRACT (month FROM ts ) QUARTER ( ts : timestamp ) \u2192 numeric Returns the quarter of the year from ts . The value ranges from 1 to 4 . Alias for EXTRACT (quarter FROM ts ) SECOND ( ts : timestamp ) \u2192 numeric Returns the second of the minute from ts . The value ranges from 0 to 59 . Alias for EXTRACT (second FROM ts ) WEEK ( ts : timestamp ) \u2192 numeric Returns the week of the year from ts . The value ranges from 1 to 53 . Alias for EXTRACT (week FROM ts ) YEAR ( ts : timestamp ) \u2192 numeric Returns the year from ts . Alias for EXTRACT (year FROM ts ) Infix Functions These are functions that are called similar to comparison operators: numeric + numeric \u2192 numeric Numeric addition timestamp + interval \u2192 timestamp Timestamp and Interval addition numeric - numeric \u2192 numeric Numeric subtraction timestamp - interval \u2192 timestamp Timestamp and Interval subtraction timestamp - timestamp \u2192 interval Timestamp subtraction numeric * numeric \u2192 numeric Numeric multiplication numeric / numeric \u2192 numeric Numeric division numeric % numeric \u2192 numeric Numeric modulo (remainder) varchar || varchar \u2192 varchar String concatenation List Functions For more details, see Working with Lists . array : list [ index : numeric ] \u2192 value Return the index th element from array . GET ( array : list , index : numeric ) \u2192 value Alias of array [ index ] . GREATEST ( array : list ) \u2192 value Return the greatest value in array . Related: LEAST . LEAST ( array : list ) \u2192 value Return the smallest value in array . Related: GREATEST . LEN ( array : list ) \u2192 numeric Alias of LENGTH ( array ). LENGTH ( array : list ) \u2192 numeric Returns the number of elements in array . LIST_CONTAINS ( array : list , value ) \u2192 boolean Return true if array contains value . See also SEARCH ( array , value ). LIST_CONTAINS_ANY ( array : list , values : list ) \u2192 boolean Return true if array contains any elements in values . LIST_CONTAINS_ALL ( array : list , values : list ) \u2192 boolean Return true if array contains all of elements in values . SEARCH ( array : list , value ) \u2192 boolean Return true if array contains value . SORT ( array : list ) \u2192 list Return array in ascending order. Numeric Functions ABS ( x : numeric ) \u2192 numeric Alias of ABSOLUTE . ABSOLUTE ( x : numeric ) \u2192 numeric Returns the absolute value of x . CEIL ( x : numeric ) \u2192 numeric Alias of CEILING . CEILING ( x : numeric ) \u2192 numeric Returns x rounded up to the nearest integer. Related: FLOOR E () \u2192 numeric Returns the constant e , also known as Euler's number . Related: LN . FLOOR ( x : numeric ) \u2192 numeric Returns x rounded down to the nearest integer. PHI () \u2192 numeric Returns the constant \u03c6 ( phi ), also known as the golden ratio . PI () \u2192 numeric Returns the constant \u03c0 ( pi ). POWER ( base : numeric , exponent : numeric**) \u2192 _numeric Returns base to the power of exponent . LN ( x : numeric ) \u2192 numeric Returns the natural logarithm of x . Related: E , LOG , LOG10 , LOG2 . LOG ( x : numeric , base : numeric ) \u2192 numeric Returns the logarithm of x for base base . Related: LN , LOG10 , LOG2 . LOG10 ( x : numeric ) \u2192 numeric Returns the logarithm for base 10 of x . Related: LN , LOG , LOG2 . LOG2 ( x : numeric ) \u2192 numeric Returns the logarithm for base 2 of x . Related: LN , LOG , LOG10 . ROUND ( x : numeric ) \u2192 numeric Returns x rounded to the nearest integer. ROUND ( x : numeric , places : numeric ) \u2192 numeric Returns x rounded to places decimal places. SIGN ( x : numeric ) \u2192 numeric Returns the signum function of x ; 0 if x is 0, -1 if x is less than 0 and 1 if x is greater than 0. SIGNUM ( x : numeric ) \u2192 numeric Alias for SIGN . SQRT ( x : numeric ) \u2192 numeric Returns the square root of x . TRUNC ( x : numeric ) \u2192 numeric Alias of TRUNCATE . TRUNCATE ( x : numeric ) \u2192 numeric Returns x rounded to integer by dropping digits after decimal point. String Functions Functions for examining and manipulating string values. str : varchar [ index : numeric ] \u2192 varchar Subscript operator, return the index th character from str . CONCAT ( list : array < varchar >) \u2192 varchar Returns the result of concatenating, or joining, of two or more string values in an end-to-end manner. Related: CONCAT_WS . CONCAT_WS ( separator : varchar , list : array < varchar >) \u2192 varchar Returns the result of concatenating, or joining, of two or more string values with a separator used to delimit individual values. Related: CONCAT . ENDS_WITH ( str : varchar , value : varchar ) \u2192 boolean Return true if str ends with value . Related: STARTS_WITH . GET ( str : varchar , index : numeric ) \u2192 varchar Alias of str [ index ] . LEFT ( str : varchar , n : numeric ) \u2192 varchar Extract the left-most n characters of str . Related: RIGHT LEN ( str : varchar ) \u2192 numeric Alias of LENGTH LENGTH ( str : varchar ) \u2192 numeric Returns the length of str in characters. LOWER ( str : varchar ) \u2192 varchar Converts str to lowercase. Related: UPPER , TITLE . REVERSE ( str : varchar ) \u2192 varchar Returns str with the characters in reverse order. RIGHT ( str : varchar , n : numeric ) \u2192 varchar Extract the right-most n characters of str . Related: LEFT . SOUNDEX ( str : varchar ) \u2192 varchar Returns a character string containing the phonetic representation of char. See Soundex \ud83e\udc55 . SEARCH ( str : varchar , value : varchar ) \u2192 boolean Return true if str contains value . SUBSTRING ( str : varchar , start : numeric ) \u2192 varchar Return substring from a string from start position to the end of str . New in 0.6 SUBSTRING ( str : varchar , start : numeric , length : numeric ) \u2192 varchar Return substring from a string from start position for length characters. New in 0.6 STARTS_WITH ( str : varchar , value : varchar ) \u2192 boolean Return true if str starts with value . Related: ENDS_WITH TITLE ( str : varchar ) \u2192 varchar Returns str with the first letter of each work in upper case. Related: LOWER , UPPER . TRIM ( str : varchar ) \u2192 varchar Removes leading and trailing whitespace from str . UPPER ( str : varchar ) \u2192 varchar Converts str to uppercase. Related: LOWER , TITLE . Struct Functions For more details, see Working with Structs . object : struct [ key : varchar ] \u2192 value _ Subscript operator, return the value for key from object . GET ( object : struct , key : varchar ) \u2192 value _ Alias of object [ key ] . SEARCH ( object : struct , value : varchar ) \u2192 boolean _ Return true if any of the values in object is value . Note SEARCH does not match struct keys. System Functions VERSION () \u2192 varchar Return the version of the query engine. Other Functions BASE64_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using BASE64 encoding. Related: BASE64_ENCODE . BASE64_ENCODE ( any ) \u2192 varchar Encode value with BASE64 encoding. Related: BASE64_DECODE . BASE85_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using BASE85 encoding. Related: BASE85_ENCODE . BASE85_ENCODE ( any ) \u2192 varchar Encode value with BASE85 encoding. Related: BASE85_DECODE . COALESCE ( arg1 , arg2 , ...) \u2192 [input type] Return the first item from args which is not null . Related: IFNULL . GENERATE_SERIES ( stop : numeric ) \u2192 list < numeric > _ Return a numeric list between 1 and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric ) \u2192 list < numeric > Return a numeric list between start and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric , step : numeric ) \u2192 list < numeric > Return a numeric list between start and stop , with an increment of step . GENERATE_SERIES ( start : timestamp , stop : timestamp , interval ) \u2192 list < timestamp > _ _ Return a timestamp list between start and stop , with a interval of step . GENERATE_SERIES ( cidr : varchar ) \u2192 list < varchar > Return a list of IP addresses from a given cidr . IFNULL ( check_expression : any , replacement_value : any ) \u2192 [input type] Returns check_expression if not null , otherwise returns replacement_value . Related: COALESCE . HASH ( any ) \u2192 varchar Calculate the CityHash (64 bit). HEX_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using hexadecimal (Base16) encoding. Related: HEX_ENCODE . HEX_ENCODE ( any ) \u2192 varchar Encode value with hexadecimal (Base16) encoding. Related: HEX_DECODE . IIF ( condition , true_value , false_value ) \u2192 [input type] Return the true_value if the condition evaluates to True , otherwise return the false_value . IFNULL ( value1 : any , value2 : any ) \u2192 [input type] Returns null if value1 equals value2 , otherwise returns value1 . New in 0.6 NORMAL () \u2192 numeric Random number from a normal (Gaussian) distribution; distribution is centred at 0.0 and has a standard deviation of 1.0. MD5 ( any ) \u2192 varchar Calculate the MD5 hash. RAND () \u2192 numeric Returns a random number between 0 and 1. RANDOM () \u2192 numeric Alias of RAND (). RANDOM_STRING ( length : numeric ) \u2192 varchar Returns a random string of lowercase alphabetic characters with a length of length . SHA1 ( any ) \u2192 varchar Calculate the SHA1 hash. Related: SHA224 , SHA256 , SHA384 , SHA512 . SHA224 ( any ) \u2192 varchar Calculate the SHA224 hash. Related: SHA1 , SHA256 , SHA384 , SHA512 . SHA256 ( any ) \u2192 varchar Calculate the SHA256 hash. Related: SHA1 , SHA224 , SHA384 , SHA512 . SHA384 ( any ) \u2192 varchar Calculate the SHA384 hash. Related: SHA1 , SHA224 , SHA256 , SHA512 . SHA512 ( any ) \u2192 varchar Calculate the SHA512 hash. Related: SHA1 , SHA224 , SHA256 , SHA384 . UNNEST ( array : list ) \u2192 relation Create a virtual relation with a row for each element in array .","title":"Functions"},{"location":"sql-reference/functions/#functions","text":"This document describes the supported SQL functions and operators. Generally functions will return null on null input, although note that this is not true in all circumstances, especially for null-aware functions like COALESCE and IFNULL . Definitions noted with a accept different input arguments. New functions for this version are annotated with the icon.","title":"Functions"},{"location":"sql-reference/functions/#conversion-functions","text":"BOOLEAN any : any \u2192 boolean Cast any to a BOOLEAN , raises an error if cast is not possible. Note BOOLEAN does not require parenthesis, however any aliases do. Alias for CAST ( any AS BOOLEAN). CAST ( any : any AS type ) \u2192 [type] Cast any to type , raises an error if cast is not possible. Also implemented as individual cast functions. INT ( num : numeric ) \u2192 numeric Alias for INTEGER . INTEGER ( num : numeric ) \u2192 numeric Convert num to an integer. INTEGER is a psuedo-type, CAST is not supported and values may be coerced to NUMERIC . FLOAT ( num : numeric ) \u2192 numeric Convert num to a floating point number. FLOAT is a psuedo-type, CAST is not supported and values may be coerced to NUMERIC . NUMERIC any : any \u2192 numeric Cast any to a floating point number, raises an error if cast is not possible. Note NUMERIC does not require parenthesis, however any aliases do. Alias for CAST ( any AS NUMERIC). SAFE_CAST ( any : any AS type ) \u2192 [type] Alias for TRY_CAST ( any AS type ). STR ( any : any ) \u2192 varchar Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) New in 0.6 STRING ( any : any ) \u2192 varchar Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) TIMESTAMP iso8601 : varchar \u2192 timestamp Cast an ISO 8601 format string to a timestamp, raises an error if cast is not possible. Note TIMESTAMP does not require parenthesis, however any aliases do. Alias for CAST ( iso8601 AS TIMESTAMP). TRY_CAST ( any : any AS type ) \u2192 [type] Cast any to type , if cast is not possible, returns null . VARCHAR ( any ) \u2192 varchar Cast any to a string, raises an error if cast is not possible. Alias for CAST ( any AS VARCHAR).","title":"Conversion Functions"},{"location":"sql-reference/functions/#date-time-functions","text":"For more details, see Working with Timestamps . current_date \u2192 timestamp Return the current date, in UTC. Note current_date does not require parenthesis. current_time \u2192 timestamp Return the current date and time, in UTC. Note current_time does not require parenthesis. DATE ( ts : timestamp ) \u2192 timestamp Remove any time information, leaving just the date part of ts . DATE_FORMAT ( ts : timestamp , format : varchar ) \u2192 varchar Formats ts as a string using format . DATEPART ( unit : varchar , ts : timestamp ) \u2192 numeric Alias of EXTRACT ( unit FROM ts ). DATE_TRUNC ( unit : varchar , ts : timestamp ) \u2192 varchar Returns ts truncated to unit . DATEDIFF ( unit : varchar , start : timestamp , end : timestamp ) \u2192 numeric Calculate the difference between the start and end timestamps in a given unit . DAY ( timestamp ) \u2192 numeric Extract day number from a timestamp. See EXTRACT . EXTRACT ( unit FROM timestamp ) \u2192 numeric Extract unit of a timestamp. Also implemented as individual extraction functions. NOW () \u2192 timestamp Alias for current_time . TIME () \u2192 timestamp Current Time (UTC). TIME_BUCKET ( timestamp , multiple : numeric , unit : varchar ) \u2192 timestamp Floor timestamps into fixed time interval buckets. unit is optional and will be day if not provided. TODAY () \u2192 timestamp Alias for current_date . HOUR ( ts : timestamp ) \u2192 numeric Returns the hour of the day from ts . The value ranges from 0 to 23 . Alias for EXTRACT (hour FROM ts ). MINUTE ( ts : timestamp ) \u2192 numeric Returns the minute of the hour from ts . The value ranges from 0 to 59 . Alias for EXTRACT (minute FROM ts ) MONTH ( ts : timestamp ) \u2192 numeric Returns the month of the year from ts . The value ranges from 1 to 12 . Alias for EXTRACT (month FROM ts ) QUARTER ( ts : timestamp ) \u2192 numeric Returns the quarter of the year from ts . The value ranges from 1 to 4 . Alias for EXTRACT (quarter FROM ts ) SECOND ( ts : timestamp ) \u2192 numeric Returns the second of the minute from ts . The value ranges from 0 to 59 . Alias for EXTRACT (second FROM ts ) WEEK ( ts : timestamp ) \u2192 numeric Returns the week of the year from ts . The value ranges from 1 to 53 . Alias for EXTRACT (week FROM ts ) YEAR ( ts : timestamp ) \u2192 numeric Returns the year from ts . Alias for EXTRACT (year FROM ts )","title":"Date &amp; Time Functions"},{"location":"sql-reference/functions/#infix-functions","text":"These are functions that are called similar to comparison operators: numeric + numeric \u2192 numeric Numeric addition timestamp + interval \u2192 timestamp Timestamp and Interval addition numeric - numeric \u2192 numeric Numeric subtraction timestamp - interval \u2192 timestamp Timestamp and Interval subtraction timestamp - timestamp \u2192 interval Timestamp subtraction numeric * numeric \u2192 numeric Numeric multiplication numeric / numeric \u2192 numeric Numeric division numeric % numeric \u2192 numeric Numeric modulo (remainder) varchar || varchar \u2192 varchar String concatenation","title":"Infix Functions"},{"location":"sql-reference/functions/#list-functions","text":"For more details, see Working with Lists . array : list [ index : numeric ] \u2192 value Return the index th element from array . GET ( array : list , index : numeric ) \u2192 value Alias of array [ index ] . GREATEST ( array : list ) \u2192 value Return the greatest value in array . Related: LEAST . LEAST ( array : list ) \u2192 value Return the smallest value in array . Related: GREATEST . LEN ( array : list ) \u2192 numeric Alias of LENGTH ( array ). LENGTH ( array : list ) \u2192 numeric Returns the number of elements in array . LIST_CONTAINS ( array : list , value ) \u2192 boolean Return true if array contains value . See also SEARCH ( array , value ). LIST_CONTAINS_ANY ( array : list , values : list ) \u2192 boolean Return true if array contains any elements in values . LIST_CONTAINS_ALL ( array : list , values : list ) \u2192 boolean Return true if array contains all of elements in values . SEARCH ( array : list , value ) \u2192 boolean Return true if array contains value . SORT ( array : list ) \u2192 list Return array in ascending order.","title":"List Functions"},{"location":"sql-reference/functions/#numeric-functions","text":"ABS ( x : numeric ) \u2192 numeric Alias of ABSOLUTE . ABSOLUTE ( x : numeric ) \u2192 numeric Returns the absolute value of x . CEIL ( x : numeric ) \u2192 numeric Alias of CEILING . CEILING ( x : numeric ) \u2192 numeric Returns x rounded up to the nearest integer. Related: FLOOR E () \u2192 numeric Returns the constant e , also known as Euler's number . Related: LN . FLOOR ( x : numeric ) \u2192 numeric Returns x rounded down to the nearest integer. PHI () \u2192 numeric Returns the constant \u03c6 ( phi ), also known as the golden ratio . PI () \u2192 numeric Returns the constant \u03c0 ( pi ). POWER ( base : numeric , exponent : numeric**) \u2192 _numeric Returns base to the power of exponent . LN ( x : numeric ) \u2192 numeric Returns the natural logarithm of x . Related: E , LOG , LOG10 , LOG2 . LOG ( x : numeric , base : numeric ) \u2192 numeric Returns the logarithm of x for base base . Related: LN , LOG10 , LOG2 . LOG10 ( x : numeric ) \u2192 numeric Returns the logarithm for base 10 of x . Related: LN , LOG , LOG2 . LOG2 ( x : numeric ) \u2192 numeric Returns the logarithm for base 2 of x . Related: LN , LOG , LOG10 . ROUND ( x : numeric ) \u2192 numeric Returns x rounded to the nearest integer. ROUND ( x : numeric , places : numeric ) \u2192 numeric Returns x rounded to places decimal places. SIGN ( x : numeric ) \u2192 numeric Returns the signum function of x ; 0 if x is 0, -1 if x is less than 0 and 1 if x is greater than 0. SIGNUM ( x : numeric ) \u2192 numeric Alias for SIGN . SQRT ( x : numeric ) \u2192 numeric Returns the square root of x . TRUNC ( x : numeric ) \u2192 numeric Alias of TRUNCATE . TRUNCATE ( x : numeric ) \u2192 numeric Returns x rounded to integer by dropping digits after decimal point.","title":"Numeric Functions"},{"location":"sql-reference/functions/#string-functions","text":"Functions for examining and manipulating string values. str : varchar [ index : numeric ] \u2192 varchar Subscript operator, return the index th character from str . CONCAT ( list : array < varchar >) \u2192 varchar Returns the result of concatenating, or joining, of two or more string values in an end-to-end manner. Related: CONCAT_WS . CONCAT_WS ( separator : varchar , list : array < varchar >) \u2192 varchar Returns the result of concatenating, or joining, of two or more string values with a separator used to delimit individual values. Related: CONCAT . ENDS_WITH ( str : varchar , value : varchar ) \u2192 boolean Return true if str ends with value . Related: STARTS_WITH . GET ( str : varchar , index : numeric ) \u2192 varchar Alias of str [ index ] . LEFT ( str : varchar , n : numeric ) \u2192 varchar Extract the left-most n characters of str . Related: RIGHT LEN ( str : varchar ) \u2192 numeric Alias of LENGTH LENGTH ( str : varchar ) \u2192 numeric Returns the length of str in characters. LOWER ( str : varchar ) \u2192 varchar Converts str to lowercase. Related: UPPER , TITLE . REVERSE ( str : varchar ) \u2192 varchar Returns str with the characters in reverse order. RIGHT ( str : varchar , n : numeric ) \u2192 varchar Extract the right-most n characters of str . Related: LEFT . SOUNDEX ( str : varchar ) \u2192 varchar Returns a character string containing the phonetic representation of char. See Soundex \ud83e\udc55 . SEARCH ( str : varchar , value : varchar ) \u2192 boolean Return true if str contains value . SUBSTRING ( str : varchar , start : numeric ) \u2192 varchar Return substring from a string from start position to the end of str . New in 0.6 SUBSTRING ( str : varchar , start : numeric , length : numeric ) \u2192 varchar Return substring from a string from start position for length characters. New in 0.6 STARTS_WITH ( str : varchar , value : varchar ) \u2192 boolean Return true if str starts with value . Related: ENDS_WITH TITLE ( str : varchar ) \u2192 varchar Returns str with the first letter of each work in upper case. Related: LOWER , UPPER . TRIM ( str : varchar ) \u2192 varchar Removes leading and trailing whitespace from str . UPPER ( str : varchar ) \u2192 varchar Converts str to uppercase. Related: LOWER , TITLE .","title":"String Functions"},{"location":"sql-reference/functions/#struct-functions","text":"For more details, see Working with Structs . object : struct [ key : varchar ] \u2192 value _ Subscript operator, return the value for key from object . GET ( object : struct , key : varchar ) \u2192 value _ Alias of object [ key ] . SEARCH ( object : struct , value : varchar ) \u2192 boolean _ Return true if any of the values in object is value . Note SEARCH does not match struct keys.","title":"Struct Functions"},{"location":"sql-reference/functions/#system-functions","text":"VERSION () \u2192 varchar Return the version of the query engine.","title":"System Functions"},{"location":"sql-reference/functions/#other-functions","text":"BASE64_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using BASE64 encoding. Related: BASE64_ENCODE . BASE64_ENCODE ( any ) \u2192 varchar Encode value with BASE64 encoding. Related: BASE64_DECODE . BASE85_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using BASE85 encoding. Related: BASE85_ENCODE . BASE85_ENCODE ( any ) \u2192 varchar Encode value with BASE85 encoding. Related: BASE85_DECODE . COALESCE ( arg1 , arg2 , ...) \u2192 [input type] Return the first item from args which is not null . Related: IFNULL . GENERATE_SERIES ( stop : numeric ) \u2192 list < numeric > _ Return a numeric list between 1 and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric ) \u2192 list < numeric > Return a numeric list between start and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric , step : numeric ) \u2192 list < numeric > Return a numeric list between start and stop , with an increment of step . GENERATE_SERIES ( start : timestamp , stop : timestamp , interval ) \u2192 list < timestamp > _ _ Return a timestamp list between start and stop , with a interval of step . GENERATE_SERIES ( cidr : varchar ) \u2192 list < varchar > Return a list of IP addresses from a given cidr . IFNULL ( check_expression : any , replacement_value : any ) \u2192 [input type] Returns check_expression if not null , otherwise returns replacement_value . Related: COALESCE . HASH ( any ) \u2192 varchar Calculate the CityHash (64 bit). HEX_DECODE ( any ) \u2192 varchar Decode a value which has been encoded using hexadecimal (Base16) encoding. Related: HEX_ENCODE . HEX_ENCODE ( any ) \u2192 varchar Encode value with hexadecimal (Base16) encoding. Related: HEX_DECODE . IIF ( condition , true_value , false_value ) \u2192 [input type] Return the true_value if the condition evaluates to True , otherwise return the false_value . IFNULL ( value1 : any , value2 : any ) \u2192 [input type] Returns null if value1 equals value2 , otherwise returns value1 . New in 0.6 NORMAL () \u2192 numeric Random number from a normal (Gaussian) distribution; distribution is centred at 0.0 and has a standard deviation of 1.0. MD5 ( any ) \u2192 varchar Calculate the MD5 hash. RAND () \u2192 numeric Returns a random number between 0 and 1. RANDOM () \u2192 numeric Alias of RAND (). RANDOM_STRING ( length : numeric ) \u2192 varchar Returns a random string of lowercase alphabetic characters with a length of length . SHA1 ( any ) \u2192 varchar Calculate the SHA1 hash. Related: SHA224 , SHA256 , SHA384 , SHA512 . SHA224 ( any ) \u2192 varchar Calculate the SHA224 hash. Related: SHA1 , SHA256 , SHA384 , SHA512 . SHA256 ( any ) \u2192 varchar Calculate the SHA256 hash. Related: SHA1 , SHA224 , SHA384 , SHA512 . SHA384 ( any ) \u2192 varchar Calculate the SHA384 hash. Related: SHA1 , SHA224 , SHA256 , SHA512 . SHA512 ( any ) \u2192 varchar Calculate the SHA512 hash. Related: SHA1 , SHA224 , SHA256 , SHA384 . UNNEST ( array : list ) \u2192 relation Create a virtual relation with a row for each element in array .","title":"Other Functions"},{"location":"sql-reference/introduction/","text":"SQL Introduction This tutorial is reworked from the DuckDB tutorial. Overview This page provides an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an introduction and is not a complete tutorial on SQL. All queries use the internal sample NASA datasets and should work regardless of the data your installation and set up has access to. Concepts Opteryx is a system for querying ad hoc data stored in files as relations . A relation is mathematical term for a data table. Each relation is a named collection of rows, organized in columns, each column should be a common datatype. As an ad hoc query engine, the relations and their schema do not need to be predefined, they are determined at the time the query is run. This is one of the reasons Opteryx cannot be considered a RDBMS (relational database management system), even though it can be used to query data using SQL. Querying Relations To retrieve data from a relation, the relation is queried using a SQL SELECT statement. Basic statements are made of three parts; the list of columns to be returned and the list of relations to retrieve data from, and optional clauses to shape and filter the data that is returned. SELECT * FROM $ planets ; The * is shorthand for \"all columns\", by convention keywords are capitalized, and ; optionally terminates the query. SELECT id , name FROM $ planets WHERE name = 'Earth' ; The output of the above query should be id | name ----+------- 3 | Earth You can write functions, not just simple column references, in the select list. For example, you can write: SELECT id , UPPER ( name ) AS uppercase_name FROM $ planets WHERE id = 3 ; This should give: id | uppercase_name ----+---------------- 3 | EARTH Notice how the AS clause is used to relabel the output column. (The AS clause is optional.) A query can be \u201cqualified\u201d by adding a WHERE clause that specifies which rows are wanted. The WHERE clause contains a Boolean (truth value) expression, and only rows for which the Boolean expression is true are returned. The usual Boolean operators ( AND , OR , and NOT ) are allowed in the qualification. The SELECT clause can be thought of as choosing which columns we want from the relation, and the WHERE clause as choosing which rows we want from the relation. For example, the following the planets with fewer than 10 moons and a day longer than 24 hours: SELECT * FROM $ planets WHERE lengthOfDay > 24 AND numberOfMoons < 10 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 Mars | 24.7 | 2 Pluto | 153.3 | 5 The order of results are not guaranteed and should not be relied upon. If you request the results of the below query, you might get the Mercury or Venus in either order. Note The same query, of the same data in the same version of the query engine will likely to return results in the same order, don't expect to test result order non-determinism by rerunning the query millions of times and looking for differences. These differences may manifest over different versions, or from subtle differences to the query statement or data. SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 But you\u2019d always get the results shown above if you do: SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ORDER BY name ; You can request that duplicate rows be removed from the result of a query: SELECT DISTINCT planetId FROM $ satellites ; Result: planetId -------- 3 4 5 6 7 8 9 Here again, the result row ordering might vary. You can ensure consistent results by using DISTINCT and ORDER BY together: SELECT DISTINCT planetId FROM $ satellites ORDER BY planetId ; Joins Between Relations So far our queries have only accessed one relation at a time. Queries can access multiple relations at once, or access the same relation in such a way that multiple rows of the relation are being processed at the same time. A query that accesses multiple rows of the same or different relations at one time is called a join query. As an example, say you wish to list all the $satellites records together with the planet they orbit. To do that, we need to compare the planetId of each row of the $satellites relation with the id column of all rows in the $planets relation, and return the pairs of rows where these values match. This would be accomplished by the following query: SELECT * FROM $ satellites , $ planets WHERE planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Observe two things about the result set: There are no result row for the planets of Mercury or Venus ( planetIds 1 and 2). This is because there is no matching entry in the $satellites relation for these planets, so the join ignores the unmatched rows in the $planets relation. Each of the relations being joined have an id and a name column, to ensure it is clear which relation the value being displayed is from, columns with clashing names are qualified with the relation name. To avoid abiguity and problems in the future if new columns are added to relations, it is good practice to qualify column names in join conditions: SELECT * FROM $ satellites , $ planets WHERE $ satellites . planetId = $ planets . id ; Will return the same result as above, but be more resistant to future failure. Join queries of the kind seen thus far can also be written in this alternative form: SELECT * FROM $ satellites INNER JOIN $ planets ON $ satellites . planetId = $ planets . id ; The planner currently uses a different execution strategy for these two similar queries, the explicit INNER JOIN style generally executes faster. Now we will figure out how we can get the Mercury and Venus records back in. What we want the query to do is to scan the $planets relation and for each row to find the matching $satellites row(s). If no matching row is found we want some \u201cempty values\u201d to be substituted for the $satellites relations columns. This kind of query is called an outer join. (The joins we have seen so far are inner joins and cross joins.) The command looks like this: SELECT * FROM $ satellites LEFT OUTER JOIN $ planets ON $ satellites . planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- | 1 | | | 2 | | 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Using the LEFT OUTER JOIN will mean the relation mentioned on the left of the join operator will have each of its rows in the output at least once, whereas the relation on the right will only have those rows output that match some row of the left relation. When outputting a left-relation row for which there is no right-relation match, empty ( null ) values are substituted for the right-relation columns. Note How null values are displayed may be different between different systems, common approaches are to display an empty cell or display 'none' or 'null' in an alternate format (e.g. italics or different font color). This is not controlled by the query engine. Aggregate Functions An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT , SUM , AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.","title":"SQL Introduction"},{"location":"sql-reference/introduction/#sql-introduction","text":"This tutorial is reworked from the DuckDB tutorial.","title":"SQL Introduction"},{"location":"sql-reference/introduction/#overview","text":"This page provides an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an introduction and is not a complete tutorial on SQL. All queries use the internal sample NASA datasets and should work regardless of the data your installation and set up has access to.","title":"Overview"},{"location":"sql-reference/introduction/#concepts","text":"Opteryx is a system for querying ad hoc data stored in files as relations . A relation is mathematical term for a data table. Each relation is a named collection of rows, organized in columns, each column should be a common datatype. As an ad hoc query engine, the relations and their schema do not need to be predefined, they are determined at the time the query is run. This is one of the reasons Opteryx cannot be considered a RDBMS (relational database management system), even though it can be used to query data using SQL.","title":"Concepts"},{"location":"sql-reference/introduction/#querying-relations","text":"To retrieve data from a relation, the relation is queried using a SQL SELECT statement. Basic statements are made of three parts; the list of columns to be returned and the list of relations to retrieve data from, and optional clauses to shape and filter the data that is returned. SELECT * FROM $ planets ; The * is shorthand for \"all columns\", by convention keywords are capitalized, and ; optionally terminates the query. SELECT id , name FROM $ planets WHERE name = 'Earth' ; The output of the above query should be id | name ----+------- 3 | Earth You can write functions, not just simple column references, in the select list. For example, you can write: SELECT id , UPPER ( name ) AS uppercase_name FROM $ planets WHERE id = 3 ; This should give: id | uppercase_name ----+---------------- 3 | EARTH Notice how the AS clause is used to relabel the output column. (The AS clause is optional.) A query can be \u201cqualified\u201d by adding a WHERE clause that specifies which rows are wanted. The WHERE clause contains a Boolean (truth value) expression, and only rows for which the Boolean expression is true are returned. The usual Boolean operators ( AND , OR , and NOT ) are allowed in the qualification. The SELECT clause can be thought of as choosing which columns we want from the relation, and the WHERE clause as choosing which rows we want from the relation. For example, the following the planets with fewer than 10 moons and a day longer than 24 hours: SELECT * FROM $ planets WHERE lengthOfDay > 24 AND numberOfMoons < 10 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 Mars | 24.7 | 2 Pluto | 153.3 | 5 The order of results are not guaranteed and should not be relied upon. If you request the results of the below query, you might get the Mercury or Venus in either order. Note The same query, of the same data in the same version of the query engine will likely to return results in the same order, don't expect to test result order non-determinism by rerunning the query millions of times and looking for differences. These differences may manifest over different versions, or from subtle differences to the query statement or data. SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 But you\u2019d always get the results shown above if you do: SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ORDER BY name ; You can request that duplicate rows be removed from the result of a query: SELECT DISTINCT planetId FROM $ satellites ; Result: planetId -------- 3 4 5 6 7 8 9 Here again, the result row ordering might vary. You can ensure consistent results by using DISTINCT and ORDER BY together: SELECT DISTINCT planetId FROM $ satellites ORDER BY planetId ;","title":"Querying Relations"},{"location":"sql-reference/introduction/#joins-between-relations","text":"So far our queries have only accessed one relation at a time. Queries can access multiple relations at once, or access the same relation in such a way that multiple rows of the relation are being processed at the same time. A query that accesses multiple rows of the same or different relations at one time is called a join query. As an example, say you wish to list all the $satellites records together with the planet they orbit. To do that, we need to compare the planetId of each row of the $satellites relation with the id column of all rows in the $planets relation, and return the pairs of rows where these values match. This would be accomplished by the following query: SELECT * FROM $ satellites , $ planets WHERE planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Observe two things about the result set: There are no result row for the planets of Mercury or Venus ( planetIds 1 and 2). This is because there is no matching entry in the $satellites relation for these planets, so the join ignores the unmatched rows in the $planets relation. Each of the relations being joined have an id and a name column, to ensure it is clear which relation the value being displayed is from, columns with clashing names are qualified with the relation name. To avoid abiguity and problems in the future if new columns are added to relations, it is good practice to qualify column names in join conditions: SELECT * FROM $ satellites , $ planets WHERE $ satellites . planetId = $ planets . id ; Will return the same result as above, but be more resistant to future failure. Join queries of the kind seen thus far can also be written in this alternative form: SELECT * FROM $ satellites INNER JOIN $ planets ON $ satellites . planetId = $ planets . id ; The planner currently uses a different execution strategy for these two similar queries, the explicit INNER JOIN style generally executes faster. Now we will figure out how we can get the Mercury and Venus records back in. What we want the query to do is to scan the $planets relation and for each row to find the matching $satellites row(s). If no matching row is found we want some \u201cempty values\u201d to be substituted for the $satellites relations columns. This kind of query is called an outer join. (The joins we have seen so far are inner joins and cross joins.) The command looks like this: SELECT * FROM $ satellites LEFT OUTER JOIN $ planets ON $ satellites . planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- | 1 | | | 2 | | 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Using the LEFT OUTER JOIN will mean the relation mentioned on the left of the join operator will have each of its rows in the output at least once, whereas the relation on the right will only have those rows output that match some row of the left relation. When outputting a left-relation row for which there is no right-relation match, empty ( null ) values are substituted for the right-relation columns. Note How null values are displayed may be different between different systems, common approaches are to display an empty cell or display 'none' or 'null' in an alternate format (e.g. italics or different font color). This is not controlled by the query engine.","title":"Joins Between Relations"},{"location":"sql-reference/introduction/#aggregate-functions","text":"An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT , SUM , AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.","title":"Aggregate Functions"},{"location":"sql-reference/joins/","text":"Joins Joins allow you to combine data from multiple relations into a single relation. There are a number of different join types available, each combines relations in different ways. CROSS JOIN FROM left_relation CROSS JOIN right_relation FROM left_relation, right_relation A CROSS JOIN returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explicit CROSS JOIN syntax or by specifying multiple relations in the FROM clause. SELECT * FROM left_relation CROSS JOIN right_relation ; The size of the resultant dataset when using CROSS JOIN is length of the two datasets multiplied together (2 x 3 = 6, in the pictorial example), which can easily result in extremely large datasets. When an alternate join approach is possible, it will almost always perform better than a CROSS JOIN . INNER JOIN FROM left_relation [ INNER ] JOIN right_relation < ON condition | USING (column) > An INNER JOIN returns rows from both relations where the value in the joining column of one relation matches the value in the joining column of the other relation. Inner joins can either be specified using the full INNER JOIN syntax or the shorter JOIN syntax, and the joining logic specified using ON condition or USING(column) syntax. SELECT * FROM left_relation INNER JOIN right_relation ON left_relation . column_name = right_relation . column_name ; In this example, the blue column is used as the joining column in both relations. Only the value 1 occurs in both relations so the resultant dataset is the combination of the row with 1 in right_relation and the row with 1 in left_relation . LEFT JOIN FROM left_relation LEFT [ OUTER ] JOIN right_relation ON condition A LEFT JOIN returns all rows from the left relation, and rows from the right relation where there is a matching row, otherwise the fields for the right relation are populated with NULL . SELECT * FROM left_relation LEFT OUTER JOIN right_relation ON left_relation . column_name = right_relation . column_name ; RIGHT JOIN A RIGHT JOIN is the same as a LEFT JOIN with the relations swapped. FULL JOIN FROM left_relation FULL [ OUTER ] JOIN right_relation ON condition The FULL JOIN keyword returns all rows from the left relation, and all rows from the right relation. Where they have a matching value in the joining column, the rows will be aligned, otherwise the fields will be populated with NULL . SELECT * FROM left_relation FULL OUTER JOIN right_relation ON left_relation . column_name = right_relation . column_name ;","title":"Joins"},{"location":"sql-reference/joins/#joins","text":"Joins allow you to combine data from multiple relations into a single relation. There are a number of different join types available, each combines relations in different ways.","title":"Joins"},{"location":"sql-reference/joins/#cross-join","text":"FROM left_relation CROSS JOIN right_relation FROM left_relation, right_relation A CROSS JOIN returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explicit CROSS JOIN syntax or by specifying multiple relations in the FROM clause. SELECT * FROM left_relation CROSS JOIN right_relation ; The size of the resultant dataset when using CROSS JOIN is length of the two datasets multiplied together (2 x 3 = 6, in the pictorial example), which can easily result in extremely large datasets. When an alternate join approach is possible, it will almost always perform better than a CROSS JOIN .","title":"CROSS JOIN"},{"location":"sql-reference/joins/#inner-join","text":"FROM left_relation [ INNER ] JOIN right_relation < ON condition | USING (column) > An INNER JOIN returns rows from both relations where the value in the joining column of one relation matches the value in the joining column of the other relation. Inner joins can either be specified using the full INNER JOIN syntax or the shorter JOIN syntax, and the joining logic specified using ON condition or USING(column) syntax. SELECT * FROM left_relation INNER JOIN right_relation ON left_relation . column_name = right_relation . column_name ; In this example, the blue column is used as the joining column in both relations. Only the value 1 occurs in both relations so the resultant dataset is the combination of the row with 1 in right_relation and the row with 1 in left_relation .","title":"INNER JOIN"},{"location":"sql-reference/joins/#left-join","text":"FROM left_relation LEFT [ OUTER ] JOIN right_relation ON condition A LEFT JOIN returns all rows from the left relation, and rows from the right relation where there is a matching row, otherwise the fields for the right relation are populated with NULL . SELECT * FROM left_relation LEFT OUTER JOIN right_relation ON left_relation . column_name = right_relation . column_name ;","title":"LEFT JOIN"},{"location":"sql-reference/joins/#right-join","text":"A RIGHT JOIN is the same as a LEFT JOIN with the relations swapped.","title":"RIGHT JOIN"},{"location":"sql-reference/joins/#full-join","text":"FROM left_relation FULL [ OUTER ] JOIN right_relation ON condition The FULL JOIN keyword returns all rows from the left relation, and all rows from the right relation. Where they have a matching value in the joining column, the rows will be aligned, otherwise the fields will be populated with NULL . SELECT * FROM left_relation FULL OUTER JOIN right_relation ON left_relation . column_name = right_relation . column_name ;","title":"FULL JOIN"},{"location":"sql-reference/statements/","text":"Statements The following statement forms are supported. EXPLAIN Show the logical execution plan of a statement. EXPLAIN statement The EXPLAIN clause outputs a summary of the execution plan for the query in the SELECT statement. Warning The data returned by the EXPLAIN statement is intended for interactive usage only and the output format may change between releases. Applications should not depend on the output of the EXPLAIN statement. SELECT Retrieve rows from zero or more relations. SELECT [ DISTINCT ] < expression > , .. FROM < relation > FOR < period > [ WITH ( NO_CACHE | NO_PARTITION | NO_PUSH_PROJECTION )] [ INNER ] JOIN < relation > | < function > | ( < subquery > ) CROSS JOIN < relation > | < function > | ( < subquery > ) LEFT [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) RIGHT [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) FULL [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) ON < expression > USING ( < columns > ) WHERE < expression > , .. GROUP BY HAVING < expression > , .. ORDER BY < expression > , .. OFFSET < offset > LIMIT < limit > SELECT clause SELECT [ DISTINCT ] expression [, ...] The SELECT clause specifies the list of columns that will be returned by the query. While it appears first in the clause, logically the expressions here are executed after most other clauses. The SELECT clause can contain arbitrary expressions that transform the output, as well as aggregate functions. The DISTINCT modifier is specified, only unique rows are included in the result set. In this case, each output column must be of a type that allows comparison. FROM / JOIN clauses FROM relation [FOR period] [WITH (NO_CACHE, NO_PARTITION, NO_PUSH_PROJECTION)] [, ...] FROM relation [FOR period] [ INNER ] JOIN relation [FOR period] < USING (columns) | ON condition > FROM relation [FOR period] LEFT [ OUTER ] JOIN relation [FOR period] < USING (columns) | ON condition > FROM relation [FOR period] < RIGHT | FULL > [OUTER ] JOIN relation [FOR period] FROM relation [FOR period] CROSS JOIN < relation [FOR period] | UNNEST(column) > The FROM clause specifies the source of the data on which the remainder of the query should operate. Logically, the FROM clause is where the query starts execution. The FROM clause can contain a single relation, a combination of multiple relations that are joined together, or another SELECT query inside a subquery node. JOIN clauses allow you to combine data from multiple relations. If no JOIN qualifier is provided, INNER will be used. JOIN qualifiers are mutually exclusive. ON and USING clauses are also mutually exclusive and can only be used with INNER and LEFT joins. See Joins for more information on JOIN syntax and functionality. Hints can be provided as part of the statement to direct the query planner and executor to make decisions. Relation hints are declared as WITH statements following a relation in the FROM and JOIN clauses, for example FROM $astronauts WITH (NO_CACHE) . Reconised hints are: Hint Effect NO_CACHE Ignores any cache configuration NO_PARTITION Do not use partition configuration when reading NO_PUSH_PROJECTION Do not attempt to prune columns when reading Note Hints are not guaranteed to be followed, the query planner and executor may ignore hints in specific circumstances. FOR clause FOR date FOR DATES BETWEEN start AND end FOR DATES IN range The FOR clause is a non-standard clause which filters data by the date it was recorded for. When provided FOR clauses must directly follow the relation name in a FROM or JOIN clause. If not provided FOR TODAY is assumed. See Time Travel for more information on FOR syntax and functionality. WHERE clause WHERE condition The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Logically the WHERE clause is applied immediately after the FROM clause. GROUP BY / HAVING clauses GROUP BY expression [, ...] HAVING group_filter The GROUP BY clause specifies which grouping columns should be used to perform any aggregations in the SELECT clause. If the GROUP BY clause is specified, the query is always an aggregate query, even if no aggregations are present in the SELECT clause. The HAVING clause specifies filters to apply to aggregated data, HAVING clauses require a GROUP BY clause. GROUP BY expressions may use column numbers, however, this is not recommended for statements intended for reuse. ORDER BY / LIMIT / OFFSET clauses ORDER BY expression [ ASC | DESC ] [, ...] OFFSET count LIMIT count ORDER BY , LIMIT and OFFSET are output modifiers. Logically they are applied at the very end of the query. The OFFSET clause discards initial rows from the returned set, the LIMIT clause restricts the amount of rows fetched, and the ORDER BY clause sorts the rows on the sorting criteria in either ascending or descending order. ORDER BY expressions may use column numbers, however, this is not recommended for statements intended for reuse. SET Specifies the value of a variable, the variable is available to the scope of the executing query batch. SET variable = value User defined variable names must be prefixed with an 'at' symbol ( @ ) and the value must be a literal value. The variable can be used within SELECT clauses within the same query batch. A SET statement without a SELECT statement is invalid. System parameters can also be temporarily for a query batch and are prefixed with a dollar sign ( $ ). Related : SHOW VARIABLES and SHOW PARAMETER SHOW COLUMNS List the columns in a relation along with their data type. Without any modifiers, SHOW COLUMNS only reads a single page of data before returning. SHOW [ EXTENDED ] [ FULL ] COLUMNS FROM relation LIKE pattern FOR period EXTENDED modifier Inclusion of the EXTENDED modifier includes summary statistics about the columns which take longer and more memory to create than the standard summary information without the modifier. The summary information varies between column types and values. FULL modifier Inclusion of the FULL modifier uses the entire dataset in order to return complete column information, rather than just the first page from the dataset. LIKE clause LIKE pattern A case-insensitive LIKE clause to filter the results to the desired subset by the column name. This does not require a left-hand operator, it will always filter by the column name. FOR clause FOR date FOR DATES BETWEEN start AND end FOR DATES IN range The FOR clause is a non-standard clause which filters data by the date it was recorded for. When provided FOR clauses must directly follow the relation name the FROM clause. If not provided FOR TODAY is assumed. See Time Travel for more information on FOR syntax and functionality. SHOW CREATE TABLE Show an approximation of the SQL to create a specified relation. SHOW CREATE TABLE table FOR period The SQL generated by this statement is unlikely to be able to be used with any SQL engine to create the table without some intervention and edits. It is intended to reduce effort to obtain this SQL, not eliminate it. FOR clause FOR date FOR DATES BETWEEN start AND end FOR DATES IN range The FOR clause is a non-standard clause which filters data by the date it was recorded for. When provided FOR clauses must directly follow the relation name. If not provided FOR TODAY is assumed. SHOW FUNCTIONS List the functions and aggregators supported by the engine. SHOW FUNCTIONS LIKE pattern LIKE clause LIKE pattern A case-insensitive LIKE clause to filter the results to the desired subset by the function name. This does not require a left-hand operator, it will always filter by the function name. SHOW PARAMETER Display the value of a given configuration setting. SHOW PARAMETER parameter SHOW STORES Display the set of configured data stores. SHOW STORES SHOW VARIABLES List the variables set in the query batch. SHOW VARIABLES LIKE pattern LIKE clause LIKE pattern A case-insensitive LIKE clause to filter the results to the desired subset by the variable name. This does not require a left-hand operator, it will always filter by the variable name. Related : SET","title":"Statements"},{"location":"sql-reference/statements/#statements","text":"The following statement forms are supported.","title":"Statements"},{"location":"sql-reference/statements/#explain","text":"Show the logical execution plan of a statement. EXPLAIN statement The EXPLAIN clause outputs a summary of the execution plan for the query in the SELECT statement. Warning The data returned by the EXPLAIN statement is intended for interactive usage only and the output format may change between releases. Applications should not depend on the output of the EXPLAIN statement.","title":"EXPLAIN"},{"location":"sql-reference/statements/#select","text":"Retrieve rows from zero or more relations. SELECT [ DISTINCT ] < expression > , .. FROM < relation > FOR < period > [ WITH ( NO_CACHE | NO_PARTITION | NO_PUSH_PROJECTION )] [ INNER ] JOIN < relation > | < function > | ( < subquery > ) CROSS JOIN < relation > | < function > | ( < subquery > ) LEFT [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) RIGHT [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) FULL [ OUTER ] JOIN < relation > | < function > | ( < subquery > ) ON < expression > USING ( < columns > ) WHERE < expression > , .. GROUP BY HAVING < expression > , .. ORDER BY < expression > , .. OFFSET < offset > LIMIT < limit >","title":"SELECT"},{"location":"sql-reference/statements/#set","text":"Specifies the value of a variable, the variable is available to the scope of the executing query batch. SET variable = value User defined variable names must be prefixed with an 'at' symbol ( @ ) and the value must be a literal value. The variable can be used within SELECT clauses within the same query batch. A SET statement without a SELECT statement is invalid. System parameters can also be temporarily for a query batch and are prefixed with a dollar sign ( $ ). Related : SHOW VARIABLES and SHOW PARAMETER","title":"SET"},{"location":"sql-reference/statements/#show-columns","text":"List the columns in a relation along with their data type. Without any modifiers, SHOW COLUMNS only reads a single page of data before returning. SHOW [ EXTENDED ] [ FULL ] COLUMNS FROM relation LIKE pattern FOR period","title":"SHOW COLUMNS"},{"location":"sql-reference/statements/#show-create-table","text":"Show an approximation of the SQL to create a specified relation. SHOW CREATE TABLE table FOR period The SQL generated by this statement is unlikely to be able to be used with any SQL engine to create the table without some intervention and edits. It is intended to reduce effort to obtain this SQL, not eliminate it.","title":"SHOW CREATE TABLE"},{"location":"sql-reference/statements/#show-functions","text":"List the functions and aggregators supported by the engine. SHOW FUNCTIONS LIKE pattern","title":"SHOW FUNCTIONS"},{"location":"sql-reference/statements/#show-parameter","text":"Display the value of a given configuration setting. SHOW PARAMETER parameter","title":"SHOW PARAMETER"},{"location":"sql-reference/statements/#show-stores","text":"Display the set of configured data stores. SHOW STORES","title":"SHOW STORES"},{"location":"sql-reference/statements/#show-variables","text":"List the variables set in the query batch. SHOW VARIABLES LIKE pattern","title":"SHOW VARIABLES"}]}